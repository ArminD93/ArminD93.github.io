[{"categories":null,"contents":"Data analysis from the Big Five Personality test.\nlink to wikipedia\nlink to GitHub Repository\n","date":"June 27, 2020","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/bigfive/bigfive_about/","summary":"Data analysis from the Big Five Personality test.\nlink to wikipedia\nlink to GitHub Repository","tags":null,"title":"Big Five Personality test - about project"},{"categories":null,"contents":"Using convolutional neural network to classifying image into 6 classes.\nlink to data\nlink to GitHub Repository\n","date":"April 6, 2020","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/imageclassproject/project_imageclass_about/","summary":"Using convolutional neural network to classifying image into 6 classes.\nlink to data\nlink to GitHub Repository","tags":null,"title":"Convolutional network testing for image classification - about project"},{"categories":null,"contents":"Data analysis from the Kepler Exoplanet Search Results report.\nlink to GitHub Repository\n","date":"April 2, 2020","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/kepler/project_kepler_about/","summary":"Data analysis from the Kepler Exoplanet Search Results report.\nlink to GitHub Repository","tags":null,"title":"Kepler Exoplanet Search Results - about project"},{"categories":null,"contents":"The data contain informations which might have influence for people\u0026rsquo;s happiness score in the selected country. We have 5 reports divided by years, from 2015 to 2019.\nAcording to data, the happiness score can be estimated from six factors - economic production, social support, life expectancy, freedom, absence of corruption, and generosity. These factors contribute to making life evaluations higher in each country than they are in Dystopia, a hypothetical country that has values equal to the world’s lowest national averages for each of the six factors. They have no impact on the total score reported for each country, but they do explain why some countries rank higher than others.\nlink to data\nlink to GitHub Repository\n","date":"February 19, 2020","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/worldhappiness/project_worldhappiness_about/","summary":"The data contain informations which might have influence for people\u0026rsquo;s happiness score in the selected country. We have 5 reports divided by years, from 2015 to 2019.\nAcording to data, the happiness score can be estimated from six factors - economic production, social support, life expectancy, freedom, absence of corruption, and generosity. These factors contribute to making life evaluations higher in each country than they are in Dystopia, a hypothetical country that has values equal to the world’s lowest national averages for each of the six factors.","tags":null,"title":"World Happiness project - about project"},{"categories":null,"contents":"link to GitHub Repository\nIntroduction The device was built using elements from a 3D printer (frame, stepper motors). Three randomly placed wooden blocks on the shelves of the rack are picked up by the gripper and stored in one of the cells in the bottom row of this rack. The place on the bottom shelf is chosen, where the path from these blocks to this place is the shortest\nOverview   Device control through a graphical user interface:\n Selecting on the keyboard the buttons corresponding to the places where the blocks are located. START button to start the device. Displaying information about selected items.    Removing and putting away wooden blocks with the gripper, which is mounted on the trolley from DVD-ROM drive.\n  Using the Dijkstra algorithm by which you can find the shortest path from the starting point to the destination.\n  Identifying whether the object is located in the right column.\n  There are QR codes on the wooden blocks which are recognized by the webcam located above the gripper.\n","date":"July 5, 2019","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/libraryproject/project_library/","summary":"link to GitHub Repository\nIntroduction The device was built using elements from a 3D printer (frame, stepper motors). Three randomly placed wooden blocks on the shelves of the rack are picked up by the gripper and stored in one of the cells in the bottom row of this rack. The place on the bottom shelf is chosen, where the path from these blocks to this place is the shortest\nOverview   Device control through a graphical user interface:","tags":null,"title":"Project of an automated system for selecting wooden blocks from a rack using Raspberry Pi"},{"categories":null,"contents":"link to GitHub repository\nIntroduction I wanted to get informations about books prices, which I\u0026rsquo;d like to buy in the future. Additionally, it would be nice to see the price curve on the chart.\nFunctions:  Downloading informations about books (title, price, date) from the acount on the website: ebooki.swiatczytnikow.pl. Saving data to the database. Displaying charts - how the price has changed over time for a specific title.  Main menu:   Menu if click t: Menu if click n: All titles: Entering new book data: If you would like to add new book data to database, you have to enter your email from page \u0026ldquo;ebooki.swiatczytnikow.pl/mojealerty\u0026rdquo;.\nAll titles: Program check, if email is correct:\n","date":"October 8, 2018","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/bookprices/project_booksprices/","summary":"link to GitHub repository\nIntroduction I wanted to get informations about books prices, which I\u0026rsquo;d like to buy in the future. Additionally, it would be nice to see the price curve on the chart.\nFunctions:  Downloading informations about books (title, price, date) from the acount on the website: ebooki.swiatczytnikow.pl. Saving data to the database. Displaying charts - how the price has changed over time for a specific title.  Main menu:   Menu if click t: Menu if click n: All titles: Entering new book data: If you would like to add new book data to database, you have to enter your email from page \u0026ldquo;ebooki.","tags":null,"title":"Books prices project"},{"categories":null,"contents":"Hi, I\u0026rsquo;m Armin Skills:   Python:  Completed Udemy course: Python Intermediate (Credential ID: UC-CZ6ZMUWE) Program implemented in the Master’s and Bachelor’s projects    Data analysis:  Completed Udemy course: Data Science: Data analysis in Python and pandas (Credential ID: UC-K631OZR7) pandas, numpy, matplolib, seaborn    Machine learning:  Completed Udemy: Machine Learning in Python. Basics, perceptron, regression (Credential ID: UC-1f13cf3e-e300-4e06-81d4-0a9d318179d6) Completed Udemy: Machine Learning Bootcampin Python language ep. I (Credential ID: UC-6ae7344a-81cc-4347-8b0b-6c330a2626b0) Scikit-learn basics    Neural networks:  Basics of neural networks in engineering and master\u0026rsquo;s studies Completed Udemy: Introduction to neural networks - Tensorflow 2.0 + Keras (Credential ID: UC-6ae7344a-81cc-4347-8b0b-6c330a2626b0) Keras basics    Linux system   Relational database and language:  MySQL, PostgreSQL    Basics of C and C ++   Languages and Tools: ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/about/","summary":"Hi, I\u0026rsquo;m Armin Skills:   Python:  Completed Udemy course: Python Intermediate (Credential ID: UC-CZ6ZMUWE) Program implemented in the Master’s and Bachelor’s projects    Data analysis:  Completed Udemy course: Data Science: Data analysis in Python and pandas (Credential ID: UC-K631OZR7) pandas, numpy, matplolib, seaborn    Machine learning:  Completed Udemy: Machine Learning in Python. Basics, perceptron, regression (Credential ID: UC-1f13cf3e-e300-4e06-81d4-0a9d318179d6) Completed Udemy: Machine Learning Bootcampin Python language ep.","tags":null,"title":""},{"categories":null,"contents":"Social icons +++ [[params.social]] name = \u0026ldquo;email\u0026rdquo; url = \u0026ldquo;mailto:armin.derencz@gmail.com\u0026rdquo; weight = 1 [[params.social]] name = \u0026ldquo;github\u0026rdquo; url = \u0026ldquo;https://github.com/ArminD93\u0026quot; weight = 2\n[params.social]] name = \u0026ldquo;linkedin\u0026rdquo; url = \u0026ldquo;https://www.linkedin.com/in/armin-derencz/\u0026quot; weight = 3 +++\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/contact/","summary":"Social icons +++ [[params.social]] name = \u0026ldquo;email\u0026rdquo; url = \u0026ldquo;mailto:armin.derencz@gmail.com\u0026rdquo; weight = 1 [[params.social]] name = \u0026ldquo;github\u0026rdquo; url = \u0026ldquo;https://github.com/ArminD93\u0026quot; weight = 2\n[params.social]] name = \u0026ldquo;linkedin\u0026rdquo; url = \u0026ldquo;https://www.linkedin.com/in/armin-derencz/\u0026quot; weight = 3 +++","tags":null,"title":""},{"categories":null,"contents":"Dataset contains 1,015,342 questionnaire answers collected online by Open-Source Psychometrics Project. According to information from the page openpsychometrics.org, The big five personality traits are the best accepted and most commonly used model of personality in academic psychology. Thanks to results from this test, researchers can analyse factors of personality which can be applied to individual person.\nOn wikipedia page we can got to know that The Big Five personality traits is a taxonomy, or grouping for personality traits. This test is also called as the five-factor model (FFM) or the OCEAN model. The theory identifies five factors:\n openness to experience (inventive/curious vs. consistent/cautious) conscientiousness (efficient/organized vs. extravagant/careless) extraversion (outgoing/energetic vs. solitary/reserved) agreeableness (friendly/compassionate vs. challenging/callous) neuroticism (sensitive/nervous vs. resilient/confident)  Description of the values ​​from the columns The following items were presented on one page and each was rated on a five point scale using radio buttons. The order on page was was EXT1, AGR1, CSN1, EST1, OPN1, EXT2, etc. The scale was labeled 1=Disagree, 3=Neutral, 5=Agree\n5 major dimensions of personality: Openness, Conscientiousness, Agreeableness, Extraversion, and Neuroticism.\n EXT - Extraversion, EST - Neuroticism, AGR - Agreeableness (Zgodność), CSN - Conscientiousness (sumienność), OPN - Openness  link to GitHub Repository\n   indication Description indication Description indication Description     EXT1 I am the life of the party. EST1 I get stressed out easily. AGR1 I feel little concern for others.   EXT2 I don\u0026rsquo;t talk a lot. EST2 I am relaxed most of the time. AGR2 I am interested in people.   EXT3 I feel comfortable around people. EST3 I worry about things. AGR3 I insult people.   EXT4 I keep in the background. EST4 I seldom feel blue. AGR4 I sympathize with others\u0026rsquo; feelings.   EXT5 I start conversations. EST5 I am easily disturbed. AGR5 I am not interested in other people\u0026rsquo;s problems.   EXT6 I have little to say. EST6 I get upset easily. AGR6 I have a soft heart.   EXT7 I talk to a lot of different people at parties. EST7 I change my mood a lot. AGR7 I am not really interested in others.   EXT8 I don\u0026rsquo;t like to draw attention to myself. EST8 I have frequent mood swings. AGR8 I take time out for others.   EXT9 I don\u0026rsquo;t mind being the center of attention. EST9 I get irritated easily. AGR9 I feel others\u0026rsquo; emotions.   EXT10 I am quiet around strangers. EST10 I often feel blue. AGR10 I make people feel at ease.           CSN1 I am always prepared. OPN1 I have a rich vocabulary. dateload The timestamp when the survey was started.   CSN2 I leave my belongings around. OPN2 I have difficulty understanding abstract ideas. screenw The width the of user\u0026rsquo;s screen in pixels   CSN3 I pay attention to details. OPN3 I have a vivid imagination. screenh The height of the user\u0026rsquo;s screen in pixels   CSN4 I make a mess of things. OPN4 I am not interested in abstract ideas. introelapse The time in seconds spent on the landing / intro page   CSN5 I get chores done right away. OPN5 I have excellent ideas. testelapse The time in seconds spent on the page with the survey questions   CSN6 I often forget to put things back in their proper place. OPN6 I do not have a good imagination. endelapse The time in seconds spent on the finalization page (where the user was asked to indicate if they has answered accurately and their answers could be stored and used for research. Again: this dataset only includes users who answered \u0026ldquo;Yes\u0026rdquo; to this question, users were free to answer no and could still view their results either way)   CSN7 I like order. OPN7 I am quick to understand things. IPC The number of records from the user\u0026rsquo;s IP address in the dataset. For max cleanliness, only use records where this value is 1. High values can be because of shared networks (e.g. entire universities) or multiple submissions   CSN8 I shirk my duties. OPN8 I use difficult words. country The country, determined by technical information (NOT ASKED AS A QUESTION)   CSN9 I follow a schedule. OPN9 I spend time reflecting on things. lat_appx_lots_of_err approximate latitude of user. determined by technical information, THIS IS NOT VERY ACCURATE.   CSN10 I am exacting in my work. OPN10 I am full of ideas. long_appx_lots_of_err approximate longitude of user     The time spent on each question is also recorded in milliseconds. These are the variables ending in _E. This was calculated by taking the time when the button for the question was clicked minus the time of the most recent other button click.\nBig-Five Factor Markers  Factor I - Extraversion 10-item scale (Alpha = .87)\n   + keyed – keyed     Am the life of the party. Don\u0026rsquo;t talk a lot.   Feel comfortable around people. Keep in the background.   Start conversations. Have little to say.   Talk to a lot of different people at parties. Don\u0026rsquo;t like to draw attention to myself.   Don\u0026rsquo;t mind being the center of attention. Am quiet around strangers.     Factor II - Agreeableness 10-item scale (Alpha = .82)\n   + keyed – keyed     Am interested in people. Am not really interested in others.   Sympathize with others\u0026rsquo; feelings. Insult people.   Have a soft heart. Am not interested in other people\u0026rsquo;s problems.   Take time out for others. Feel little concern for others.   Feel others\u0026rsquo; emotions.    Make people feel at ease.      Factor III - Conscientiousness 10-item scale (Alpha = .79)\n   + keyed – keyed     Am always prepared. Leave my belongings around.   Pay attention to details. Make a mess of things.   Get chores done right away. Often forget to put things back in their proper place.   Like order. Shirk my duties.   Follow a schedule.    Am exacting in my work.      Factor IV - Emotional Stability (Neuroticism) 10-item scale (Alpha = .86)\n   + keyed – keyed     Am relaxed most of the time. Get stressed out easily.   Seldom feel blue. Worry about things.    Am easily disturbed.    Get upset easily.    Change my mood a lot.    Have frequent mood swings.    Get irritated easily.    Often feel blue.     Factor V - Intellect or Imagination (Openness) 10-item scale (Alpha = .84)\n   + keyed – keyed     Have a rich vocabulary. Have difficulty understanding abstract ideas.   Have a vivid imagination. Am not interested in abstract ideas.   Have excellent ideas. Do not have a good imagination   Am quick to understand things.    Use difficult words.    Spend time reflecting on things.    Am full of ideas.     ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/bigfive/bigfive_col_names/","summary":"Dataset contains 1,015,342 questionnaire answers collected online by Open-Source Psychometrics Project. According to information from the page openpsychometrics.org, The big five personality traits are the best accepted and most commonly used model of personality in academic psychology. Thanks to results from this test, researchers can analyse factors of personality which can be applied to individual person.\nOn wikipedia page we can got to know that The Big Five personality traits is a taxonomy, or grouping for personality traits.","tags":null,"title":"Big Five Personality test - columns description"},{"categories":null,"contents":"Data analysis: Preparation of groups based on the name of the country countries_groups = BigFive.groupby(by='country') # How many groups are there?: len(countries_groups) 221   Countries_groups_counts = countries_groups.size().to_frame('count') Countries_groups_counts.reset_index(inplace=True) Countries_groups_counts.head()      country count     0 Afghanistan 46   1 Albania 353   2 Algeria 230   3 American Samoa 1   4 Andorra 15    Countries_groups_counts_descending = Countries_groups_counts.sort_values('count', ascending=False ) plt.figure(figsize=(27, 10) ) squarify.plot( sizes= Countries_groups_counts_descending['count'], label= Countries_groups_counts_descending['country'][:25], alpha=.7 ) plt.axis('off') plt.show()  The number of times each response occurs for each issue Correlation analysis in individual traits Description and classification of markings Data preparation Correlation maps for individual traits Countries with the highest Extraversion level Radar diagram for the selected row BigFive.head(3)    EXT1 EXT2 EXT3 EXT4 EXT5 EXT6 EXT7 EXT8 EXT9 EXT10 EST1 EST2 EST3 EST4 EST5 EST6 EST7 EST8 EST9 EST10 AGR1 AGR2 AGR3 AGR4 AGR5 AGR6 AGR7 AGR8 AGR9 AGR10 CSN1 CSN2 CSN3 CSN4 CSN5 CSN6 CSN7 CSN8 CSN9 CSN10 OPN1 OPN2 OPN3 OPN4 OPN5 OPN6 OPN7 OPN8 OPN9 OPN10 dateload introelapse testelapse endelapse alpha2_code lat_appx_lots_of_err long_appx_lots_of_err country 0 4.0 1.0 5.0 2.0 5.0 1.0 5.0 2.0 4.0 1.0 1.0 4.0 4.0 2.0 2.0 2.0 2.0 2.0 3.0 2.0 2.0 5.0 2.0 4.0 2.0 3.0 2.0 4.0 3.0 4.0 3.0 4.0 3.0 2.0 2.0 4.0 4.0 2.0 4.0 4.0 5.0 1.0 4.0 1.0 4.0 1.0 5.0 3.0 4.0 5.0 2016-03-03 02:01:01 9.0 234.0 6 GB 51.5448 0.1991 United Kingdom 1 3.0 5.0 3.0 4.0 3.0 3.0 2.0 5.0 1.0 5.0 2.0 3.0 4.0 1.0 3.0 1.0 2.0 1.0 3.0 1.0 1.0 4.0 1.0 5.0 1.0 5.0 3.0 4.0 5.0 3.0 3.0 2.0 5.0 3.0 3.0 1.0 3.0 3.0 5.0 3.0 1.0 2.0 4.0 2.0 3.0 1.0 4.0 2.0 5.0 3.0 2016-03-03 02:01:20 12.0 179.0 11 MY 3.1698 101.706 Malaysia 2 2.0 3.0 4.0 4.0 3.0 2.0 1.0 3.0 2.0 5.0 4.0 4.0 4.0 2.0 2.0 2.0 2.0 2.0 1.0 3.0 1.0 4.0 1.0 4.0 2.0 4.0 1.0 4.0 4.0 3.0 4.0 2.0 2.0 2.0 3.0 3.0 4.0 2.0 4.0 2.0 5.0 1.0 2.0 1.0 4.0 2.0 5.0 3.0 4.0 4.0 2016-03-03 02:01:56 3.0 186.0 7 GB 54.9119 -1.3833 United Kingdom  Preparing the RadarChart class class RadarChart: def __init__(self, rowNo, data): self.rowNo = rowNo self.data = data self.cols = 2 self.rows = 3 self.scale_ticks_in_numbers = np.array([0, 1, 2, 3, 4, 5]) # 1=Disagree, 3=Neutral, 5=Agree self.factors_ticks_in_numbers = np.arange(0, 41, 5) self.Factors_name_list = [Factor_I_Extraversion_names_list, Factor_II_Agreeableness_names_list, Factor_III_Conscientiousness_names_list, Factor_IV_EmotionalStability_names_list, Factor_V_Openness_names_list ] self.scale_ticks = ['No\\nanswer', 'Disagree', 'slightly\\ndisagree', 'Neutral', 'Slightly\\nagree', 'Agree'] self.title_list = ['Factor I - Extraversion', 'Factor II - Agreeableness', 'Factor III - Conscientiousness', 'Factor IV - Emotional Stability (Neuroticism)', 'Factor V - Intellect or Imagination (Openness)'] self.color_list = ['green', 'blue', 'yellow', 'red', 'purple'] self.factors_list = ['Extraversion', 'Agreeableness', 'Conscientiousness', 'Emotional Stability', 'Openness'] def __CheckFlag(self, flag): self.scores_data = 0 self.Factors_name_list if flag == 0: self.scores_data = self.data.columns[1:51] # self.questions_list = self.Factors_name_list self.name_list = [ quest for quest in self.Factors_name_list][self.FactorNo] elif flag == 1: self.scores_data = ['Extraversion', 'Agreeableness', 'Conscientiousness', 'EmotionalStability', 'Openness']# self.data.columns[-5:] # self.questions_list = self.factors_list self.name_list = self.factors_list self.start, self.stop = ( 0, 10 ) def __getnRowFromDataFrame(self): self.first_row_scores = np.array( self.data.loc[ self.rowNo, self.scores_data ] ) self.country_name = np.array( self.data.loc[ self.rowNo, self.data.columns[0] ] ) self.first_row_scores_with_country_name = np.append( self.first_row_scores, self.country_name ) def __setTitleAndColor(self, flag): if flag == 0: self.setTitle = [ title for title in self.title_list ][self.titleNo] self.setColor = [ color for color in self.color_list ][self.ColorNo] elif flag == 1: self.setTitle = 'Personality' self.setColor = self.color_list[0] def __getScoresAndQuestions(self): self.iterator = islice(self.first_row_scores_with_country_name[:-1], self.start, self.stop ) self.factor_scores = [scores for scores in self.iterator] self.factor_questions = self.name_list def __setQuestions_and_scores_for_chart(self): self.factor_questions = np.array(self.factor_questions) self.angles = np.linspace(0, 2*np.pi, len(self.factor_questions), endpoint=False) self.factor_scores = np.concatenate( [ self.factor_scores, [self.factor_scores[0]] ] ) self.angles = np.concatenate( [ self.angles, [self.angles[0]] ] ) def __makePolarChart(self): self.ax.plot(self.angles, self.factor_scores, 'o-', linewidth=2, color= self.setColor) self.ax.fill(self.angles, self.factor_scores, alpha=0.3, color= self.setColor) self.ax.set_thetagrids(self.angles * 180/np.pi, self.factor_questions) def __addDescriptionToChart(self, flag): self.ax.set_title(self.setTitle) self.fig.suptitle(f'Row number: {self.rowNo} - Country: { self.first_row_scores_with_country_name[-1:] }', fontsize=25) self.ax.set_rlabel_position(30) if flag == 0: plt.yticks(self.scale_ticks_in_numbers, self.scale_ticks, color='grey', size=12) # map values with names plt.ylim(0, 5) elif flag == 1: plt.yticks(self.factors_ticks_in_numbers, color='grey', size=12) plt.ylim(0, 40) def create_chart(self, flag=0): self.FactorNo = 0 self.titleNo = 0 self.ColorNo = 0 self. __CheckFlag(flag) self.__getnRowFromDataFrame() if flag == 0: self.fig= plt.figure(figsize=(28, 28) ) self.fig.subplots_adjust(hspace=0.2, wspace=0.1) for self.i, self.start, self.stop in zip( range(1, 6), range(0, 50, 10), range(10, 60, 10)): self.ax = self.fig.add_subplot(self.rows, self.cols, self.i, polar=True) self.__setTitleAndColor(flag) self.__getScoresAndQuestions() self.__setQuestions_and_scores_for_chart() self.__makePolarChart() self.__addDescriptionToChart(flag) self.titleNo += 1 self.ColorNo += 1 self.FactorNo += 1 elif flag == 1: self.fig= plt.figure(figsize=(10, 10) ) self.ax = self.fig.add_subplot(1, 1, 1, polar=True) self.__setTitleAndColor(flag) self.__getScoresAndQuestions() self.__setQuestions_and_scores_for_chart() self.__makePolarChart() self.__addDescriptionToChart(flag) return plt.show()  Creating objects scoresRadar_row_0 = RadarChart( rowNo= 0 ) scoresRadar_row_25 = RadarChart( rowNo= 25 ) scoresRadar_row_100 = RadarChart( rowNo= 100 )  Presentation of the results for each personality trait for the selected row scoresRadar_row_0.create_chart(flag=0)  scoresRadar_row_25.create_chart(flag=0)  scoresRadar_row_100.create_chart(flag=0 )  Calculating points for individual features Equations:\n E = 20 + (1) ___ - (6) ___ +(11) ___ - (16) ___ +(21) ___ - (26) ___ +(31) ___ - (36) ___ +(41) ___ - (46) ___ = _____ A = 14 - (2) ___ + (7) ___ -(12) ___ +(17) ___ -(22) ___ +(27) ___ -(32) ___ +(37) ___ +(42) ___ +(47) ___ = _____ C = 14 + (3) ___ - (8) ___ +(13) ___ - (18) ___ +(23) ___ - (28) ___ +(33) ___ - (38) ___ +(43) ___ +(48) ___ = _____ N = 38 - (4) ___ + (9) ___ -(14) ___ +(19) ___ -(24) ___ - (29) ___ -(34) ___ - (39) ___ -(44) ___ - (49) ___ = _____ O = 8 + (5) ___ - (10) ___ +(15) ___ - (20) ___ +(25) ___ - (30) ___ +(35) ___ +(40) ___ +(45) ___ +(50) ___ = _____  Where:\n E - Extraversion, A - Agreeableness, C - Conscientiousness, N - Emotional Stability, O - Intellect or Imagination (Openness)  (question number)\nsource: https://openpsychometrics.org/printable/big-five-personality-test.pdf\nThus, after substituting the markings which occurs in the data frame to the formulas, we get:\n E = 20 + EXT1 - EXT2 + EXT3 - EXT4 + EXT5 - EXT6 + EXT7 - EXT8 + EXT9 - EXT10 = A = 14 - AGR1 + AGR2 - AGR3 + AGR4 - AGR5 + AGR6 - AGR7 + AGR8 + AGR9 + AGR10 = C = 14 + CSN1 - CSN2 + CSN3 - CSN4 + CSN5 - CSN6 + CSN7 - CSN8 + CSN9 + CSN10 = N = 38 - EST1 + EST2 - EST3 + EST4 - EST5 - EST6 - EST7 - EST8 - EST9 - EST10 = O = 8 + OPN1 - OPN2 + OPN3 - OPN4 + OPN5 - OPN6 + OPN7 + OPN8 + OPN9 + OPN10 =  Preparation of additional columns Enter the calculated results into the data frame def getExtraversionScore(df): ExtraversionScore = 0 ExtraversionScore = 20 + df['EXT1'] - df['EXT2'] + df['EXT3'] - df['EXT4'] + df['EXT5'] - df['EXT6'] + df['EXT7'] - df['EXT8'] + df['EXT9'] - df['EXT10'] return ExtraversionScore def getAgreeablenessScore(df): AgreeablenessScore = 0 AgreeablenessScore = 14 - df['AGR1'] + df['AGR2'] - df['AGR3'] + df['AGR4'] - df['AGR5'] + df['AGR6'] - df['AGR7'] + df['AGR8'] + df['AGR9'] + df['AGR10'] return AgreeablenessScore def getConscientiousnessScore(df): ConscientiousnessScore = 0 ConscientiousnessScore = 14 + df['CSN1'] - df['CSN2'] + df['CSN3'] - df['CSN4'] + df['CSN5'] - df['CSN6'] + df['CSN7'] - df['CSN8'] + df['CSN9'] + df['CSN10'] return ConscientiousnessScore def getEmotionalStabilityScore(df): EmotionalStabilityScore = 0 EmotionalStabilityScore = 38 - df['EST1'] + df['EST2'] - df['EST3'] + df['EST4'] - df['EST5'] - df['EST6'] - df['EST7'] - df['EST8'] - df['EST9'] - df['EST10'] return EmotionalStabilityScore def getOpennessScore(df): OpennessScore = 0 OpennessScore = 8 + df['OPN1'] - df['OPN2'] + df['OPN3'] - df['OPN4'] + df['OPN5'] - df['OPN6'] + df['OPN7'] + df['OPN8'] + df['OPN9'] + df['OPN10'] return OpennessScore BigFive['Extraversion'] = BigFive.apply( getExtraversionScore, axis=1 ) BigFive['Agreeableness'] = BigFive.apply( getAgreeablenessScore, axis=1 ) BigFive['Conscientiousness'] = BigFive.apply( getConscientiousnessScore, axis=1 ) BigFive['EmotionalStability'] = BigFive.apply( getEmotionalStabilityScore, axis=1 ) BigFive['Openness'] = BigFive.apply( getOpennessScore, axis=1 )   BigFive.head()   | | EXT1 | EXT2 | EXT3 | EXT4 | EXT5 | EXT6 | EXT7 | EXT8 | EXT9 | EXT10 | EST1 | EST2 | EST3 | EST4 | EST5 | EST6 | EST7 | EST8 | EST9 | EST10 | AGR1 | AGR2 | AGR3 | AGR4 | AGR5 | AGR6 | AGR7 | AGR8 | AGR9 | AGR10 | CSN1 | CSN2 | CSN3 | CSN4 | CSN5 | CSN6 | CSN7 | CSN8 | CSN9 | CSN10 | OPN1 | OPN2 | OPN3 | OPN4 | OPN5 | OPN6 | OPN7 | OPN8 | OPN9 | OPN10 | dateload | introelapse | testelapse | endelapse | alpha2_code | lat_appx_lots_of_err | long_appx_lots_of_err | country | Extraversion | Agreeableness | Conscientiousness | EmotionalStability | Openness | |---|------|------|------|------|------|------|------|------|------|-------|------|------|------|------|------|------|------|------|------|-------|------|------|------|------|------|------|------|------|------|-------|------|------|------|------|------|------|------|------|------|-------|------|------|------|------|------|------|------|------|------|-------|---------------------|-------------|------------|-----------|-------------|----------------------|-----------------------|----------------|--------------|---------------|-------------------|--------------------|----------| | 0 | 4.0 | 1.0 | 5.0 | 2.0 | 5.0 | 1.0 | 5.0 | 2.0 | 4.0 | 1.0 | 1.0 | 4.0 | 4.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 3.0 | 2.0 | 2.0 | 5.0 | 2.0 | 4.0 | 2.0 | 3.0 | 2.0 | 4.0 | 3.0 | 4.0 | 3.0 | 4.0 | 3.0 | 2.0 | 2.0 | 4.0 | 4.0 | 2.0 | 4.0 | 4.0 | 5.0 | 1.0 | 4.0 | 1.0 | 4.0 | 1.0 | 5.0 | 3.0 | 4.0 | 5.0 | 2016-03-03 02:01:01 | 9.0 | 234.0 | 6 | GB | 51.5448 | 0.1991 | United Kingdom | 36.0 | 29.0 | 22.0 | 26.0 | 35.0 | | 1 | 3.0 | 5.0 | 3.0 | 4.0 | 3.0 | 3.0 | 2.0 | 5.0 | 1.0 | 5.0 | 2.0 | 3.0 | 4.0 | 1.0 | 3.0 | 1.0 | 2.0 | 1.0 | 3.0 | 1.0 | 1.0 | 4.0 | 1.0 | 5.0 | 1.0 | 5.0 | 3.0 | 4.0 | 5.0 | 3.0 | 3.0 | 2.0 | 5.0 | 3.0 | 3.0 | 1.0 | 3.0 | 3.0 | 5.0 | 3.0 | 1.0 | 2.0 | 4.0 | 2.0 | 3.0 | 1.0 | 4.0 | 2.0 | 5.0 | 3.0 | 2016-03-03 02:01:20 | 12.0 | 179.0 | 11 | MY | 3.1698 | 101.706 | Malaysia | 10.0 | 34.0 | 27.0 | 25.0 | 25.0 | | 2 | 2.0 | 3.0 | 4.0 | 4.0 | 3.0 | 2.0 | 1.0 | 3.0 | 2.0 | 5.0 | 4.0 | 4.0 | 4.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 1.0 | 3.0 | 1.0 | 4.0 | 1.0 | 4.0 | 2.0 | 4.0 | 1.0 | 4.0 | 4.0 | 3.0 | 4.0 | 2.0 | 2.0 | 2.0 | 3.0 | 3.0 | 4.0 | 2.0 | 4.0 | 2.0 | 5.0 | 1.0 | 2.0 | 1.0 | 4.0 | 2.0 | 5.0 | 3.0 | 4.0 | 4.0 | 2016-03-03 02:01:56 | 3.0 | 186.0 | 7 | GB | 54.9119 | -1.3833 | United Kingdom | 15.0 | 32.0 | 24.0 | 24.0 | 31.0 | | 3 | 2.0 | 2.0 | 2.0 | 3.0 | 4.0 | 2.0 | 2.0 | 4.0 | 1.0 | 4.0 | 3.0 | 3.0 | 3.0 | 2.0 | 3.0 | 2.0 | 2.0 | 2.0 | 4.0 | 3.0 | 2.0 | 4.0 | 3.0 | 4.0 | 2.0 | 4.0 | 2.0 | 4.0 | 3.0 | 4.0 | 2.0 | 4.0 | 4.0 | 4.0 | 1.0 | 2.0 | 2.0 | 3.0 | 1.0 | 4.0 | 4.0 | 2.0 | 5.0 | 2.0 | 3.0 | 1.0 | 4.0 | 4.0 | 3.0 | 3.0 | 2016-03-03 02:02:02 | 186.0 | 219.0 | 7 | GB | 51.75 | -1.25 | United Kingdom | 16.0 | 28.0 | 15.0 | 21.0 | 29.0 | | 5 | 3.0 | 3.0 | 4.0 | 2.0 | 4.0 | 2.0 | 2.0 | 3.0 | 3.0 | 4.0 | 3.0 | 4.0 | 3.0 | 2.0 | 2.0 | 1.0 | 2.0 | 1.0 | 2.0 | 2.0 | 2.0 | 3.0 | 1.0 | 4.0 | 2.0 | 3.0 | 2.0 | 3.0 | 4.0 | 4.0 | 3.0 | 2.0 | 4.0 | 1.0 | 3.0 | 2.0 | 4.0 | 3.0 | 4.0 | 3.0 | 5.0 | 1.0 | 5.0 | 1.0 | 3.0 | 1.0 | 5.0 | 4.0 | 5.0 | 2.0 | 2016-03-03 02:03:12 | 4.0 | 196.0 | 3 | SE | 59.3333 | 18.05 | Sweden | 22.0 | 28.0 | 27.0 | 28.0 | 34 |  Personality summary for the selected row in the radar diagram scoresRadar_row_0.create_chart(flag=1 )  scoresRadar_row_25.create_chart(flag=1 )  scoresRadar_row_100.create_chart(flag=1)  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/bigfive/project_bigfive_data_analysis/","summary":"Data analysis: Preparation of groups based on the name of the country countries_groups = BigFive.groupby(by='country') # How many groups are there?: len(countries_groups) 221   Countries_groups_counts = countries_groups.size().to_frame('count') Countries_groups_counts.reset_index(inplace=True) Countries_groups_counts.head()      country count     0 Afghanistan 46   1 Albania 353   2 Algeria 230   3 American Samoa 1   4 Andorra 15    Countries_groups_counts_descending = Countries_groups_counts.","tags":null,"title":"Big Five Personality test - data analysis"},{"categories":null,"contents":"Polish cities - data distribution First, let\u0026rsquo;s see what the distribution of data from Polish cities looks like.\nThere is a lot of data collected from the city of Warsaw. There are definitely more of them than for other cities. It can be concluded that the data is not properly balanced.\nPresentation of approximate locations in a scatterplot diagram I wanted to present approximate users locations gathered from Poland on scatterplot diagram. In order to do this, first I extracted group from main data frame, which contain informations only about Poland.\npol_data = countries_groups.get_group('Poland') pol_data.head(3)   | | EXT1 | EXT2 | EXT3 | EXT4 | EXT5 | EXT6 | EXT7 | EXT8 | EXT9 | EXT10 | EST1 | EST2 | EST3 | EST4 | EST5 | EST6 | EST7 | EST8 | EST9 | EST10 | AGR1 | AGR2 | AGR3 | AGR4 | AGR5 | AGR6 | AGR7 | AGR8 | AGR9 | AGR10 | CSN1 | CSN2 | CSN3 | CSN4 | CSN5 | CSN6 | CSN7 | CSN8 | CSN9 | CSN10 | OPN1 | OPN2 | OPN3 | OPN4 | OPN5 | OPN6 | OPN7 | OPN8 | OPN9 | OPN10 | dateload | introelapse | testelapse | endelapse | alpha2_code | lat_appx_lots_of_err | long_appx_lots_of_err | country | Extraversion | Agreeableness | Conscientiousness | EmotionalStability | Openness | |-----|------|------|------|------|------|------|------|------|------|-------|------|------|------|------|------|------|------|------|------|-------|------|------|------|------|------|------|------|------|------|-------|------|------|------|------|------|------|------|------|------|-------|------|------|------|------|------|------|------|------|------|-------|---------------------|-------------|------------|-----------|-------------|----------------------|-----------------------|---------|--------------|---------------|-------------------|--------------------|----------| | 222 | 1.0 | 5.0 | 2.0 | 5.0 | 1.0 | 5.0 | 1.0 | 4.0 | 1.0 | 5.0 | 1.0 | 3.0 | 2.0 | 1.0 | 1.0 | 2.0 | 4.0 | 4.0 | 2.0 | 5.0 | 4.0 | 1.0 | 3.0 | 2.0 | 3.0 | 4.0 | 4.0 | 2.0 | 4.0 | 4.0 | 2.0 | 5.0 | 5.0 | 5.0 | 4.0 | 1.0 | 1.0 | 2.0 | 1.0 | 4.0 | 5.0 | 1.0 | 4.0 | 2.0 | 3.0 | 1.0 | 4.0 | 4.0 | 5.0 | 5.0 | 2016-03-03 05:46:21 | 10.0 | 204.0 | 11 | PL | 51.2036 | 18.8997 | Poland | 2.0 | 17.0 | 18.0 | 21.0 | 34.0 | | 325 | 2.0 | 5.0 | 3.0 | 4.0 | 2.0 | 5.0 | 1.0 | 3.0 | 4.0 | 5.0 | 2.0 | 2.0 | 4.0 | 2.0 | 4.0 | 1.0 | 5.0 | 5.0 | 3.0 | 5.0 | 3.0 | 3.0 | 2.0 | 2.0 | 4.0 | 4.0 | 4.0 | 4.0 | 2.0 | 3.0 | 1.0 | 4.0 | 4.0 | 5.0 | 5.0 | 5.0 | 3.0 | 5.0 | 2.0 | 1.0 | 2.0 | 5.0 | 1.0 | 1.0 | 1.0 | 5.0 | 1.0 | 1.0 | 5.0 | 1.0 | 2016-03-03 06:58:57 | 41.0 | 308.0 | 21 | PL | 52.4167 | 16.9667 | Poland | 10.0 | 19.0 | 11.0 | 13.0 | 9.0 | | 376 | 2.0 | 3.0 | 2.0 | 5.0 | 2.0 | 1.0 | 1.0 | 5.0 | 2.0 | 5.0 | 4.0 | 3.0 | 5.0 | 3.0 | 3.0 | 5.0 | 1.0 | 1.0 | 5.0 | 3.0 | 3.0 | 5.0 | 1.0 | 4.0 | 2.0 | 4.0 | 3.0 | 5.0 | 4.0 | 3.0 | 3.0 | 0.0 | 5.0 | 4.0 | 2.0 | 2.0 | 3.0 | 4.0 | 1.0 | 4.0 | 4.0 | 1.0 | 5.0 | 2.0 | 5.0 | 2.0 | 4.0 | 2.0 | 5.0 | 5.0 | 2016-03-03 07:53:14 | 16.0 | 356.0 | 14 | PL | 52.25 | 21.0 | Poland | 10.0 | 30.0 | 22.0 | 17.0 | 33.0 |  It\u0026rsquo;s good to know, what are the data types in columns. Mainly, we have deal with float64 data type. However there are three data with data type category, one datetime64[ns] data type, one int64 data type and one object data type.\npol_data.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 4189 entries, 222 to 1015293 Data columns (total 63 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 EXT1 4189 non-null float64 1 EXT2 4189 non-null float64 2 EXT3 4189 non-null float64 3 EXT4 4189 non-null float64 4 EXT5 4189 non-null float64 5 EXT6 4189 non-null float64 6 EXT7 4189 non-null float64 7 EXT8 4189 non-null float64 8 EXT9 4189 non-null float64 9 EXT10 4189 non-null float64 10 EST1 4189 non-null float64 11 EST2 4189 non-null float64 12 EST3 4189 non-null float64 13 EST4 4189 non-null float64 14 EST5 4189 non-null float64 15 EST6 4189 non-null float64 16 EST7 4189 non-null float64 17 EST8 4189 non-null float64 18 EST9 4189 non-null float64 19 EST10 4189 non-null float64 20 AGR1 4189 non-null float64 21 AGR2 4189 non-null float64 22 AGR3 4189 non-null float64 23 AGR4 4189 non-null float64 24 AGR5 4189 non-null float64 25 AGR6 4189 non-null float64 26 AGR7 4189 non-null float64 27 AGR8 4189 non-null float64 28 AGR9 4189 non-null float64 29 AGR10 4189 non-null float64 30 CSN1 4189 non-null float64 31 CSN2 4189 non-null float64 32 CSN3 4189 non-null float64 33 CSN4 4189 non-null float64 34 CSN5 4189 non-null float64 35 CSN6 4189 non-null float64 36 CSN7 4189 non-null float64 37 CSN8 4189 non-null float64 38 CSN9 4189 non-null float64 39 CSN10 4189 non-null float64 40 OPN1 4189 non-null float64 41 OPN2 4189 non-null float64 42 OPN3 4189 non-null float64 43 OPN4 4189 non-null float64 44 OPN5 4189 non-null float64 45 OPN6 4189 non-null float64 46 OPN7 4189 non-null float64 47 OPN8 4189 non-null float64 48 OPN9 4189 non-null float64 49 OPN10 4189 non-null float64 50 dateload 4189 non-null datetime64[ns] 51 introelapse 4189 non-null float64 52 testelapse 4189 non-null float64 53 endelapse 4189 non-null int64 54 alpha2_code 4189 non-null category 55 lat_appx_lots_of_err 4189 non-null category 56 long_appx_lots_of_err 4189 non-null category 57 country 4189 non-null object 58 Extraversion 4189 non-null float64 59 Agreeableness 4189 non-null float64 60 Conscientiousness 4189 non-null float64 61 EmotionalStability 4189 non-null float64 62 Openness 4189 non-null float64 dtypes: category(3), datetime64[ns](1), float64(57), int64(1), object(1) memory usage: 5.0+ MB  We can use values from columns long_appx_lots_of_err and lat_appx_lots_of_err in order to draw approximate shape of Poland. The data type for this two columns is category. It is necessary to change type to float, in order to use values in scatterplot.\npol_data['long_appx_lots_of_err'] = pol_data['long_appx_lots_of_err'].astype('float') pol_data['lat_appx_lots_of_err'] = pol_data['lat_appx_lots_of_err'].astype('float')  I applied astype method with \u0026lsquo;float\u0026rsquo; as argument on data series. Results of this method was typed into long_appx_lots_of_err and lat_appx_lots_of_err columns.\ncols = 2 rows = 3 personality_list = pol_data.columns[-5:] color_list = ['Reds', 'Greens', 'Blues', 'Purples', 'YlOrBr'] cities_list =['Olsztyn', 'Gdańsk', 'Warszawa', 'Kraków', 'Wrocław', 'Rzeszów', 'Poznań'] index = 0 fig = plt.figure(figsize=(25, 28)) fig.subplots_adjust(hspace=0.1, wspace=0.1 ) for i in range(1, 6): fig.add_subplot(rows, cols, i) p1 = sns.scatterplot(x='long_appx_lots_of_err', y='lat_appx_lots_of_err', hue= personality_list[index], size= personality_list[index], data=pol_data, palette= color_list[index], sizes=(10,200), alpha=0.7 ) for i, city in enumerate(cities_list): loc = geolocator.geocode(city) city_coordinates = (loc.longitude, loc.latitude ) plt.annotate(city, xy= city_coordinates, xycoords='data', fontsize=15) index +=1 plt.show()  To have good orientation in data distribution on this plot, I\u0026rsquo;ve marked a few main cities. The values are more concentrated, where cities are located. However, If we look at this graph, there is no huge correlaction in living location with gained scores from the personality test. Maybe, I\u0026rsquo;ve to look deeper and compare individual cities with each other.\nCounted statistical values for personalities data personality_scores = pol_data.columns[-7:] pol_personality_scores = pol_data[personality_scores] pol_personality_scores.head()   | | Extraversion | Agreeableness | Conscientiousness | EmotionalStability | Openness | state | place | |-----|--------------|---------------|-------------------|--------------------|----------|---------------|--------------------| | 156 | 2.0 | 17.0 | 18.0 | 21.0 | 34.0 | łódzkie | powiat pajęczański | | 223 | 10.0 | 19.0 | 11.0 | 13.0 | 9.0 | wielkopolskie | Poznań | | 254 | 10.0 | 30.0 | 22.0 | 17.0 | 33.0 | mazowieckie | Warszawa | | 393 | 32.0 | 35.0 | 15.0 | 17.0 | 28.0 | mazowieckie | Warszawa | | 411 | 29.0 | 32.0 | 30.0 | 20.0 | 28.0 | mazowieckie | Warszawa |  Now, I would like to look at basic statistical values for the personalities cols. I use describe() method to get: count, mean, std, min, max values and quantiles for each column. Additionaly I transposed table to have better transparency at reading data.\npol_personality_scores.describe().T | | count | mean | std | min | 25% | 50% | 75% | max | |--------------------|--------|-----------|----------|------|------|------|------|------| | Extraversion | 4189.0 | 17.033660 | 9.341835 | -1.0 | 10.0 | 16.0 | 24.0 | 40.0 | | Agreeableness | 4189.0 | 23.841490 | 7.734842 | -1.0 | 19.0 | 24.0 | 30.0 | 40.0 | | Conscientiousness | 4189.0 | 20.909525 | 7.404900 | 0.0 | 16.0 | 21.0 | 26.0 | 40.0 | | EmotionalStability | 4189.0 | 17.297684 | 8.814573 | -1.0 | 11.0 | 17.0 | 23.0 | 40.0 | | Openness | 4189.0 | 30.747195 | 6.178097 | 0.0 | 27.0 | 31.0 | 35.0 | 41.0 |  In Poland, mean score value for extraversion and emotional stability is the lowest compared to other personalities, but standard deviation for extraversion is slightly higher from emotional stability.\nBox plots for personalities data: cols = ['Extraversion', 'Agreeableness', 'Conscientiousness', 'EmotionalStability', 'Openness'] colors = ['y', 'b', 'r', 'g', 'm'] fig, ax = plt.subplots(1, 5, figsize=(20,2), constrained_layout=True) for i, col in enumerate(cols): sns.boxplot( x=col, data=pol_personality_scores, ax=ax[i], color=colors[i]) plt.show()  There is a huge amount of outliers below minimum value for openness trait. Same as for openness trait, there are outliers for agreeableness trait and Conscientiousness trait. However, for conscientiousness trait we can notice only one outlier below minimum.\nHistograms for personalities data: fig, ax = plt.subplots(5, 5, figsize=(20,15), constrained_layout=True) bins_list = [5, 10, 15, 20, 25] for i, bin in enumerate(bins_list): for j, col in enumerate(cols): g = sns.histplot(data=pol_personality_scores, x =col, ax=ax[i, j], kde=True, bins=bins_list[i], color=colors[i] ) g.set( title=f'bins: {bin}' ) plt.show()   Thanks to box plots, we see on them values for every personality traits through their quartiles. In each voivodeship scores obtained from the personality test for openness trait are the highest and obtained extraversion trait points are the lowest. In every voivodeship, there are some outliers. The most outliers occur in openness trait.\nGeopandas maps: The Geopandas module allowed for the presentation of data from the personality test on a map.\nscoresRadar_pol_data_row_156 = RadarChart( rowNo= 156, data=pol_data ) scoresRadar_pol_data_row_223 = RadarChart( rowNo= 223, data=pol_data ) scoresRadar_pol_data_row_393 = RadarChart( rowNo= 393, data=pol_data )  Radar plots: scoresRadar_pol_data_row_156.create_chart(flag=0)  scoresRadar_pol_data_row_223.create_chart(flag=0)  scoresRadar_pol_data_row_393.create_chart(flag=0)  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/bigfive/project_bigfive_data_analysisforpoland/","summary":"Polish cities - data distribution First, let\u0026rsquo;s see what the distribution of data from Polish cities looks like.\nThere is a lot of data collected from the city of Warsaw. There are definitely more of them than for other cities. It can be concluded that the data is not properly balanced.\nPresentation of approximate locations in a scatterplot diagram I wanted to present approximate users locations gathered from Poland on scatterplot diagram.","tags":null,"title":"Big Five Personality test - data analysis for Poland"},{"categories":null,"contents":"uk_data.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 49662 entries, 0 to 693689 Data columns (total 62 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 country 49662 non-null object 1 EXT1 49662 non-null float32 2 EXT2 49662 non-null float32 3 EXT3 49662 non-null float32 4 EXT4 49662 non-null float32 5 EXT5 49662 non-null float32 6 EXT6 49662 non-null float32 7 EXT7 49662 non-null float32 8 EXT8 49662 non-null float32 9 EXT9 49662 non-null float32 10 EXT10 49662 non-null float32 11 EST1 49662 non-null float32 12 EST2 49662 non-null float32 13 EST3 49662 non-null float32 14 EST4 49662 non-null float32 15 EST5 49662 non-null float32 16 EST6 49662 non-null float32 17 EST7 49662 non-null float32 18 EST8 49662 non-null float32 19 EST9 49662 non-null float32 20 EST10 49662 non-null float32 21 AGR1 49662 non-null float32 22 AGR2 49662 non-null float32 23 AGR3 49662 non-null float32 24 AGR4 49662 non-null float32 25 AGR5 49662 non-null float32 26 AGR6 49662 non-null float32 27 AGR7 49662 non-null float32 28 AGR8 49662 non-null float32 29 AGR9 49662 non-null float32 30 AGR10 49662 non-null float32 31 CSN1 49662 non-null float32 32 CSN2 49662 non-null float32 33 CSN3 49662 non-null float32 34 CSN4 49662 non-null float32 35 CSN5 49662 non-null float32 36 CSN6 49662 non-null float32 37 CSN7 49662 non-null float32 38 CSN8 49662 non-null float32 39 CSN9 49662 non-null float32 40 CSN10 49662 non-null float32 41 OPN1 49662 non-null float32 42 OPN2 49662 non-null float32 43 OPN3 49662 non-null float32 44 OPN4 49662 non-null float32 45 OPN5 49662 non-null float32 46 OPN6 49662 non-null float32 47 OPN7 49662 non-null float32 48 OPN8 49662 non-null float32 49 OPN9 49662 non-null float32 50 OPN10 49662 non-null float32 51 dateload 49662 non-null datetime64[ns] 52 introelapse 49662 non-null float32 53 testelapse 49662 non-null float32 54 endelapse 49662 non-null int16 55 lat_appx_lots_of_err 49662 non-null category 56 long_appx_lots_of_err 49662 non-null category 57 Extraversion 49662 non-null float32 58 Agreeableness 49662 non-null float32 59 Conscientiousness 49662 non-null float32 60 EmotionalStability 49662 non-null float32 61 Openness 49662 non-null float32 dtypes: category(2), datetime64[ns](1), float32(57), int16(1), object(1) memory usage: 15.2+ MB   uk_data['long_appx_lots_of_err'] = uk_data['long_appx_lots_of_err'].astype('float') uk_data['lat_appx_lots_of_err'] = uk_data['lat_appx_lots_of_err'].astype('float')   %%time cols = 2 rows = 3 personality_list = pol_data.columns[-7:-2] color_list = ['Reds', 'Greens', 'Blues', 'Purples', 'YlOrBr'] cities_list =['London', 'Cambridge', 'Oxfordshire', 'Plymouth', 'Cardiff', 'Truro', 'Manchester', 'Liverpool', 'Edinburgh', 'Glasgow', 'Aberdeen','Belfast'] index = 0 fig = plt.figure(figsize=(30, 75)) fig.subplots_adjust(hspace=0.1, wspace=0.1 ) for i in range(1, 6): fig.add_subplot(rows, cols, i) p1 = sns.scatterplot(x='long_appx_lots_of_err', y='lat_appx_lots_of_err', hue= personality_list[index], size= personality_list[index], data=uk_data, palette= color_list[index], sizes=(10,200), alpha=0.5 ) for i, city in enumerate(cities_list): loc = geolocator.geocode(city) city_coordinates = (loc.longitude, loc.latitude ) plt.annotate(city, xy= city_coordinates, xycoords='data', fontsize=20 ) index +=1 plt.show()  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/bigfive/project_bigfive_data_analysisforuk/","summary":"uk_data.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 49662 entries, 0 to 693689 Data columns (total 62 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 country 49662 non-null object 1 EXT1 49662 non-null float32 2 EXT2 49662 non-null float32 3 EXT3 49662 non-null float32 4 EXT4 49662 non-null float32 5 EXT5 49662 non-null float32 6 EXT6 49662 non-null float32 7 EXT7 49662 non-null float32 8 EXT8 49662 non-null float32 9 EXT9 49662 non-null float32 10 EXT10 49662 non-null float32 11 EST1 49662 non-null float32 12 EST2 49662 non-null float32 13 EST3 49662 non-null float32 14 EST4 49662 non-null float32 15 EST5 49662 non-null float32 16 EST6 49662 non-null float32 17 EST7 49662 non-null float32 18 EST8 49662 non-null float32 19 EST9 49662 non-null float32 20 EST10 49662 non-null float32 21 AGR1 49662 non-null float32 22 AGR2 49662 non-null float32 23 AGR3 49662 non-null float32 24 AGR4 49662 non-null float32 25 AGR5 49662 non-null float32 26 AGR6 49662 non-null float32 27 AGR7 49662 non-null float32 28 AGR8 49662 non-null float32 29 AGR9 49662 non-null float32 30 AGR10 49662 non-null float32 31 CSN1 49662 non-null float32 32 CSN2 49662 non-null float32 33 CSN3 49662 non-null float32 34 CSN4 49662 non-null float32 35 CSN5 49662 non-null float32 36 CSN6 49662 non-null float32 37 CSN7 49662 non-null float32 38 CSN8 49662 non-null float32 39 CSN9 49662 non-null float32 40 CSN10 49662 non-null float32 41 OPN1 49662 non-null float32 42 OPN2 49662 non-null float32 43 OPN3 49662 non-null float32 44 OPN4 49662 non-null float32 45 OPN5 49662 non-null float32 46 OPN6 49662 non-null float32 47 OPN7 49662 non-null float32 48 OPN8 49662 non-null float32 49 OPN9 49662 non-null float32 50 OPN10 49662 non-null float32 51 dateload 49662 non-null datetime64[ns] 52 introelapse 49662 non-null float32 53 testelapse 49662 non-null float32 54 endelapse 49662 non-null int16 55 lat_appx_lots_of_err 49662 non-null category 56 long_appx_lots_of_err 49662 non-null category 57 Extraversion 49662 non-null float32 58 Agreeableness 49662 non-null float32 59 Conscientiousness 49662 non-null float32 60 EmotionalStability 49662 non-null float32 61 Openness 49662 non-null float32 dtypes: category(2), datetime64[ns](1), float32(57), int16(1), object(1) memory usage: 15.","tags":null,"title":"Big Five Personality test - data analysis for United Kingdom"},{"categories":null,"contents":"Data preparation: Data presentation BigFive_raw = pd.read_csv( '/content/drive/My Drive/ColabNotebooks/BigFivePersonality/data-final.csv', low_memory=False, sep='\\t', usecols=['EXT1', 'EXT2', 'EXT3', 'EXT4', 'EXT5', 'EXT6', 'EXT7', 'EXT8', 'EXT9', 'EXT10', 'EST1', 'EST2', 'EST3', 'EST4', 'EST5', 'EST6', 'EST7', 'EST8', 'EST9', 'EST10', 'AGR1', 'AGR2', 'AGR3', 'AGR4', 'AGR5', 'AGR6', 'AGR7', 'AGR8', 'AGR9', 'AGR10', 'CSN1', 'CSN2', 'CSN3', 'CSN4', 'CSN5', 'CSN6', 'CSN7', 'CSN8', 'CSN9', 'CSN10', 'OPN1', 'OPN2', 'OPN3', 'OPN4', 'OPN5', 'OPN6', 'OPN7', 'OPN8', 'OPN9', 'OPN10', # 'EXT1_E', 'EXT2_E', 'EXT3_E', 'EXT4_E', 'EXT5_E', 'EXT6_E', 'EXT7_E', 'EXT8_E', 'EXT9_E', 'EXT10_E', # 'EST1_E', 'EST2_E', 'EST3_E', 'EST4_E', 'EST5_E', 'EST6_E', 'EST7_E', 'EST8_E', 'EST9_E', 'EST10_E', # 'AGR1_E', 'AGR2_E', 'AGR3_E', 'AGR4_E', 'AGR5_E', 'AGR6_E', 'AGR7_E', 'AGR8_E', 'AGR9_E', 'AGR10_E', # 'CSN1_E', 'CSN2_E', 'CSN3_E', 'CSN4_E', 'CSN5_E', 'CSN6_E', 'CSN7_E', 'CSN8_E', 'CSN9_E', 'CSN10_E', # 'OPN1_E', 'OPN2_E', 'OPN3_E', 'OPN4_E', 'OPN5_E', 'OPN6_E', 'OPN7_E', 'OPN8_E', 'OPN9_E', 'OPN10_E', 'dateload', 'introelapse', 'testelapse', 'endelapse', 'IPC', 'country', 'lat_appx_lots_of_err', 'long_appx_lots_of_err' ] ) # Kopia danych BigFive = BigFive_raw.copy() BigFive.head()  This dataset consist 58 columns. Columns from 0 to 49 contain answers from personality test. Informations about the rest of columns are presented in the table below:\n   column name description     dateload The timestamp when the survey was started.   introelapse The time in seconds spent on the landing / intro page   testelapse The time in seconds spent on the page with the survey questions.   endelapse The time in seconds spent on the finalization page (where the user was asked to indicate if they has answered accurately and their answers could be stored and used for research.   IPC The number of records from the user\u0026rsquo;s IP address in the dataset.   country The country, determined by technical information (NOT ASKED AS A QUESTION)   lat_appx_lots_of_err Approximate latitude of user.   long_appx_lots_of_err Approximate longitude of user     EXT1 EXT2 EXT3 EXT4 EXT5 EXT6 EXT7 EXT8 EXT9 EXT10 EST1 EST2 EST3 EST4 EST5 EST6 EST7 EST8 EST9 EST10 AGR1 AGR2 AGR3 AGR4 AGR5 AGR6 AGR7 AGR8 AGR9 AGR10 CSN1 CSN2 CSN3 CSN4 CSN5 CSN6 CSN7 CSN8 CSN9 CSN10 OPN1 OPN2 OPN3 OPN4 OPN5 OPN6 OPN7 OPN8 OPN9 OPN10 dateload introelapse testelapse endelapse IPC country lat_appx_lots_of_err long_appx_lots_of_err 0 4.0 1.0 5.0 2.0 5.0 1.0 5.0 2.0 4.0 1.0 1.0 4.0 4.0 2.0 2.0 2.0 2.0 2.0 3.0 2.0 2.0 5.0 2.0 4.0 2.0 3.0 2.0 4.0 3.0 4.0 3.0 4.0 3.0 2.0 2.0 4.0 4.0 2.0 4.0 4.0 5.0 1.0 4.0 1.0 4.0 1.0 5.0 3.0 4.0 5.0 2016-03-03 02:01:01 9.0 234.0 6 1 GB 51.5448 0.1991 1 3.0 5.0 3.0 4.0 3.0 3.0 2.0 5.0 1.0 5.0 2.0 3.0 4.0 1.0 3.0 1.0 2.0 1.0 3.0 1.0 1.0 4.0 1.0 5.0 1.0 5.0 3.0 4.0 5.0 3.0 3.0 2.0 5.0 3.0 3.0 1.0 3.0 3.0 5.0 3.0 1.0 2.0 4.0 2.0 3.0 1.0 4.0 2.0 5.0 3.0 2016-03-03 02:01:20 12.0 179.0 11 1 MY 3.1698 101.706 2 2.0 3.0 4.0 4.0 3.0 2.0 1.0 3.0 2.0 5.0 4.0 4.0 4.0 2.0 2.0 2.0 2.0 2.0 1.0 3.0 1.0 4.0 1.0 4.0 2.0 4.0 1.0 4.0 4.0 3.0 4.0 2.0 2.0 2.0 3.0 3.0 4.0 2.0 4.0 2.0 5.0 1.0 2.0 1.0 4.0 2.0 5.0 3.0 4.0 4.0 2016-03-03 02:01:56 3.0 186.0 7 1 GB 54.9119 -1.3833 3 2.0 2.0 2.0 3.0 4.0 2.0 2.0 4.0 1.0 4.0 3.0 3.0 3.0 2.0 3.0 2.0 2.0 2.0 4.0 3.0 2.0 4.0 3.0 4.0 2.0 4.0 2.0 4.0 3.0 4.0 2.0 4.0 4.0 4.0 1.0 2.0 2.0 3.0 1.0 4.0 4.0 2.0 5.0 2.0 3.0 1.0 4.0 4.0 3.0 3.0 2016-03-03 02:02:02 186.0 219.0 7 1 GB 51.75 -1.25 4 3.0 3.0 3.0 3.0 5.0 3.0 3.0 5.0 3.0 4.0 1.0 5.0 5.0 3.0 1.0 1.0 1.0 1.0 3.0 2.0 1.0 5.0 1.0 5.0 1.0 3.0 1.0 5.0 5.0 3.0 5.0 1.0 5.0 1.0 3.0 1.0 5.0 1.0 5.0 5.0 5.0 1.0 5.0 1.0 5.0 1.0 5.0 3.0 5.0 5.0 2016-03-03 02:02:57 8.0 315.0 17 2 KE 1.0 38.0   Dataset are using about 450MB of memory. Here, there are 52 columns of float64 type, 2 cols of int64 type and 4 cols of object type.\nBigFive.info(memory_usage='Deep') \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 1015341 entries, 0 to 1015340 Data columns (total 58 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 EXT1 1013558 non-null float64 1 EXT2 1013558 non-null float64 2 EXT3 1013558 non-null float64 3 EXT4 1013558 non-null float64 4 EXT5 1013558 non-null float64 5 EXT6 1013558 non-null float64 6 EXT7 1013558 non-null float64 7 EXT8 1013558 non-null float64 8 EXT9 1013558 non-null float64 9 EXT10 1013558 non-null float64 10 EST1 1013558 non-null float64 11 EST2 1013558 non-null float64 12 EST3 1013558 non-null float64 13 EST4 1013558 non-null float64 14 EST5 1013558 non-null float64 15 EST6 1013558 non-null float64 16 EST7 1013558 non-null float64 17 EST8 1013558 non-null float64 18 EST9 1013558 non-null float64 19 EST10 1013558 non-null float64 20 AGR1 1013558 non-null float64 21 AGR2 1013558 non-null float64 22 AGR3 1013558 non-null float64 23 AGR4 1013558 non-null float64 24 AGR5 1013558 non-null float64 25 AGR6 1013558 non-null float64 26 AGR7 1013558 non-null float64 27 AGR8 1013558 non-null float64 28 AGR9 1013558 non-null float64 29 AGR10 1013558 non-null float64 30 CSN1 1013558 non-null float64 31 CSN2 1013558 non-null float64 32 CSN3 1013558 non-null float64 33 CSN4 1013558 non-null float64 34 CSN5 1013558 non-null float64 35 CSN6 1013558 non-null float64 36 CSN7 1013558 non-null float64 37 CSN8 1013558 non-null float64 38 CSN9 1013558 non-null float64 39 CSN10 1013558 non-null float64 40 OPN1 1013558 non-null float64 41 OPN2 1013558 non-null float64 42 OPN3 1013558 non-null float64 43 OPN4 1013558 non-null float64 44 OPN5 1013558 non-null float64 45 OPN6 1013558 non-null float64 46 OPN7 1013558 non-null float64 47 OPN8 1013558 non-null float64 48 OPN9 1013558 non-null float64 49 OPN10 1013558 non-null float64 50 dateload 1015341 non-null object 51 introelapse 1013275 non-null float64 52 testelapse 1013558 non-null float64 53 endelapse 1015341 non-null int64 54 IPC 1015341 non-null int64 55 country 1015264 non-null object 56 lat_appx_lots_of_err 1015341 non-null object 57 long_appx_lots_of_err 1015341 non-null object dtypes: float64(52), int64(2), object(4) memory usage: 449.3+ MB  Save data to parquet:  # Save data to parquet BigFive.to_parquet('/content/drive/My Drive/ColabNotebooks/BigFivePersonality/data-final.gzip', compression='gzip')   # Load parquet data %%time BigFive = pd.read_parquet('/content/drive/My Drive/ColabNotebooks/BigFivePersonality/data-final.gzip' ) CPU times: user 1.74 s, sys: 485 ms, total: 2.23 s Wall time: 2.11 s   Changing the data types BigFive.columns[:50]   Index(['EXT1', 'EXT2', 'EXT3', 'EXT4', 'EXT5', 'EXT6', 'EXT7', 'EXT8', 'EXT9', 'EXT10', 'EST1', 'EST2', 'EST3', 'EST4', 'EST5', 'EST6', 'EST7', 'EST8', 'EST9', 'EST10', 'AGR1', 'AGR2', 'AGR3', 'AGR4', 'AGR5', 'AGR6', 'AGR7', 'AGR8', 'AGR9', 'AGR10', 'CSN1', 'CSN2', 'CSN3', 'CSN4', 'CSN5', 'CSN6', 'CSN7', 'CSN8', 'CSN9', 'CSN10', 'OPN1', 'OPN2', 'OPN3', 'OPN4', 'OPN5', 'OPN6', 'OPN7', 'OPN8', 'OPN9', 'OPN10'], dtype='object')   def change_dtypes(df): float_cols = [ col for col in df if df[col].dtype == 'float64'] df[float_cols] = df[float_cols].astype(np.float32) int_cols = [ col for col in df if df[col].dtype == 'int64'] df[int_cols] = df[int_cols].astype(np.int16) return df BigFive = change_dtypes(BigFive)   cols_name_list = ['country', 'lat_appx_lots_of_err', 'long_appx_lots_of_err', 'dateload'] for i, col_name in enumerate(cols_name_list): if col_name == 'dateload': BigFive['dateload'] = pd.to_datetime(BigFive['dateload']) else: BigFive[col_name] = BigFive[col_name].astype('category') # BigFive['long_appx_lots_of_err'] = BigFive['long_appx_lots_of_err'].astype('float') # BigFive['lat_appx_lots_of_err'] = BigFive['lat_appx_lots_of_err'].astype('float') BigFive.info(memory_usage='Deep')  Results\n\u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 1015341 entries, 0 to 1015340 Data columns (total 58 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 EXT1 1013558 non-null float64 1 EXT2 1013558 non-null float64 2 EXT3 1013558 non-null float64 3 EXT4 1013558 non-null float64 4 EXT5 1013558 non-null float64 5 EXT6 1013558 non-null float64 6 EXT7 1013558 non-null float64 7 EXT8 1013558 non-null float64 8 EXT9 1013558 non-null float64 9 EXT10 1013558 non-null float64 10 EST1 1013558 non-null float64 11 EST2 1013558 non-null float64 12 EST3 1013558 non-null float64 13 EST4 1013558 non-null float64 14 EST5 1013558 non-null float64 15 EST6 1013558 non-null float64 16 EST7 1013558 non-null float64 17 EST8 1013558 non-null float64 18 EST9 1013558 non-null float64 19 EST10 1013558 non-null float64 20 AGR1 1013558 non-null float64 21 AGR2 1013558 non-null float64 22 AGR3 1013558 non-null float64 23 AGR4 1013558 non-null float64 24 AGR5 1013558 non-null float64 25 AGR6 1013558 non-null float64 26 AGR7 1013558 non-null float64 27 AGR8 1013558 non-null float64 28 AGR9 1013558 non-null float64 29 AGR10 1013558 non-null float64 30 CSN1 1013558 non-null float64 31 CSN2 1013558 non-null float64 32 CSN3 1013558 non-null float64 33 CSN4 1013558 non-null float64 34 CSN5 1013558 non-null float64 35 CSN6 1013558 non-null float64 36 CSN7 1013558 non-null float64 37 CSN8 1013558 non-null float64 38 CSN9 1013558 non-null float64 39 CSN10 1013558 non-null float64 40 OPN1 1013558 non-null float64 41 OPN2 1013558 non-null float64 42 OPN3 1013558 non-null float64 43 OPN4 1013558 non-null float64 44 OPN5 1013558 non-null float64 45 OPN6 1013558 non-null float64 46 OPN7 1013558 non-null float64 47 OPN8 1013558 non-null float64 48 OPN9 1013558 non-null float64 49 OPN10 1013558 non-null float64 50 dateload 1015341 non-null datetime64[ns] 51 introelapse 1013275 non-null float64 52 testelapse 1013558 non-null float64 53 endelapse 1015341 non-null int64 54 IPC 1015341 non-null int64 55 country 1015264 non-null category 56 lat_appx_lots_of_err 1015341 non-null category 57 long_appx_lots_of_err 1015341 non-null category dtypes: category(3), datetime64[ns](1), float64(52), int64(2) memory usage: 434.9 MB  Missing values BigFive.isnull().sum()   EXT1 1783 EXT2 1783 EXT3 1783 EXT4 1783 EXT5 1783 EXT6 1783 EXT7 1783 EXT8 1783 EXT9 1783 EXT10 1783 EST1 1783 EST2 1783 EST3 1783 EST4 1783 EST5 1783 EST6 1783 EST7 1783 EST8 1783 EST9 1783 EST10 1783 AGR1 1783 AGR2 1783 AGR3 1783 AGR4 1783 AGR5 1783 AGR6 1783 AGR7 1783 AGR8 1783 AGR9 1783 AGR10 1783 CSN1 1783 CSN2 1783 CSN3 1783 CSN4 1783 CSN5 1783 CSN6 1783 CSN7 1783 CSN8 1783 CSN9 1783 CSN10 1783 OPN1 1783 OPN2 1783 OPN3 1783 OPN4 1783 OPN5 1783 OPN6 1783 OPN7 1783 OPN8 1783 OPN9 1783 OPN10 1783 dateload 0 introelapse 2066 testelapse 1783 endelapse 0 IPC 0 country 77 lat_appx_lots_of_err 0 long_appx_lots_of_err 0 dtype: int64   BigFive[pd.isnull(BigFive['EXT1'])].head()    EXT1 EXT2 EXT3 EXT4 EXT5 EXT6 EXT7 EXT8 EXT9 EXT10 EST1 EST2 EST3 EST4 EST5 EST6 EST7 EST8 EST9 EST10 AGR1 AGR2 AGR3 AGR4 AGR5 AGR6 AGR7 AGR8 AGR9 AGR10 CSN1 CSN2 CSN3 CSN4 CSN5 CSN6 CSN7 CSN8 CSN9 CSN10 OPN1 OPN2 OPN3 OPN4 OPN5 OPN6 OPN7 OPN8 OPN9 OPN10 dateload introelapse testelapse endelapse IPC country lat_appx_lots_of_err long_appx_lots_of_err 78795 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2016-05-03 11:12:38 30.0 NaN 133 1 US 38.0 -97.0 78854 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2016-05-03 12:02:08 41.0 NaN 25 32 US 26.5584 -81.8997 78889 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2016-05-03 12:19:04 15.0 NaN 11 3 US 28.0222 -81.7329 153202 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2016-07-12 01:42:46 5.0 NaN 64279 77 SG 1.3667 103.8 153204 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2016-07-12 01:43:12 22.0 NaN 55 77 SG 1.3667 103.8   BigFive = BigFive.dropna() BigFive.info(memory_usage='Deep')   \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 1012050 entries, 0 to 1015340 Data columns (total 58 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 EXT1 1012050 non-null float64 1 EXT2 1012050 non-null float64 2 EXT3 1012050 non-null float64 3 EXT4 1012050 non-null float64 4 EXT5 1012050 non-null float64 5 EXT6 1012050 non-null float64 6 EXT7 1012050 non-null float64 7 EXT8 1012050 non-null float64 8 EXT9 1012050 non-null float64 9 EXT10 1012050 non-null float64 10 EST1 1012050 non-null float64 11 EST2 1012050 non-null float64 12 EST3 1012050 non-null float64 13 EST4 1012050 non-null float64 14 EST5 1012050 non-null float64 15 EST6 1012050 non-null float64 16 EST7 1012050 non-null float64 17 EST8 1012050 non-null float64 18 EST9 1012050 non-null float64 19 EST10 1012050 non-null float64 20 AGR1 1012050 non-null float64 21 AGR2 1012050 non-null float64 22 AGR3 1012050 non-null float64 23 AGR4 1012050 non-null float64 24 AGR5 1012050 non-null float64 25 AGR6 1012050 non-null float64 26 AGR7 1012050 non-null float64 27 AGR8 1012050 non-null float64 28 AGR9 1012050 non-null float64 29 AGR10 1012050 non-null float64 30 CSN1 1012050 non-null float64 31 CSN2 1012050 non-null float64 32 CSN3 1012050 non-null float64 33 CSN4 1012050 non-null float64 34 CSN5 1012050 non-null float64 35 CSN6 1012050 non-null float64 36 CSN7 1012050 non-null float64 37 CSN8 1012050 non-null float64 38 CSN9 1012050 non-null float64 39 CSN10 1012050 non-null float64 40 OPN1 1012050 non-null float64 41 OPN2 1012050 non-null float64 42 OPN3 1012050 non-null float64 43 OPN4 1012050 non-null float64 44 OPN5 1012050 non-null float64 45 OPN6 1012050 non-null float64 46 OPN7 1012050 non-null float64 47 OPN8 1012050 non-null float64 48 OPN9 1012050 non-null float64 49 OPN10 1012050 non-null float64 50 dateload 1012050 non-null datetime64[ns] 51 introelapse 1012050 non-null float64 52 testelapse 1012050 non-null float64 53 endelapse 1012050 non-null int64 54 IPC 1012050 non-null int64 55 country 1012050 non-null category 56 lat_appx_lots_of_err 1012050 non-null category 57 long_appx_lots_of_err 1012050 non-null category dtypes: category(3), datetime64[ns](1), float64(52), int64(2) memory usage: 441.2 MB  Choosing rows where IPC != 1 For max cleanliness, only use records where this value is 1. High values can be because of shared networks (e.g. entire universities) or multiple submissions.\nBigFive.IPC != 1 0 False 1 False 2 False 3 False 4 True ... 1015336 True 1015337 False 1015338 True 1015339 False 1015340 False Name: IPC, Length: 1012050, dtype: bool   IPC_index = BigFive[BigFive.IPC != 1].index BigFive.drop(IPC_index , inplace=True) BigFive.IPC != 1 0 False 1 False 2 False 3 False 5 False ... 1015334 False 1015335 False 1015337 False 1015339 False 1015340 False Name: IPC, Length: 694886, dtype: bool   BigFive.drop(['IPC'], 1, inplace=True) BigFive.head(3)    EXT1 EXT2 EXT3 EXT4 EXT5 EXT6 EXT7 EXT8 EXT9 EXT10 EST1 EST2 EST3 EST4 EST5 EST6 EST7 EST8 EST9 EST10 AGR1 AGR2 AGR3 AGR4 AGR5 AGR6 AGR7 AGR8 AGR9 AGR10 CSN1 CSN2 CSN3 CSN4 CSN5 CSN6 CSN7 CSN8 CSN9 CSN10 OPN1 OPN2 OPN3 OPN4 OPN5 OPN6 OPN7 OPN8 OPN9 OPN10 dateload introelapse testelapse endelapse country lat_appx_lots_of_err long_appx_lots_of_err 0 4.0 1.0 5.0 2.0 5.0 1.0 5.0 2.0 4.0 1.0 1.0 4.0 4.0 2.0 2.0 2.0 2.0 2.0 3.0 2.0 2.0 5.0 2.0 4.0 2.0 3.0 2.0 4.0 3.0 4.0 3.0 4.0 3.0 2.0 2.0 4.0 4.0 2.0 4.0 4.0 5.0 1.0 4.0 1.0 4.0 1.0 5.0 3.0 4.0 5.0 2016-03-03 02:01:01 9.0 234.0 6 GB 51.5448 0.1991 1 3.0 5.0 3.0 4.0 3.0 3.0 2.0 5.0 1.0 5.0 2.0 3.0 4.0 1.0 3.0 1.0 2.0 1.0 3.0 1.0 1.0 4.0 1.0 5.0 1.0 5.0 3.0 4.0 5.0 3.0 3.0 2.0 5.0 3.0 3.0 1.0 3.0 3.0 5.0 3.0 1.0 2.0 4.0 2.0 3.0 1.0 4.0 2.0 5.0 3.0 2016-03-03 02:01:20 12.0 179.0 11 MY 3.1698 101.706 2 2.0 3.0 4.0 4.0 3.0 2.0 1.0 3.0 2.0 5.0 4.0 4.0 4.0 2.0 2.0 2.0 2.0 2.0 1.0 3.0 1.0 4.0 1.0 4.0 2.0 4.0 1.0 4.0 4.0 3.0 4.0 2.0 2.0 2.0 3.0 3.0 4.0 2.0 4.0 2.0 5.0 1.0 2.0 1.0 4.0 2.0 5.0 3.0 4.0 4.0 2016-03-03 02:01:56 3.0 186.0 7 GB 54.9119 -1.3833  Are there any null values? if0_df = BigFive.loc[(BigFive['introelapse']==0) | (BigFive['testelapse']==0 )| (BigFive['endelapse']==0) ] len(if0_df) 21   if0_df.head(3)  Result:\n EXT1 EXT2 EXT3 EXT4 EXT5 EXT6 EXT7 EXT8 EXT9 EXT10 EST1 EST2 EST3 EST4 EST5 EST6 EST7 EST8 EST9 EST10 AGR1 AGR2 AGR3 AGR4 AGR5 AGR6 AGR7 AGR8 AGR9 AGR10 CSN1 CSN2 CSN3 CSN4 CSN5 CSN6 CSN7 CSN8 CSN9 CSN10 OPN1 OPN2 OPN3 OPN4 OPN5 OPN6 OPN7 OPN8 OPN9 OPN10 dateload introelapse testelapse endelapse country lat_appx_lots_of_err long_appx_lots_of_err 46867 4.0 3.0 5.0 1.0 5.0 4.0 5.0 5.0 1.0 1.0 2.0 3.0 1.0 3.0 3.0 2.0 3.0 3.0 2.0 1.0 5.0 1.0 3.0 1.0 3.0 1.0 3.0 3.0 2.0 1.0 2.0 5.0 3.0 1.0 5.0 4.0 3.0 3.0 3.0 1.0 2.0 5.0 1.0 5.0 3.0 5.0 3.0 1.0 1.0 1.0 2016-04-08 12:51:17 0.0 158.0 14 US 37.526 -122.3558 50094 1.0 5.0 2.0 5.0 1.0 5.0 1.0 5.0 1.0 5.0 3.0 3.0 3.0 2.0 1.0 2.0 1.0 2.0 4.0 1.0 4.0 3.0 2.0 2.0 3.0 3.0 5.0 3.0 1.0 3.0 4.0 1.0 5.0 1.0 2.0 2.0 5.0 2.0 3.0 5.0 5.0 1.0 5.0 1.0 5.0 1.0 5.0 4.0 5.0 5.0 2016-04-09 21:18:15 0.0 208.0 12 US 37.526 -122.3558 57062 3.0 1.0 5.0 2.0 4.0 4.0 5.0 4.0 3.0 4.0 1.0 5.0 1.0 0.0 2.0 2.0 1.0 1.0 3.0 1.0 1.0 5.0 1.0 5.0 5.0 5.0 1.0 5.0 4.0 4.0 5.0 1.0 5.0 1.0 5.0 2.0 5.0 2.0 5.0 5.0 5.0 3.0 3.0 3.0 4.0 2.0 5.0 3.0 4.0 5.0 2016-04-14 04:15:07 0.0 872.0 31 NO 59.95 10.75   BigFive.loc[(BigFive['introelapse']==0)] len(BigFive.loc[(BigFive['introelapse']==0)]) 21 BigFive.loc[(BigFive['lat_appx_lots_of_err']==0) | (BigFive['long_appx_lots_of_err']==0 )] len(BigFive.loc[(BigFive['lat_appx_lots_of_err']==0) | (BigFive['long_appx_lots_of_err']==0 )]) 0 BigFive.loc[ (BigFive['dateload']==0) ] len(BigFive.loc[ (BigFive['dateload']==0) ]) 0   for i, col_name in enumerate(BigFive.columns[:-7]): BigFive.loc[ (BigFive[col_name]==0) ] print(f'{col_name}: {len(BigFive.loc[ (BigFive[col_name]==0) ])} ')   EXT1: 2247 EXT2: 3782 EXT3: 3763 EXT4: 4302 EXT5: 6277 EXT6: 3904 EXT7: 5240 EXT8: 3995 EXT9: 4386 EXT10: 3967 EST1: 4720 EST2: 6300 EST3: 3410 EST4: 6932 EST5: 3811 EST6: 7164 EST7: 3986 EST8: 4749 EST9: 4020 EST10: 4574 AGR1: 3072 AGR2: 5985 AGR3: 3083 AGR4: 4821 AGR5: 4087 AGR6: 6772 AGR7: 3953 AGR8: 5473 AGR9: 3815 AGR10: 5349 CSN1: 7559 CSN2: 4078 CSN3: 3762 CSN4: 4341 CSN5: 4980 CSN6: 4168 CSN7: 4624 CSN8: 5189 CSN9: 4800 CSN10: 5486 OPN1: 5753 OPN2: 3653 OPN3: 4819 OPN4: 4160 OPN5: 4916 OPN6: 5198 OPN7: 5389 OPN8: 5203 OPN9: 4822 OPN10: 3976   quest_codes = BigFive.columns[:-7] quest_codes Index(['EXT1', 'EXT2', 'EXT3', 'EXT4', 'EXT5', 'EXT6', 'EXT7', 'EXT8', 'EXT9', 'EXT10', 'EST1', 'EST2', 'EST3', 'EST4', 'EST5', 'EST6', 'EST7', 'EST8', 'EST9', 'EST10', 'AGR1', 'AGR2', 'AGR3', 'AGR4', 'AGR5', 'AGR6', 'AGR7', 'AGR8', 'AGR9', 'AGR10', 'CSN1', 'CSN2', 'CSN3', 'CSN4', 'CSN5', 'CSN6', 'CSN7', 'CSN8', 'CSN9', 'CSN10', 'OPN1', 'OPN2', 'OPN3', 'OPN4', 'OPN5', 'OPN6', 'OPN7', 'OPN8', 'OPN9', 'OPN10'], dtype='object')   # Ile jest wierszy, gdzie dla wszystkich pytań jest wartość 0 IndexZeroQuestion = BigFive.loc[ ( BigFive['EXT1'] == 0 ) \u0026amp; ( BigFive['EXT2'] == 0 ) \u0026amp; ( BigFive['EXT3'] == 0 ) \u0026amp; ( BigFive['EXT4'] == 0 ) \u0026amp; ( BigFive['EXT5'] == 0 ) \u0026amp; ( BigFive['EXT6'] == 0 ) \u0026amp; ( BigFive['EXT7'] == 0 ) \u0026amp; ( BigFive['EXT8'] == 0 ) \u0026amp; ( BigFive['EXT9'] == 0 ) \u0026amp; ( BigFive['EXT10'] == 0 ) \u0026amp; ( BigFive['EST1'] == 0 ) \u0026amp; ( BigFive['EST2'] == 0 ) \u0026amp; ( BigFive['EST3'] == 0 ) \u0026amp; ( BigFive['EST4'] == 0 ) \u0026amp; ( BigFive['EST5'] == 0 ) \u0026amp; ( BigFive['EST6'] == 0 ) \u0026amp; ( BigFive['EST7'] == 0 ) \u0026amp; ( BigFive['EST8'] == 0 ) \u0026amp; ( BigFive['EST9'] == 0 ) \u0026amp; ( BigFive['EST10'] == 0 ) \u0026amp; ( BigFive['AGR1'] == 0 ) \u0026amp; ( BigFive['AGR2'] == 0 ) \u0026amp; ( BigFive['AGR3'] == 0 ) \u0026amp; ( BigFive['AGR4'] == 0 ) \u0026amp; ( BigFive['AGR5'] == 0 ) \u0026amp; ( BigFive['AGR6'] == 0 ) \u0026amp; ( BigFive['AGR7'] == 0 ) \u0026amp; ( BigFive['AGR8'] == 0 ) \u0026amp; ( BigFive['AGR9'] == 0 ) \u0026amp; ( BigFive['AGR10'] == 0 ) \u0026amp; ( BigFive['CSN1'] == 0 ) \u0026amp; ( BigFive['CSN2'] == 0 ) \u0026amp; ( BigFive['CSN3'] == 0 ) \u0026amp; ( BigFive['CSN4'] == 0 ) \u0026amp; ( BigFive['CSN5'] == 0 ) \u0026amp; ( BigFive['CSN6'] == 0 ) \u0026amp; ( BigFive['CSN7'] == 0 ) \u0026amp; ( BigFive['CSN8'] == 0 ) \u0026amp; ( BigFive['CSN9'] == 0 ) \u0026amp; ( BigFive['CSN10'] == 0 ) \u0026amp; ( BigFive['OPN1'] == 0 ) \u0026amp; ( BigFive['OPN2'] == 0 ) \u0026amp; ( BigFive['OPN3'] == 0 ) \u0026amp; ( BigFive['OPN4'] == 0 ) \u0026amp; ( BigFive['OPN5'] == 0 ) \u0026amp; ( BigFive['OPN6'] == 0 ) \u0026amp; ( BigFive['OPN7'] == 0 ) \u0026amp; ( BigFive['OPN8'] == 0 ) \u0026amp; ( BigFive['OPN9'] == 0 ) \u0026amp; ( BigFive['OPN10'] == 0 ) ].index len(IndexZeroQuestion) 1192   BigFive.drop(IndexZeroQuestion, inplace=True) BigFive.head()    EXT1 EXT2 EXT3 EXT4 EXT5 EXT6 EXT7 EXT8 EXT9 EXT10 EST1 EST2 EST3 EST4 EST5 EST6 EST7 EST8 EST9 EST10 AGR1 AGR2 AGR3 AGR4 AGR5 AGR6 AGR7 AGR8 AGR9 AGR10 CSN1 CSN2 CSN3 CSN4 CSN5 CSN6 CSN7 CSN8 CSN9 CSN10 OPN1 OPN2 OPN3 OPN4 OPN5 OPN6 OPN7 OPN8 OPN9 OPN10 dateload introelapse testelapse endelapse country lat_appx_lots_of_err long_appx_lots_of_err 0 4.0 1.0 5.0 2.0 5.0 1.0 5.0 2.0 4.0 1.0 1.0 4.0 4.0 2.0 2.0 2.0 2.0 2.0 3.0 2.0 2.0 5.0 2.0 4.0 2.0 3.0 2.0 4.0 3.0 4.0 3.0 4.0 3.0 2.0 2.0 4.0 4.0 2.0 4.0 4.0 5.0 1.0 4.0 1.0 4.0 1.0 5.0 3.0 4.0 5.0 2016-03-03 02:01:01 9.0 234.0 6 GB 51.5448 0.1991 1 3.0 5.0 3.0 4.0 3.0 3.0 2.0 5.0 1.0 5.0 2.0 3.0 4.0 1.0 3.0 1.0 2.0 1.0 3.0 1.0 1.0 4.0 1.0 5.0 1.0 5.0 3.0 4.0 5.0 3.0 3.0 2.0 5.0 3.0 3.0 1.0 3.0 3.0 5.0 3.0 1.0 2.0 4.0 2.0 3.0 1.0 4.0 2.0 5.0 3.0 2016-03-03 02:01:20 12.0 179.0 11 MY 3.1698 101.706 2 2.0 3.0 4.0 4.0 3.0 2.0 1.0 3.0 2.0 5.0 4.0 4.0 4.0 2.0 2.0 2.0 2.0 2.0 1.0 3.0 1.0 4.0 1.0 4.0 2.0 4.0 1.0 4.0 4.0 3.0 4.0 2.0 2.0 2.0 3.0 3.0 4.0 2.0 4.0 2.0 5.0 1.0 2.0 1.0 4.0 2.0 5.0 3.0 4.0 4.0 2016-03-03 02:01:56 3.0 186.0 7 GB 54.9119 -1.3833 3 2.0 2.0 2.0 3.0 4.0 2.0 2.0 4.0 1.0 4.0 3.0 3.0 3.0 2.0 3.0 2.0 2.0 2.0 4.0 3.0 2.0 4.0 3.0 4.0 2.0 4.0 2.0 4.0 3.0 4.0 2.0 4.0 4.0 4.0 1.0 2.0 2.0 3.0 1.0 4.0 4.0 2.0 5.0 2.0 3.0 1.0 4.0 4.0 3.0 3.0 2016-03-03 02:02:02 186.0 219.0 7 GB 51.75 -1.25 5 3.0 3.0 4.0 2.0 4.0 2.0 2.0 3.0 3.0 4.0 3.0 4.0 3.0 2.0 2.0 1.0 2.0 1.0 2.0 2.0 2.0 3.0 1.0 4.0 2.0 3.0 2.0 3.0 4.0 4.0 3.0 2.0 4.0 1.0 3.0 2.0 4.0 3.0 4.0 3.0 5.0 1.0 5.0 1.0 3.0 1.0 5.0 4.0 5.0 2.0 2016-03-03 02:03:12 4.0 196.0 3 SE 59.3333 18.05  Renaming column \u0026lsquo;country\u0026rsquo; to \u0026lsquo;alpha2_code\u0026rsquo; BigFive = BigFive.rename(columns={'country' : 'alpha2_code'}) BigFive.head()    EXT1 EXT2 EXT3 EXT4 EXT5 EXT6 EXT7 EXT8 EXT9 EXT10 EST1 EST2 EST3 EST4 EST5 EST6 EST7 EST8 EST9 EST10 AGR1 AGR2 AGR3 AGR4 AGR5 AGR6 AGR7 AGR8 AGR9 AGR10 CSN1 CSN2 CSN3 CSN4 CSN5 CSN6 CSN7 CSN8 CSN9 CSN10 OPN1 OPN2 OPN3 OPN4 OPN5 OPN6 OPN7 OPN8 OPN9 OPN10 dateload introelapse testelapse endelapse alpha2_code lat_appx_lots_of_err long_appx_lots_of_err 0 4.0 1.0 5.0 2.0 5.0 1.0 5.0 2.0 4.0 1.0 1.0 4.0 4.0 2.0 2.0 2.0 2.0 2.0 3.0 2.0 2.0 5.0 2.0 4.0 2.0 3.0 2.0 4.0 3.0 4.0 3.0 4.0 3.0 2.0 2.0 4.0 4.0 2.0 4.0 4.0 5.0 1.0 4.0 1.0 4.0 1.0 5.0 3.0 4.0 5.0 2016-03-03 02:01:01 9.0 234.0 6 GB 51.5448 0.1991 1 3.0 5.0 3.0 4.0 3.0 3.0 2.0 5.0 1.0 5.0 2.0 3.0 4.0 1.0 3.0 1.0 2.0 1.0 3.0 1.0 1.0 4.0 1.0 5.0 1.0 5.0 3.0 4.0 5.0 3.0 3.0 2.0 5.0 3.0 3.0 1.0 3.0 3.0 5.0 3.0 1.0 2.0 4.0 2.0 3.0 1.0 4.0 2.0 5.0 3.0 2016-03-03 02:01:20 12.0 179.0 11 MY 3.1698 101.706 2 2.0 3.0 4.0 4.0 3.0 2.0 1.0 3.0 2.0 5.0 4.0 4.0 4.0 2.0 2.0 2.0 2.0 2.0 1.0 3.0 1.0 4.0 1.0 4.0 2.0 4.0 1.0 4.0 4.0 3.0 4.0 2.0 2.0 2.0 3.0 3.0 4.0 2.0 4.0 2.0 5.0 1.0 2.0 1.0 4.0 2.0 5.0 3.0 4.0 4.0 2016-03-03 02:01:56 3.0 186.0 7 GB 54.9119 -1.3833 3 2.0 2.0 2.0 3.0 4.0 2.0 2.0 4.0 1.0 4.0 3.0 3.0 3.0 2.0 3.0 2.0 2.0 2.0 4.0 3.0 2.0 4.0 3.0 4.0 2.0 4.0 2.0 4.0 3.0 4.0 2.0 4.0 4.0 4.0 1.0 2.0 2.0 3.0 1.0 4.0 4.0 2.0 5.0 2.0 3.0 1.0 4.0 4.0 3.0 3.0 2016-03-03 02:02:02 186.0 219.0 7 GB 51.75 -1.25 5 3.0 3.0 4.0 2.0 4.0 2.0 2.0 3.0 3.0 4.0 3.0 4.0 3.0 2.0 2.0 1.0 2.0 1.0 2.0 2.0 2.0 3.0 1.0 4.0 2.0 3.0 2.0 3.0 4.0 4.0 3.0 2.0 4.0 1.0 3.0 2.0 4.0 3.0 4.0 3.0 5.0 1.0 5.0 1.0 3.0 1.0 5.0 4.0 5.0 2.0 2016-03-03 02:03:12 4.0 196.0 3 SE 59.3333 18.05   BigFive['alpha2_code'].unique() ['GB', 'MY', 'SE', 'US', 'FI', ..., 'DJ', 'ML', 'GN', 'AS', 'SH'] Length: 221 Categories (221, object): ['GB', 'MY', 'SE', 'US', ..., 'ML', 'GN', 'AS', 'SH']   # Według normy ISO 3166-1 alfa-2 kod dla każdego panstwa składa się z dwóch liter, # zatem zobaczymy, czy występuje jakiś składający się z trzech lub więcej znaków? for code in BigFive['alpha2_code'].unique(): if (len(code) \u0026gt; 3): print(code) NONE   # Ile jest takich wierszy z zawartością NONE ? len(BigFive.loc[(BigFive['alpha2_code']=='NONE')]) 9580 len(BigFive['alpha2_code']) 693694   # Decoding of country names def check_country_code(df): if len(df['alpha2_code']) == 2: if df['alpha2_code'] == 'XK': return 'Kosovo' else: return pc.country_alpha2_to_country_name( country_2_code= df['alpha2_code']) else: return 'NONE' BigFive['country'] = BigFive.apply( check_country_code, axis=1) BigFive.head(3)    EXT1 EXT2 EXT3 EXT4 EXT5 EXT6 EXT7 EXT8 EXT9 EXT10 EST1 EST2 EST3 EST4 EST5 EST6 EST7 EST8 EST9 EST10 AGR1 AGR2 AGR3 AGR4 AGR5 AGR6 AGR7 AGR8 AGR9 AGR10 CSN1 CSN2 CSN3 CSN4 CSN5 CSN6 CSN7 CSN8 CSN9 CSN10 OPN1 OPN2 OPN3 OPN4 OPN5 OPN6 OPN7 OPN8 OPN9 OPN10 dateload introelapse testelapse endelapse alpha2_code lat_appx_lots_of_err long_appx_lots_of_err country 0 4.0 1.0 5.0 2.0 5.0 1.0 5.0 2.0 4.0 1.0 1.0 4.0 4.0 2.0 2.0 2.0 2.0 2.0 3.0 2.0 2.0 5.0 2.0 4.0 2.0 3.0 2.0 4.0 3.0 4.0 3.0 4.0 3.0 2.0 2.0 4.0 4.0 2.0 4.0 4.0 5.0 1.0 4.0 1.0 4.0 1.0 5.0 3.0 4.0 5.0 2016-03-03 02:01:01 9.0 234.0 6 GB 51.5448 0.1991 United Kingdom 1 3.0 5.0 3.0 4.0 3.0 3.0 2.0 5.0 1.0 5.0 2.0 3.0 4.0 1.0 3.0 1.0 2.0 1.0 3.0 1.0 1.0 4.0 1.0 5.0 1.0 5.0 3.0 4.0 5.0 3.0 3.0 2.0 5.0 3.0 3.0 1.0 3.0 3.0 5.0 3.0 1.0 2.0 4.0 2.0 3.0 1.0 4.0 2.0 5.0 3.0 2016-03-03 02:01:20 12.0 179.0 11 MY 3.1698 101.706 Malaysia 2 2.0 3.0 4.0 4.0 3.0 2.0 1.0 3.0 2.0 5.0 4.0 4.0 4.0 2.0 2.0 2.0 2.0 2.0 1.0 3.0 1.0 4.0 1.0 4.0 2.0 4.0 1.0 4.0 4.0 3.0 4.0 2.0 2.0 2.0 3.0 3.0 4.0 2.0 4.0 2.0 5.0 1.0 2.0 1.0 4.0 2.0 5.0 3.0 4.0 4.0 2016-03-03 02:01:56 3.0 186.0 7 GB 54.9119 -1.3833 United Kingdom  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/bigfive/bigfive_data_prepare/","summary":"Data preparation: Data presentation BigFive_raw = pd.read_csv( '/content/drive/My Drive/ColabNotebooks/BigFivePersonality/data-final.csv', low_memory=False, sep='\\t', usecols=['EXT1', 'EXT2', 'EXT3', 'EXT4', 'EXT5', 'EXT6', 'EXT7', 'EXT8', 'EXT9', 'EXT10', 'EST1', 'EST2', 'EST3', 'EST4', 'EST5', 'EST6', 'EST7', 'EST8', 'EST9', 'EST10', 'AGR1', 'AGR2', 'AGR3', 'AGR4', 'AGR5', 'AGR6', 'AGR7', 'AGR8', 'AGR9', 'AGR10', 'CSN1', 'CSN2', 'CSN3', 'CSN4', 'CSN5', 'CSN6', 'CSN7', 'CSN8', 'CSN9', 'CSN10', 'OPN1', 'OPN2', 'OPN3', 'OPN4', 'OPN5', 'OPN6', 'OPN7', 'OPN8', 'OPN9', 'OPN10', # 'EXT1_E', 'EXT2_E', 'EXT3_E', 'EXT4_E', 'EXT5_E', 'EXT6_E', 'EXT7_E', 'EXT8_E', 'EXT9_E', 'EXT10_E', # 'EST1_E', 'EST2_E', 'EST3_E', 'EST4_E', 'EST5_E', 'EST6_E', 'EST7_E', 'EST8_E', 'EST9_E', 'EST10_E', # 'AGR1_E', 'AGR2_E', 'AGR3_E', 'AGR4_E', 'AGR5_E', 'AGR6_E', 'AGR7_E', 'AGR8_E', 'AGR9_E', 'AGR10_E', # 'CSN1_E', 'CSN2_E', 'CSN3_E', 'CSN4_E', 'CSN5_E', 'CSN6_E', 'CSN7_E', 'CSN8_E', 'CSN9_E', 'CSN10_E', # 'OPN1_E', 'OPN2_E', 'OPN3_E', 'OPN4_E', 'OPN5_E', 'OPN6_E', 'OPN7_E', 'OPN8_E', 'OPN9_E', 'OPN10_E', 'dateload', 'introelapse', 'testelapse', 'endelapse', 'IPC', 'country', 'lat_appx_lots_of_err', 'long_appx_lots_of_err' ] ) # Kopia danych BigFive = BigFive_raw.","tags":null,"title":"Big Five Personality test - data preparation"},{"categories":null,"contents":"link to GitHub Repository\nImport modules try: import pandas as pd import numpy as np np.random.seed(42) import matplotlib.pyplot as plt plt.style.use(\u0026quot;fivethirtyeight\u0026quot;) import plotly.figure_factory as ff import plotly.express as px from plotly.subplots import make_subplots import seaborn as sns from itertools import islice, combinations from geopy.geocoders import Nominatim geolocator = Nominatim(user_agent=\u0026quot;BigFive_project\u0026quot;, timeout=None ) import pycountry_convert as pc import squarify import time import os import swifter import geopandas as gpd except: !pip3 install pycountry_convert !pip3 install squarify !pip3 install swifter !pip3 install geopandas import pycountry_convert as pc import squarify import swifter import geopandas as gpd %matplotlib inline  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/bigfive/bigfive_modules/","summary":"link to GitHub Repository\nImport modules try: import pandas as pd import numpy as np np.random.seed(42) import matplotlib.pyplot as plt plt.style.use(\u0026quot;fivethirtyeight\u0026quot;) import plotly.figure_factory as ff import plotly.express as px from plotly.subplots import make_subplots import seaborn as sns from itertools import islice, combinations from geopy.geocoders import Nominatim geolocator = Nominatim(user_agent=\u0026quot;BigFive_project\u0026quot;, timeout=None ) import pycountry_convert as pc import squarify import time import os import swifter import geopandas as gpd except: !pip3 install pycountry_convert !","tags":null,"title":"Big Five Personality test - imported modules"},{"categories":null,"contents":"2. Data preparing: !unzip -q \u0026quot;/content/drive/My Drive/ColabNotebooks/intel_image/data/intel_image.zip\u0026quot;  2.1. Number of photos for each class !rm -rf ./images base_dir = './intel_image' # Przypisujemy katalog bazowy raw_no_of_files = {} classes = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street'] for dir in classes: raw_no_of_files[dir] = len(os.listdir(os.path.join(base_dir, dir))) raw_no_of_files.items()  Results:\ndict_items([('buildings', 2628), ('forest', 2745), ('glacier', 2957), ('mountain', 3037), ('sea', 2784), ('street', 2883)])  2.2. Catalogues creating: train, test, valid data_dir = './images' # Przypisujemy katalog bazowy if not os.path.exists(data_dir): os.mkdir(data_dir) train_dir = os.path.join(data_dir, 'train') # katalog dla zbioru treningowego valid_dir = os.path.join(data_dir, 'valid') # katalog dla zbioru walidacyjnego test_dir = os.path.join(data_dir, 'test') # katalog dla zbioru testowego # Wskażemy klasy train_buildings_dir = os.path.join(train_dir, 'buildings') train_forest_dir = os.path.join(train_dir, 'forest') train_glacier_dir = os.path.join(train_dir, 'glacier') train_mountain_dir = os.path.join(train_dir, 'mountain') train_sea_dir = os.path.join(train_dir, 'sea') train_street_dir = os.path.join(train_dir, 'street') valid_buildings_dir = os.path.join(valid_dir, 'buildings') valid_forest_dir = os.path.join(valid_dir, 'forest' ) valid_glacier_dir = os.path.join(valid_dir, 'glacier') valid_mountain_dir = os.path.join(valid_dir, 'mountain') valid_sea_dir = os.path.join(valid_dir, 'sea') valid_street_dir = os.path.join(valid_dir, 'street') test_buildings_dir = os. # Pętla, która przejdzie po naszych katalogach for i, directory in enumerate([train_dir, valid_dir, test_dir]): if not os.path.exists(directory): # Jeśli takie ścieżki nie istenieją, to po prostu utowrzymy te katalogi os.mkdir(directory) # To samo wykonamy z katalogami w trzecim pozimie dirs = [train_buildings_dir, train_forest_dir, train_glacier_dir, train_mountain_dir, train_sea_dir, train_street_dir, valid_buildings_dir, valid_forest_dir, valid_glacier_dir, valid_mountain_dir, valid_sea_dir, valid_street_dir, test_buildings_dir, test_forest_dir, test_glacier_dir, test_mountain_dir, test_sea_dir, test_street_dir ] for dir in dirs: if not os.path.exists(dir): os.mkdir(dir) path.join(test_dir, 'buildings') test_forest_dir = os.path.join(test_dir, 'forest') test_glacier_dir = os.path.join(test_dir, 'glacier') test_mountain_dir = os.path.join(test_dir, 'mountain') test_sea_dir = os.path.join(test_dir, 'sea') test_street_dir = os.path.join(test_dir, 'street') # Pętla, która przejdzie po naszych katalogach for i, directory in enumerate([train_dir, valid_dir, test_dir]): if not os.path.exists(directory): # Jeśli takie ścieżki nie istenieją, to po prostu utowrzymy te katalogi os.mkdir(directory) # To samo wykonamy z katalogami w trzecim pozimie dirs = [train_buildings_dir, train_forest_dir, train_glacier_dir, train_mountain_dir, train_sea_dir, train_street_dir, valid_buildings_dir, valid_forest_dir, valid_glacier_dir, valid_mountain_dir, valid_sea_dir, valid_street_dir, test_buildings_dir, test_forest_dir, test_glacier_dir, test_mountain_dir, test_sea_dir, test_street_dir ] for dir in dirs: if not os.path.exists(dir): os.mkdir(dir) # Teraz musimy znać pełne ścieżki do naszych plików base_dir = './intel_image' # Przypisujemy katalog bazowy, gdzie są zdjęcia # Do zmiennej _fnames przypisujemy wylistowanie katalogu z katalogu bazowego, tam gdzie są zdjęcia buildings_files_names = os.listdir(os.path.join(base_dir, 'buildings')) forest_files_names = os.listdir(os.path.join(base_dir, 'forest')) glacier_files_names = os.listdir(os.path.join(base_dir, 'glacier')) mountain_files_names = os.listdir(os.path.join(base_dir, 'mountain')) sea_files_names = os.listdir(os.path.join(base_dir, 'sea')) street_files_names = os.listdir(os.path.join(base_dir, 'street')) test_files_names = os.listdir(os.path.join(base_dir, 'test'))  2.3. Choosing data size # W związku z tym, że są różne wartości danych, w różnych klasach, # Wybierzemy minimalną długość z tych dwóch klas i przypiszemy do zmiennej rozmiar size = min(len(buildings_files_names), len(forest_files_names), len(glacier_files_names), len(mountain_files_names), len(sea_files_names), len(street_files_names)) train_size = int(np.floor(0.7 * size)) # rozmiar danych treningowych to 70% wszystkich zdjęć valid_size = int(np.floor(0.28 * size)) test_size = size - (train_size + valid_size) # Testowy zbiór to będzie 10%  2.4. Copying data to selected folders def filesCopy(directory_dir): train_id = train_size valid_id = train_size + valid_size test_id = train_size + valid_size + test_size for i, dictio in enumerate(directory_dir): dir_name = [val for val in directory_dir.values() ][i] label_name = dictio for i, fname in enumerate(dir_name[0]): if i \u0026lt;= train_id: src = os.path.join(base_dir, label_name, fname) dst = os.path.join(dir_name[1], fname) shutil.copyfile(src, dst) elif train_id \u0026lt; i \u0026lt;= valid_id: src = os.path.join(base_dir, label_name, fname) dst = os.path.join(dir_name[2], fname) shutil.copyfile(src, dst) elif valid_id \u0026lt; i \u0026lt; test_id: src = os.path.join(base_dir, label_name, fname) dst = os.path.join(dir_name[3], fname) shutil.copyfile(src, dst)  3.1.1. MODEL 1: print(f\u0026rsquo;{label_name} - zbiór treningowy: { len(os.listdir(dir_name[1] )) } \u0026lsquo;) print(f\u0026rsquo;{label_name} - zbiór walidacyjny: { len(os.listdir(dir_name[2] )) } \u0026lsquo;) print(f\u0026rsquo;{label_name} - zbiór testowy: { len(os.listdir(dir_name[3] )) } \u0026lsquo;) print() # \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\ndirectory_dir = { 'buildings' : [buildings_files_names, train_buildings_dir, valid_buildings_dir, test_buildings_dir ], 'forest' : [forest_files_names, train_forest_dir, valid_forest_dir, test_forest_dir ], 'glacier' : [glacier_files_names, train_glacier_dir, valid_glacier_dir, test_glacier_dir ], 'mountain' : [mountain_files_names, train_mountain_dir, valid_mountain_dir, test_mountain_dir ], 'sea' : [sea_files_names, train_sea_dir, valid_sea_dir, test_sea_dir ], 'street' : [street_files_names, train_street_dir, valid_street_dir, test_street_dir ] } filesCopy(directory_dir)  Results:\nbuildings - zbiór treningowy: 1840 buildings - zbiór walidacyjny: 735 buildings - zbiór testowy: 53 forest - zbiór treningowy: 1840 forest - zbiór walidacyjny: 735 forest - zbiór testowy: 53 glacier - zbiór treningowy: 1840 glacier - zbiór walidacyjny: 735 glacier - zbiór testowy: 53 mountain - zbiór treningowy: 1840 mountain - zbiór walidacyjny: 735 mountain - zbiór testowy: 53 sea - zbiór treningowy: 1840 sea - zbiór walidacyjny: 735 sea - zbiór testowy: 53 street - zbiór treningowy: 1840 street - zbiór walidacyjny: 735 street - zbiór testowy: 53  2.5. Data presentation #@title Wybierz indeks przykładowego zdjęcia ze zbioru treningowego: buildings_idx = 371 #@param {type:'slider', min:0, max:1839} names_mapping = dict(enumerate(buildings_files_names)) img_path = os.path.join(train_buildings_dir, names_mapping[buildings_idx]) img_buildings = image.load_img(img_path) forest_idx = 1302 #@param {type:'slider', min:0, max:1839} names_mapping = dict(enumerate(forest_files_names)) img_path = os.path.join(train_forest_dir, names_mapping[forest_idx]) img_forest = image.load_img(img_path) glacier_idx = 122 #@param {type:'slider', min:0, max:1839} names_mapping = dict(enumerate(glacier_files_names)) img_path = os.path.join(train_glacier_dir, names_mapping[glacier_idx]) img_glacier = image.load_img(img_path) mountain_idx = 1437 #@param {type:'slider', min:0, max:1839} names_mapping = dict(enumerate(mountain_files_names)) img_path = os.path.join(train_mountain_dir, names_mapping[mountain_idx]) img_mountain = image.load_img(img_path) sea_idx = 980 #@param {type:'slider', min:0, max:1839} names_mapping = dict(enumerate(sea_files_names)) img_path = os.path.join(train_sea_dir, names_mapping[sea_idx]) img_sea = image.load_img(img_path) street_idx = 171 #@param {type:'slider', min:0, max:1839} names_mapping = dict(enumerate(street_files_names)) img_path = os.path.join(train_street_dir, names_mapping[street_idx]) img_street = image.load_img(img_path) fig, ax = plt.subplots(2,3,figsize=(23, 15)) ax[0,0].imshow(img_buildings) ax[0,1].imshow(img_forest) ax[0,2].imshow(img_glacier) ax[1,0].imshow(img_mountain) ax[1,1].imshow(img_sea) ax[1,2].imshow(img_street) ax[0,0].axis(False) ax[0,1].axis(False) ax[0,2].axis(False) ax[1,0].axis(False) ax[1,1].axis(False) ax[1,2].axis(False) plt.show()  2.6. Data augmentation train_datagen = ImageDataGenerator( rotation_range=10, # zakres kąta o który losowo zostanie wykonany obrót obrazów rescale=1./255., # przeskalowujemy wszystkie obrazy o współczynnik 1/255 width_shift_range=0.1, # pionowe przekształcenia obrazu height_shift_range=0.1, # poziome przekształcenia obrazu shear_range=0.2, # zares losowego przycianania obrazu zoom_range=0.1, # zakres losowego przybliżania obrazu horizontal_flip=True, # losowe odbicie połowy obrazu w płaszczyźnie poziomej fill_mode='nearest' # strategia wypełniania nowo utworzonych pikseli, któe mogą powstać w wyniku przekształceń ) valid_datagen = ImageDataGenerator(rescale=1./255.) # Budujemy generatory train_generator = train_datagen.flow_from_directory(directory=train_dir, target_size=(150, 150), batch_size=32, class_mode='categorical') # batch size 32 valid_generator = valid_datagen.flow_from_directory(directory=valid_dir, target_size=(150, 150), batch_size=32, class_mode='categorical')  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/imageclassproject/project_imageclass_data_preparing/","summary":"2. Data preparing: !unzip -q \u0026quot;/content/drive/My Drive/ColabNotebooks/intel_image/data/intel_image.zip\u0026quot;  2.1. Number of photos for each class !rm -rf ./images base_dir = './intel_image' # Przypisujemy katalog bazowy raw_no_of_files = {} classes = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street'] for dir in classes: raw_no_of_files[dir] = len(os.listdir(os.path.join(base_dir, dir))) raw_no_of_files.items()  Results:\ndict_items([('buildings', 2628), ('forest', 2745), ('glacier', 2957), ('mountain', 3037), ('sea', 2784), ('street', 2883)])  2.2. Catalogues creating: train, test, valid data_dir = './images' # Przypisujemy katalog bazowy if not os.","tags":null,"title":"Convolutional network testing for image classification - data preparing"},{"categories":null,"contents":"link to data\nlink to GitHub Repository\n1. Import modules import os import time from datetime import datetime as dt import numpy as np import pandas as pd import matplotlib.pyplot as plt import shutil # Pomaga w kopiowaniu plików import seaborn as sns sns.set(style=\u0026quot;ticks\u0026quot;, color_codes=True) from sklearn.metrics import confusion_matrix, classification_report from tensorflow.keras.preprocessing import image from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.models import Sequential, Model from tensorflow.keras.models import load_model from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, concatenate, BatchNormalization, GlobalAveragePooling2D from tensorflow.keras.optimizers import Adam from tensorflow.keras import optimizers from tensorflow.keras.callbacks import TensorBoard from tensorflow.keras.callbacks import ModelCheckpoint from tensorflow.keras.callbacks import EarlyStopping np.set_printoptions(precision=12, suppress=True)  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/imageclassproject/project_imageclass_modules/","summary":"link to data\nlink to GitHub Repository\n1. Import modules import os import time from datetime import datetime as dt import numpy as np import pandas as pd import matplotlib.pyplot as plt import shutil # Pomaga w kopiowaniu plików import seaborn as sns sns.set(style=\u0026quot;ticks\u0026quot;, color_codes=True) from sklearn.metrics import confusion_matrix, classification_report from tensorflow.keras.preprocessing import image from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.models import Sequential, Model from tensorflow.keras.models import load_model from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, concatenate, BatchNormalization, GlobalAveragePooling2D from tensorflow.","tags":null,"title":"Convolutional network testing for image classification - imported modules"},{"categories":null,"contents":"3.6. Models testing: Load best model:\nbest_model = load_model('/content/drive/My Drive/ColabNotebooks/intel_image/models/2020-06-05/model_3_L0.36_A0.86.h5') # best_model.summary()  3.6.1. TestModel class Create TestModel class:\nclass TestModel: def __init__(self, model_dict): self.model_dict = model_dict self.y_pred_list = [] self.models_metrics_list = [] self.models_errors_list = [] self.model_name_idx = 0 self.y_pred_idx = 0 self.models_metrics_idx = 0 self.errors_df_idx = 0 self.test_dir_list = [test_buildings_dir, test_forest_dir, test_glacier_dir, test_mountain_dir, test_sea_dir, test_street_dir] def __prepare_test_generator(self): self.test_datagen = ImageDataGenerator(rescale=1./255.) self.test_generator = self.test_datagen.flow_from_directory(test_dir, target_size=(150, 150), batch_size=32, class_mode='categorical', shuffle=False ) def __get_y_pred(self): self.__prepare_test_generator() for i, name in enumerate(self.model_dict): self.model = [val for val in self.model_dict.values() ][i] self.y_proba = self.model.predict(self.test_generator , steps=len(self.test_generator ), batch_size=32 ) self.y_pred = np.argmax(self.y_proba, axis=1) self.y_pred_list.append( self.y_pred ) def __get_model_metrics(self): for i, name in enumerate(self.model_dict): self.model = [val for val in self.model_dict.values() ][i] self.models_metrics = self.model.evaluate(self.test_generator , steps=len(self.test_generator ), batch_size=32, verbose=0) self.models_metrics_list.append(self.models_metrics) def __create_errors_data_frame(self): self.errors_df = pd.DataFrame( index= self.test_generator.filenames ) self.y_true = self.test_generator.classes for i, y_pred in enumerate(self.y_pred_list): self.errors_df['y_true'] = self.y_true self.errors_df['y_pred_' + str(i) ] = y_pred self.errors_df['incorrect_' + str(i)] = (self.y_true != y_pred) * 1 self.number_of_errors = len(self.errors_df[self.errors_df['incorrect_'+ str(i)] == 1].index) self.models_errors_list.append(self.number_of_errors) def create_matrix(self): self.__get_y_pred() self.__get_model_metrics() self.__create_errors_data_frame() sns.set_style(\u0026quot;darkgrid\u0026quot;) fig = plt.figure(figsize=(30, 8)) fig.subplots_adjust(hspace=0.7, wspace=0.2) for i in range(1, 7): fig.add_subplot(2, 3, i) self.cm = confusion_matrix(self.y_true, self.y_pred_list[self.y_pred_idx]) g = sns.heatmap(self.cm, annot=True, fmt='d', linewidth=.2, cmap='Blues', xticklabels=self.test_generator.class_indices.keys(), yticklabels=self.test_generator.class_indices.keys(), annot_kws={\u0026quot;size\u0026quot;: 15}) g.tick_params(labelsize=15) g.xaxis.tick_top() plt.yticks(rotation=0) plt.xticks(rotation=0) plt.title(f'{ list(model_dict.keys())[self.model_name_idx]}\\n Accuracy: {self.models_metrics_list[self.models_metrics_idx][1]:.4f} Loss: {self.models_metrics_list[self.models_metrics_idx][0]:.4f}\\n Liczba pomyłek: {self.models_errors_list[self.errors_df_idx]}', fontdict={'fontsize' : 15, 'weight':'bold'}) self.model_name_idx += 1 self.y_pred_idx += 1 self.models_metrics_idx += 1 self.errors_df_idx += 1 plt.show() def __set_column_name_prefix(self, model_name): for i, name in enumerate(self.model_dict): if str(i) in model_name: self.incorrect_col_name = 'incorrect_' self.y_pred_col_name = 'y_pred_' self.incorrect_col_name = self.incorrect_col_name + str(i) self.y_pred_col_name = self.y_pred_col_name + str(i) elif 'Best' in model_name: self.incorrect_col_name = 'incorrect_' self.y_pred_col_name = 'y_pred_' self.incorrect_col_name = self.incorrect_col_name + str(5) self.y_pred_col_name = self.y_pred_col_name + str(5) def __get_image_name_and_category_from_df(self, col_idx): self.img = [ img.split('/')[1] for i, img in enumerate(self.errors_df[self.errors_df[self.incorrect_col_name] == 1].index) ][col_idx] self.category = [ img.split('/')[0] for i, img in enumerate(self.errors_df[self.errors_df[self.incorrect_col_name] == 1].index) ][col_idx] def __get_true_and_pred_label_from_df(self, col_idx): self.pred_label = [self.pred_label for i, self.pred_label in enumerate(self.errors_df[self.errors_df[self.incorrect_col_name] == 1][self.y_pred_col_name].values) ][col_idx] self.true_label = [self.true_label for i, self.true_label in enumerate(self.errors_df[self.errors_df[self.incorrect_col_name] == 1]['y_true'].values) ][col_idx] def __concatenate_class_inidces_with_label_from_df(self): for key, val in self.test_generator.class_indices.items(): if val == self.pred_label: self.pred_label = key if val == self.true_label: self.true_label = key def __prepare_image_path(self): self.dir_name = test_dir + '/'+ self.category self.img_path = os.path.join(self.dir_name, self.img) def show_incorrect_images(self, model_name, amount=10): self.__set_column_name_prefix(model_name) rows= 5 cols= 8 col_idx = 0 fig = plt.figure(figsize=(30, 20)) fig.subplots_adjust(hspace=0.3, wspace=0) for i in range(1, amount): fig.add_subplot(rows, cols, i) self.__get_image_name_and_category_from_df(col_idx) self.__get_true_and_pred_label_from_df(col_idx) self.__concatenate_class_inidces_with_label_from_df() self.__prepare_image_path() self.img = image.load_img(self.img_path) plt.imshow(self.img) plt.title(f'true label: {self.true_label}\\n predicted: {self.pred_label}', fontdict = {'color' : 'green', 'weight' : 'bold', 'size' : 12}) plt.axis('off') col_idx += 1 plt.show()  Create model dictionary:\nmodel_dict = { 'model 1' : model_1, 'model 2' : model_2, 'model 3' : model_3, 'model 4' : model_4, 'model 5' : model_5, 'Best model' : best_model } TestModel_obj = TestModel( model_dict= model_dict)  Results:\nTestModel_obj.create_matrix()  3.6.2. Best models 3.6.3. Presentation of incorrect image classification TestModel_obj.show_incorrect_images(model_name= 'model 3', amount= 25)  TestModel_obj.show_incorrect_images(model_name= 'Best model', amount= 25)  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/imageclassproject/project_imageclass_model_test/","summary":"3.6. Models testing: Load best model:\nbest_model = load_model('/content/drive/My Drive/ColabNotebooks/intel_image/models/2020-06-05/model_3_L0.36_A0.86.h5') # best_model.summary()  3.6.1. TestModel class Create TestModel class:\nclass TestModel: def __init__(self, model_dict): self.model_dict = model_dict self.y_pred_list = [] self.models_metrics_list = [] self.models_errors_list = [] self.model_name_idx = 0 self.y_pred_idx = 0 self.models_metrics_idx = 0 self.errors_df_idx = 0 self.test_dir_list = [test_buildings_dir, test_forest_dir, test_glacier_dir, test_mountain_dir, test_sea_dir, test_street_dir] def __prepare_test_generator(self): self.test_datagen = ImageDataGenerator(rescale=1./255.) self.test_generator = self.test_datagen.flow_from_directory(test_dir, target_size=(150, 150), batch_size=32, class_mode='categorical', shuffle=False ) def __get_y_pred(self): self.","tags":null,"title":"Convolutional network testing for image classification - model testing"},{"categories":null,"contents":"3. Neural networks: class Network: def __init__(self, model_name, epochs, optimizer): self.model_name = model_name self.epochs = epochs self.optimizer = optimizer self.tz = 'CEST' def model_summary(self): self.model_name.summary() def save_model(self, file_name): self.models_dir = '/content/drive/My Drive/ColabNotebooks/intel_image/models/' + dt.now().strftime('%Y-%m-%d/') if not os.path.exists(self.models_dir): os.makedirs(self.models_dir) self.model_name.save(self.models_dir + file_name + '.h5') def train(self, verbose=0, ): self.model_name.compile(optimizer= self.optimizer, loss='categorical_crossentropy', metrics=['accuracy']) log_dir = 'logs/' + dt.now().strftime('%Y-%m-%d_%H:%M:%S'+ self.tz) tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1 ) # -------------------------------------------------------- time_start = time.time() self.history = self.model_name.fit( train_generator, steps_per_epoch= steps_per_epoch, epochs= self.epochs, validation_data= valid_generator, validation_steps= validation_steps, verbose= verbose, callbacks= [tensorboard, checkpoint] ) time_stop = time.time() print() print('Czas trenowania sieci: {} sek'.format(time_stop - time_start)) # -------------------------------------------------------- self.scores = self.model_name.evaluate(valid_generator, steps=validation_steps, verbose=0) print('Loss: {:.4f}'.format(self.scores[0]) ) print('Accuracy: {:.4f}'.format(self.scores[1]) ) def plot_hist(self, title): self.metric = 'accuracy' self.val_metric = 'val_accuracy' self.score = self.scores[1] sns.set_style(\u0026quot;darkgrid\u0026quot;) fig, axes = plt.subplots(1,2, figsize=(27,5)) fig.suptitle(title) for self.i in range(2): if self.i == 1: self.metric = 'loss' self.val_metric = 'val_loss' self.score = self.scores[0] axes[self.i].plot(self.history.history[ self.val_metric], label= self.val_metric) axes[self.i].plot(self.history.history[ self.metric], label= self.metric) axes[self.i].set(xlabel='epoki', ylabel= self.metric, title= self.metric+f': {self.score:.4f}') axes[self.i].legend(loc='best') plt.show()  Saving best model:\n!rm -rf ./logs path ='/content/drive/My Drive/ColabNotebooks/intel_image/model/kopia/best_model.hdf5' checkpoint = ModelCheckpoint(filepath=path, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max') es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=80) batch_size = 32 steps_per_epoch = train_size // batch_size validation_steps = valid_size // batch_size  3.1. Creating models def Conv2D_block(y, number_of_layers, filters, kernel_size): for i in range(number_of_layers): y = Conv2D(filters= filters, kernel_size= kernel_size, activation='relu', padding='same')(y) y = BatchNormalization()(y) return y  3.1.1. MODEL 1: # ------------------------MODEL 1------------------------------- visible = Input(shape=(150, 150, 3)) y = Conv2D_block(visible, number_of_layers= 1, filters=32, kernel_size=7) y = Conv2D_block(y, number_of_layers= 2, filters=32, kernel_size=3) y = MaxPooling2D(pool_size=2)(y) y = Conv2D_block(y, number_of_layers= 2, filters=32, kernel_size=3) y = MaxPooling2D(pool_size=2)(y) y = Conv2D_block(y, number_of_layers= 3, filters=32, kernel_size=3) y = MaxPooling2D(pool_size=2)(y) y = Conv2D_block(y, number_of_layers= 3, filters=32, kernel_size=3) y = MaxPooling2D(pool_size=2)(y) y = Conv2D_block(y, number_of_layers= 5, filters=32, kernel_size=3) y = Conv2D_block(y, number_of_layers= 5, filters=32, kernel_size=3) y = Conv2D_block(y, number_of_layers= 5, filters=64, kernel_size=3) y = Conv2D_block(y, number_of_layers= 1, filters=64, kernel_size=1) y = Conv2D_block(y, number_of_layers= 1, filters=64, kernel_size=3) y = Conv2D_block(y, number_of_layers= 1, filters=128, kernel_size=1) y = Conv2D_block(y, number_of_layers= 2, filters=128, kernel_size=3) y = MaxPooling2D(pool_size=2)(y) y = Conv2D_block(y, number_of_layers= 2, filters=128, kernel_size=3) y = GlobalAveragePooling2D()(y) y = Dropout(0.5)(y) output = Dense(6, activation='softmax')(y) model_1 = Model(inputs=visible, outputs=output)  3.1.1.1. Model 1 summary Model: \u0026quot;model 1\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 150, 150, 3)] 0 _________________________________________________________________ conv2d (Conv2D) (None, 150, 150, 32) 4736 _________________________________________________________________ batch_normalization (BatchNo (None, 150, 150, 32) 128 _________________________________________________________________ conv2d_1 (Conv2D) (None, 150, 150, 32) 9248 _________________________________________________________________ batch_normalization_1 (Batch (None, 150, 150, 32) 128 _________________________________________________________________ conv2d_2 (Conv2D) (None, 150, 150, 32) 9248 _________________________________________________________________ batch_normalization_2 (Batch (None, 150, 150, 32) 128 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 75, 75, 32) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 75, 75, 32) 9248 _________________________________________________________________ batch_normalization_3 (Batch (None, 75, 75, 32) 128 _________________________________________________________________ conv2d_4 (Conv2D) (None, 75, 75, 32) 9248 _________________________________________________________________ batch_normalization_4 (Batch (None, 75, 75, 32) 128 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 37, 37, 32) 0 _________________________________________________________________ conv2d_5 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_5 (Batch (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_6 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_6 (Batch (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_7 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_7 (Batch (None, 37, 37, 32) 128 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 18, 18, 32) 0 _________________________________________________________________ conv2d_8 (Conv2D) (None, 18, 18, 32) 9248 _________________________________________________________________ batch_normalization_8 (Batch (None, 18, 18, 32) 128 _________________________________________________________________ conv2d_9 (Conv2D) (None, 18, 18, 32) 9248 _________________________________________________________________ batch_normalization_9 (Batch (None, 18, 18, 32) 128 _________________________________________________________________ conv2d_10 (Conv2D) (None, 18, 18, 32) 9248 _________________________________________________________________ batch_normalization_10 (Batc (None, 18, 18, 32) 128 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 9, 9, 32) 0 _________________________________________________________________ conv2d_11 (Conv2D) (None, 9, 9, 32) 9248 _________________________________________________________________ batch_normalization_11 (Batc (None, 9, 9, 32) 128 _________________________________________________________________ conv2d_12 (Conv2D) (None, 9, 9, 32) 9248 _________________________________________________________________ batch_normalization_12 (Batc (None, 9, 9, 32) 128 _________________________________________________________________ conv2d_13 (Conv2D) (None, 9, 9, 32) 9248 _________________________________________________________________ batch_normalization_13 (Batc (None, 9, 9, 32) 128 _________________________________________________________________ conv2d_14 (Conv2D) (None, 9, 9, 32) 9248 _________________________________________________________________ batch_normalization_14 (Batc (None, 9, 9, 32) 128 _________________________________________________________________ conv2d_15 (Conv2D) (None, 9, 9, 32) 9248 _________________________________________________________________ batch_normalization_15 (Batc (None, 9, 9, 32) 128 _________________________________________________________________ conv2d_16 (Conv2D) (None, 9, 9, 32) 9248 _________________________________________________________________ batch_normalization_16 (Batc (None, 9, 9, 32) 128 _________________________________________________________________ conv2d_17 (Conv2D) (None, 9, 9, 32) 9248 _________________________________________________________________ batch_normalization_17 (Batc (None, 9, 9, 32) 128 _________________________________________________________________ conv2d_18 (Conv2D) (None, 9, 9, 32) 9248 _________________________________________________________________ batch_normalization_18 (Batc (None, 9, 9, 32) 128 _________________________________________________________________ conv2d_19 (Conv2D) (None, 9, 9, 32) 9248 _________________________________________________________________ batch_normalization_19 (Batc (None, 9, 9, 32) 128 _________________________________________________________________ conv2d_20 (Conv2D) (None, 9, 9, 32) 9248 _________________________________________________________________ batch_normalization_20 (Batc (None, 9, 9, 32) 128 _________________________________________________________________ conv2d_21 (Conv2D) (None, 9, 9, 64) 18496 _________________________________________________________________ batch_normalization_21 (Batc (None, 9, 9, 64) 256 _________________________________________________________________ conv2d_22 (Conv2D) (None, 9, 9, 64) 36928 _________________________________________________________________ batch_normalization_22 (Batc (None, 9, 9, 64) 256 _________________________________________________________________ conv2d_23 (Conv2D) (None, 9, 9, 64) 36928 _________________________________________________________________ batch_normalization_23 (Batc (None, 9, 9, 64) 256 _________________________________________________________________ conv2d_24 (Conv2D) (None, 9, 9, 64) 36928 _________________________________________________________________ batch_normalization_24 (Batc (None, 9, 9, 64) 256 _________________________________________________________________ conv2d_25 (Conv2D) (None, 9, 9, 64) 36928 _________________________________________________________________ batch_normalization_25 (Batc (None, 9, 9, 64) 256 _________________________________________________________________ conv2d_26 (Conv2D) (None, 9, 9, 64) 4160 _________________________________________________________________ batch_normalization_26 (Batc (None, 9, 9, 64) 256 _________________________________________________________________ conv2d_27 (Conv2D) (None, 9, 9, 64) 36928 _________________________________________________________________ batch_normalization_27 (Batc (None, 9, 9, 64) 256 _________________________________________________________________ conv2d_28 (Conv2D) (None, 9, 9, 128) 8320 _________________________________________________________________ batch_normalization_28 (Batc (None, 9, 9, 128) 512 _________________________________________________________________ conv2d_29 (Conv2D) (None, 9, 9, 128) 147584 _________________________________________________________________ batch_normalization_29 (Batc (None, 9, 9, 128) 512 _________________________________________________________________ conv2d_30 (Conv2D) (None, 9, 9, 128) 147584 _________________________________________________________________ batch_normalization_30 (Batc (None, 9, 9, 128) 512 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128) 0 _________________________________________________________________ conv2d_31 (Conv2D) (None, 4, 4, 128) 147584 _________________________________________________________________ batch_normalization_31 (Batc (None, 4, 4, 128) 512 _________________________________________________________________ conv2d_32 (Conv2D) (None, 4, 4, 128) 147584 _________________________________________________________________ batch_normalization_32 (Batc (None, 4, 4, 128) 512 _________________________________________________________________ global_average_pooling2d (Gl (None, 128) 0 _________________________________________________________________ dropout (Dropout) (None, 128) 0 _________________________________________________________________ dense (Dense) (None, 6) 774 ================================================================= Total params: 1,003,462 Trainable params: 999,942 Non-trainable params: 3,520  3.1.2. MODEL 2: # ------------------------MODEL 2------------------------------- visible = Input(shape=(150, 150, 3)) y = Conv2D_block(visible, number_of_layers= 1, filters= 32, kernel_size=7) y = Conv2D_block(y, number_of_layers= 3, filters= 32, kernel_size=3) y = MaxPooling2D(pool_size=2)(y) y = Conv2D_block(y, number_of_layers= 3, filters= 32, kernel_size=3) y = MaxPooling2D(pool_size=2)(y) y = Conv2D_block(y, number_of_layers= 5, filters= 32, kernel_size=3) y = Conv2D_block(y, number_of_layers= 5, filters= 32, kernel_size=3) y = Conv2D_block(y, number_of_layers= 5, filters= 32, kernel_size=3) y = Conv2D_block(y, number_of_layers= 1, filters= 32, kernel_size=1) y = Conv2D_block(y, number_of_layers= 1, filters= 32, kernel_size=3) y = Conv2D_block(y, number_of_layers= 1, filters= 64, kernel_size=1) y = Conv2D_block(y, number_of_layers= 3, filters= 64, kernel_size=3) y = MaxPooling2D(pool_size=2)(y) y = Conv2D_block(y, number_of_layers= 3, filters= 64, kernel_size=3) y = Conv2D_block(y, number_of_layers= 3, filters= 64, kernel_size=3) y = Conv2D_block(y, number_of_layers= 3, filters= 64, kernel_size=3) y = Conv2D_block(y, number_of_layers= 1, filters= 128, kernel_size=3) y = MaxPooling2D(pool_size=2)(y) y = GlobalAveragePooling2D()(y) y = Dropout(0.5)(y) output = Dense(6, activation='softmax')(y) model_2 = Model(inputs=visible, outputs=output)  3.1.2.1. Model 2 summary Model: \u0026quot;model_2\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 150, 150, 3)] 0 _________________________________________________________________ conv2d_33 (Conv2D) (None, 150, 150, 32) 4736 _________________________________________________________________ batch_normalization_33 (Batc (None, 150, 150, 32) 128 _________________________________________________________________ conv2d_34 (Conv2D) (None, 150, 150, 32) 9248 _________________________________________________________________ batch_normalization_34 (Batc (None, 150, 150, 32) 128 _________________________________________________________________ conv2d_35 (Conv2D) (None, 150, 150, 32) 9248 _________________________________________________________________ batch_normalization_35 (Batc (None, 150, 150, 32) 128 _________________________________________________________________ conv2d_36 (Conv2D) (None, 150, 150, 32) 9248 _________________________________________________________________ batch_normalization_36 (Batc (None, 150, 150, 32) 128 _________________________________________________________________ max_pooling2d_5 (MaxPooling2 (None, 75, 75, 32) 0 _________________________________________________________________ conv2d_37 (Conv2D) (None, 75, 75, 32) 9248 _________________________________________________________________ batch_normalization_37 (Batc (None, 75, 75, 32) 128 _________________________________________________________________ conv2d_38 (Conv2D) (None, 75, 75, 32) 9248 _________________________________________________________________ batch_normalization_38 (Batc (None, 75, 75, 32) 128 _________________________________________________________________ conv2d_39 (Conv2D) (None, 75, 75, 32) 9248 _________________________________________________________________ batch_normalization_39 (Batc (None, 75, 75, 32) 128 _________________________________________________________________ max_pooling2d_6 (MaxPooling2 (None, 37, 37, 32) 0 _________________________________________________________________ conv2d_40 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_40 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_41 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_41 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_42 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_42 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_43 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_43 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_44 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_44 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_45 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_45 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_46 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_46 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_47 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_47 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_48 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_48 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_49 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_49 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_50 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_50 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_51 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_51 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_52 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_52 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_53 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_53 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_54 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_54 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_55 (Conv2D) (None, 37, 37, 32) 1056 _________________________________________________________________ batch_normalization_55 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_56 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_56 (Batc (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_57 (Conv2D) (None, 37, 37, 64) 2112 _________________________________________________________________ batch_normalization_57 (Batc (None, 37, 37, 64) 256 _________________________________________________________________ conv2d_58 (Conv2D) (None, 37, 37, 64) 36928 _________________________________________________________________ batch_normalization_58 (Batc (None, 37, 37, 64) 256 _________________________________________________________________ conv2d_59 (Conv2D) (None, 37, 37, 64) 36928 _________________________________________________________________ batch_normalization_59 (Batc (None, 37, 37, 64) 256 _________________________________________________________________ conv2d_60 (Conv2D) (None, 37, 37, 64) 36928 _________________________________________________________________ batch_normalization_60 (Batc (None, 37, 37, 64) 256 _________________________________________________________________ max_pooling2d_7 (MaxPooling2 (None, 18, 18, 64) 0 _________________________________________________________________ conv2d_61 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_61 (Batc (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_62 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_62 (Batc (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_63 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_63 (Batc (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_64 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_64 (Batc (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_65 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_65 (Batc (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_66 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_66 (Batc (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_67 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_67 (Batc (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_68 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_68 (Batc (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_69 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_69 (Batc (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_70 (Conv2D) (None, 18, 18, 128) 73856 _________________________________________________________________ batch_normalization_70 (Batc (None, 18, 18, 128) 512 _________________________________________________________________ max_pooling2d_8 (MaxPooling2 (None, 9, 9, 128) 0 _________________________________________________________________ global_average_pooling2d_1 ( (None, 128) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 128) 0 _________________________________________________________________ dense_1 (Dense) (None, 6) 774 ================================================================= Total params: 736,038 Trainable params: 732,582 Non-trainable params: 3,456  3.1.3. MODEL 3: visible = Input(shape=(150, 150, 3)) y1 = MaxPooling2D(pool_size= 2)(visible) y1 = Conv2D_block(y1, number_of_layers= 1, filters= 32, kernel_size= 7) y1 = Conv2D_block(y1, number_of_layers= 2, filters= 32, kernel_size= 1) y1 = Conv2D_block(y1, number_of_layers= 2, filters= 64, kernel_size= 3) y2 = Conv2D_block(visible, number_of_layers= 1, filters= 32, kernel_size= 5) y2 = Conv2D_block(y2, number_of_layers= 2, filters= 32, kernel_size=3) y2 = Conv2D_block(y2, number_of_layers= 3, filters= 64, kernel_size=3) y2 = MaxPooling2D(pool_size= 2)(y2) y3 = Conv2D_block(visible, number_of_layers= 2, filters= 32, kernel_size= 3) y3 = Conv2D_block(y3, number_of_layers= 3, filters= 32, kernel_size= 3) y3 = MaxPooling2D(pool_size= 2)(y3) y4 = Conv2D_block(visible, number_of_layers= 1, filters= 32, kernel_size= 1) y4 = Conv2D_block(y4, number_of_layers= 2, filters= 32, kernel_size= 3) y4 = Conv2D_block(y4, number_of_layers= 3, filters= 64, kernel_size= 1) y4 = MaxPooling2D(pool_size= 2)(y4) y5 = concatenate([y1, y2, y3, y4]) y6 = MaxPooling2D(pool_size=2)(y5) y6 = Conv2D_block(y6, number_of_layers= 1, filters= 32, kernel_size= 1) y6 = Conv2D_block(y6, number_of_layers= 1, filters= 32, kernel_size= 3) y6 = Conv2D_block(y6, number_of_layers= 1, filters= 64, kernel_size= 1) y7 = Conv2D_block(y5, number_of_layers= 1, filters= 32, kernel_size= 1) y7 = Conv2D_block(y7, number_of_layers= 1, filters= 32, kernel_size= 3) y7 = Conv2D_block(y7, number_of_layers= 1, filters= 64, kernel_size= 1) y7 = MaxPooling2D(pool_size=2)(y7) y8 = Conv2D_block(y5, number_of_layers= 1, filters= 32, kernel_size= 1) y8 = Conv2D_block(y8, number_of_layers= 1, filters= 32, kernel_size= 3) y8 = Conv2D_block(y8, number_of_layers= 1, filters= 128, kernel_size= 1) y8 = MaxPooling2D(pool_size=2)(y8) y9 = Conv2D_block(y5, number_of_layers= 1, filters= 32, kernel_size= 1) y9 = Conv2D_block(y9, number_of_layers= 1, filters= 32, kernel_size= 3) y9 = Conv2D_block(y9, number_of_layers= 1, filters= 64, kernel_size= 1) y9 = Conv2D_block(y9, number_of_layers= 5, filters= 64, kernel_size= 3) y9 = MaxPooling2D(pool_size=2)(y9) y10 = concatenate([y6, y7, y8, y9]) y11 = Conv2D_block(y10, number_of_layers= 1, filters= 32, kernel_size= 3) y11 = Conv2D_block(y11, number_of_layers= 3, filters= 32, kernel_size= 3) y11 = Conv2D_block(y11, number_of_layers= 3, filters= 64, kernel_size= 3) y11 = MaxPooling2D(pool_size=2)(y11) y12 = Conv2D_block(y10, number_of_layers= 2, filters= 32, kernel_size= 3) y12 = Conv2D_block(y12, number_of_layers= 2, filters= 64, kernel_size= 3) y12 = MaxPooling2D(pool_size=2)(y12) y = concatenate([y11, y12]) y = GlobalAveragePooling2D()(y) y = Dropout(0.3)(y) output = Dense(6, activation='softmax')(y) model_3 = Model(inputs=visible, outputs=output)  3.1.3.1. Model 3 summary Model: \u0026quot;model_3\u0026quot; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_3 (InputLayer) [(None, 150, 150, 3) 0 __________________________________________________________________________________________________ conv2d_76 (Conv2D) (None, 150, 150, 32) 2432 input_3[0][0] __________________________________________________________________________________________________ conv2d_87 (Conv2D) (None, 150, 150, 32) 128 input_3[0][0] __________________________________________________________________________________________________ batch_normalization_76 (BatchNo (None, 150, 150, 32) 128 conv2d_76[0][0] __________________________________________________________________________________________________ batch_normalization_87 (BatchNo (None, 150, 150, 32) 128 conv2d_87[0][0] __________________________________________________________________________________________________ max_pooling2d_9 (MaxPooling2D) (None, 75, 75, 3) 0 input_3[0][0] __________________________________________________________________________________________________ conv2d_77 (Conv2D) (None, 150, 150, 32) 9248 batch_normalization_76[0][0] __________________________________________________________________________________________________ conv2d_82 (Conv2D) (None, 150, 150, 32) 896 input_3[0][0] __________________________________________________________________________________________________ conv2d_88 (Conv2D) (None, 150, 150, 32) 9248 batch_normalization_87[0][0] __________________________________________________________________________________________________ conv2d_71 (Conv2D) (None, 75, 75, 32) 4736 max_pooling2d_9[0][0] __________________________________________________________________________________________________ batch_normalization_77 (BatchNo (None, 150, 150, 32) 128 conv2d_77[0][0] __________________________________________________________________________________________________ batch_normalization_82 (BatchNo (None, 150, 150, 32) 128 conv2d_82[0][0] __________________________________________________________________________________________________ batch_normalization_88 (BatchNo (None, 150, 150, 32) 128 conv2d_88[0][0] __________________________________________________________________________________________________ batch_normalization_71 (BatchNo (None, 75, 75, 32) 128 conv2d_71[0][0] __________________________________________________________________________________________________ conv2d_78 (Conv2D) (None, 150, 150, 32) 9248 batch_normalization_77[0][0] __________________________________________________________________________________________________ conv2d_83 (Conv2D) (None, 150, 150, 32) 9248 batch_normalization_82[0][0] __________________________________________________________________________________________________ conv2d_89 (Conv2D) (None, 150, 150, 32) 9248 batch_normalization_88[0][0] __________________________________________________________________________________________________ conv2d_72 (Conv2D) (None, 75, 75, 32) 1056 batch_normalization_71[0][0] __________________________________________________________________________________________________ batch_normalization_78 (BatchNo (None, 150, 150, 32) 128 conv2d_78[0][0] __________________________________________________________________________________________________ batch_normalization_83 (BatchNo (None, 150, 150, 32) 128 conv2d_83[0][0] __________________________________________________________________________________________________ batch_normalization_89 (BatchNo (None, 150, 150, 32) 128 conv2d_89[0][0] __________________________________________________________________________________________________ batch_normalization_72 (BatchNo (None, 75, 75, 32) 128 conv2d_72[0][0] __________________________________________________________________________________________________ conv2d_79 (Conv2D) (None, 150, 150, 64) 18496 batch_normalization_78[0][0] __________________________________________________________________________________________________ conv2d_84 (Conv2D) (None, 150, 150, 32) 9248 batch_normalization_83[0][0] __________________________________________________________________________________________________ conv2d_90 (Conv2D) (None, 150, 150, 64) 2112 batch_normalization_89[0][0] __________________________________________________________________________________________________ conv2d_73 (Conv2D) (None, 75, 75, 32) 1056 batch_normalization_72[0][0] __________________________________________________________________________________________________ batch_normalization_79 (BatchNo (None, 150, 150, 64) 256 conv2d_79[0][0] __________________________________________________________________________________________________ batch_normalization_84 (BatchNo (None, 150, 150, 32) 128 conv2d_84[0][0] __________________________________________________________________________________________________ batch_normalization_90 (BatchNo (None, 150, 150, 64) 256 conv2d_90[0][0] __________________________________________________________________________________________________ batch_normalization_73 (BatchNo (None, 75, 75, 32) 128 conv2d_73[0][0] __________________________________________________________________________________________________ conv2d_80 (Conv2D) (None, 150, 150, 64) 36928 batch_normalization_79[0][0] __________________________________________________________________________________________________ conv2d_85 (Conv2D) (None, 150, 150, 32) 9248 batch_normalization_84[0][0] __________________________________________________________________________________________________ conv2d_91 (Conv2D) (None, 150, 150, 64) 4160 batch_normalization_90[0][0] __________________________________________________________________________________________________ conv2d_74 (Conv2D) (None, 75, 75, 64) 18496 batch_normalization_73[0][0] __________________________________________________________________________________________________ batch_normalization_80 (BatchNo (None, 150, 150, 64) 256 conv2d_80[0][0] __________________________________________________________________________________________________ batch_normalization_85 (BatchNo (None, 150, 150, 32) 128 conv2d_85[0][0] __________________________________________________________________________________________________ batch_normalization_91 (BatchNo (None, 150, 150, 64) 256 conv2d_91[0][0] __________________________________________________________________________________________________ batch_normalization_74 (BatchNo (None, 75, 75, 64) 256 conv2d_74[0][0] __________________________________________________________________________________________________ conv2d_81 (Conv2D) (None, 150, 150, 64) 36928 batch_normalization_80[0][0] __________________________________________________________________________________________________ conv2d_86 (Conv2D) (None, 150, 150, 32) 9248 batch_normalization_85[0][0] __________________________________________________________________________________________________ conv2d_92 (Conv2D) (None, 150, 150, 64) 4160 batch_normalization_91[0][0] __________________________________________________________________________________________________ conv2d_75 (Conv2D) (None, 75, 75, 64) 36928 batch_normalization_74[0][0] __________________________________________________________________________________________________ batch_normalization_81 (BatchNo (None, 150, 150, 64) 256 conv2d_81[0][0] __________________________________________________________________________________________________ batch_normalization_86 (BatchNo (None, 150, 150, 32) 128 conv2d_86[0][0] __________________________________________________________________________________________________ batch_normalization_92 (BatchNo (None, 150, 150, 64) 256 conv2d_92[0][0] __________________________________________________________________________________________________ batch_normalization_75 (BatchNo (None, 75, 75, 64) 256 conv2d_75[0][0] __________________________________________________________________________________________________ max_pooling2d_10 (MaxPooling2D) (None, 75, 75, 64) 0 batch_normalization_81[0][0] __________________________________________________________________________________________________ max_pooling2d_11 (MaxPooling2D) (None, 75, 75, 32) 0 batch_normalization_86[0][0] __________________________________________________________________________________________________ max_pooling2d_12 (MaxPooling2D) (None, 75, 75, 64) 0 batch_normalization_92[0][0] __________________________________________________________________________________________________ concatenate (Concatenate) (None, 75, 75, 224) 0 batch_normalization_75[0][0] max_pooling2d_10[0][0] max_pooling2d_11[0][0] max_pooling2d_12[0][0] __________________________________________________________________________________________________ conv2d_102 (Conv2D) (None, 75, 75, 32) 7200 concatenate[0][0] __________________________________________________________________________________________________ batch_normalization_102 (BatchN (None, 75, 75, 32) 128 conv2d_102[0][0] __________________________________________________________________________________________________ conv2d_103 (Conv2D) (None, 75, 75, 32) 9248 batch_normalization_102[0][0] __________________________________________________________________________________________________ batch_normalization_103 (BatchN (None, 75, 75, 32) 128 conv2d_103[0][0] __________________________________________________________________________________________________ conv2d_104 (Conv2D) (None, 75, 75, 64) 2112 batch_normalization_103[0][0] __________________________________________________________________________________________________ batch_normalization_104 (BatchN (None, 75, 75, 64) 256 conv2d_104[0][0] __________________________________________________________________________________________________ conv2d_105 (Conv2D) (None, 75, 75, 64) 36928 batch_normalization_104[0][0] __________________________________________________________________________________________________ batch_normalization_105 (BatchN (None, 75, 75, 64) 256 conv2d_105[0][0] __________________________________________________________________________________________________ conv2d_106 (Conv2D) (None, 75, 75, 64) 36928 batch_normalization_105[0][0] __________________________________________________________________________________________________ batch_normalization_106 (BatchN (None, 75, 75, 64) 256 conv2d_106[0][0] __________________________________________________________________________________________________ max_pooling2d_13 (MaxPooling2D) (None, 37, 37, 224) 0 concatenate[0][0] __________________________________________________________________________________________________ conv2d_96 (Conv2D) (None, 75, 75, 32) 7200 concatenate[0][0] __________________________________________________________________________________________________ conv2d_99 (Conv2D) (None, 75, 75, 32) 7200 concatenate[0][0] __________________________________________________________________________________________________ conv2d_107 (Conv2D) (None, 75, 75, 64) 36928 batch_normalization_106[0][0] __________________________________________________________________________________________________ conv2d_93 (Conv2D) (None, 37, 37, 32) 7200 max_pooling2d_13[0][0] __________________________________________________________________________________________________ batch_normalization_96 (BatchNo (None, 75, 75, 32) 128 conv2d_96[0][0] __________________________________________________________________________________________________ batch_normalization_99 (BatchNo (None, 75, 75, 32) 128 conv2d_99[0][0] __________________________________________________________________________________________________ batch_normalization_107 (BatchN (None, 75, 75, 64) 256 conv2d_107[0][0] __________________________________________________________________________________________________ batch_normalization_93 (BatchNo (None, 37, 37, 32) 128 conv2d_93[0][0] __________________________________________________________________________________________________ conv2d_97 (Conv2D) (None, 75, 75, 32) 9248 batch_normalization_96[0][0] __________________________________________________________________________________________________ conv2d_100 (Conv2D) (None, 75, 75, 32) 9248 batch_normalization_99[0][0] __________________________________________________________________________________________________ conv2d_108 (Conv2D) (None, 75, 75, 64) 36928 batch_normalization_107[0][0] __________________________________________________________________________________________________ conv2d_94 (Conv2D) (None, 37, 37, 32) 9248 batch_normalization_93[0][0] __________________________________________________________________________________________________ batch_normalization_97 (BatchNo (None, 75, 75, 32) 128 conv2d_97[0][0] __________________________________________________________________________________________________ batch_normalization_100 (BatchN (None, 75, 75, 32) 128 conv2d_100[0][0] __________________________________________________________________________________________________ batch_normalization_108 (BatchN (None, 75, 75, 64) 256 conv2d_108[0][0] __________________________________________________________________________________________________ batch_normalization_94 (BatchNo (None, 37, 37, 32) 128 conv2d_94[0][0] __________________________________________________________________________________________________ conv2d_98 (Conv2D) (None, 75, 75, 64) 2112 batch_normalization_97[0][0] __________________________________________________________________________________________________ conv2d_101 (Conv2D) (None, 75, 75, 128) 4224 batch_normalization_100[0][0] __________________________________________________________________________________________________ conv2d_109 (Conv2D) (None, 75, 75, 64) 36928 batch_normalization_108[0][0] __________________________________________________________________________________________________ conv2d_95 (Conv2D) (None, 37, 37, 64) 2112 batch_normalization_94[0][0] __________________________________________________________________________________________________ batch_normalization_98 (BatchNo (None, 75, 75, 64) 256 conv2d_98[0][0] __________________________________________________________________________________________________ batch_normalization_101 (BatchN (None, 75, 75, 128) 512 conv2d_101[0][0] __________________________________________________________________________________________________ batch_normalization_109 (BatchN (None, 75, 75, 64) 256 conv2d_109[0][0] __________________________________________________________________________________________________ batch_normalization_95 (BatchNo (None, 37, 37, 64) 256 conv2d_95[0][0] __________________________________________________________________________________________________ max_pooling2d_14 (MaxPooling2D) (None, 37, 37, 64) 0 batch_normalization_98[0][0] __________________________________________________________________________________________________ max_pooling2d_15 (MaxPooling2D) (None, 37, 37, 128) 0 batch_normalization_101[0][0] __________________________________________________________________________________________________ max_pooling2d_16 (MaxPooling2D) (None, 37, 37, 64) 0 batch_normalization_109[0][0] __________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 37, 37, 320) 0 batch_normalization_95[0][0] max_pooling2d_14[0][0] max_pooling2d_15[0][0] max_pooling2d_16[0][0] __________________________________________________________________________________________________ conv2d_110 (Conv2D) (None, 37, 37, 32) 92192 concatenate_1[0][0] __________________________________________________________________________________________________ batch_normalization_110 (BatchN (None, 37, 37, 32) 128 conv2d_110[0][0] __________________________________________________________________________________________________ conv2d_111 (Conv2D) (None, 37, 37, 32) 9248 batch_normalization_110[0][0] __________________________________________________________________________________________________ batch_normalization_111 (BatchN (None, 37, 37, 32) 128 conv2d_111[0][0] __________________________________________________________________________________________________ conv2d_112 (Conv2D) (None, 37, 37, 32) 9248 batch_normalization_111[0][0] __________________________________________________________________________________________________ batch_normalization_112 (BatchN (None, 37, 37, 32) 128 conv2d_112[0][0] __________________________________________________________________________________________________ conv2d_113 (Conv2D) (None, 37, 37, 32) 9248 batch_normalization_112[0][0] __________________________________________________________________________________________________ conv2d_117 (Conv2D) (None, 37, 37, 32) 92192 concatenate_1[0][0] __________________________________________________________________________________________________ batch_normalization_113 (BatchN (None, 37, 37, 32) 128 conv2d_113[0][0] __________________________________________________________________________________________________ batch_normalization_117 (BatchN (None, 37, 37, 32) 128 conv2d_117[0][0] __________________________________________________________________________________________________ conv2d_114 (Conv2D) (None, 37, 37, 64) 18496 batch_normalization_113[0][0] __________________________________________________________________________________________________ conv2d_118 (Conv2D) (None, 37, 37, 32) 9248 batch_normalization_117[0][0] __________________________________________________________________________________________________ batch_normalization_114 (BatchN (None, 37, 37, 64) 256 conv2d_114[0][0] __________________________________________________________________________________________________ batch_normalization_118 (BatchN (None, 37, 37, 32) 128 conv2d_118[0][0] __________________________________________________________________________________________________ conv2d_115 (Conv2D) (None, 37, 37, 64) 36928 batch_normalization_114[0][0] __________________________________________________________________________________________________ conv2d_119 (Conv2D) (None, 37, 37, 64) 18496 batch_normalization_118[0][0] __________________________________________________________________________________________________ batch_normalization_115 (BatchN (None, 37, 37, 64) 256 conv2d_115[0][0] __________________________________________________________________________________________________ batch_normalization_119 (BatchN (None, 37, 37, 64) 256 conv2d_119[0][0] __________________________________________________________________________________________________ conv2d_116 (Conv2D) (None, 37, 37, 64) 36928 batch_normalization_115[0][0] __________________________________________________________________________________________________ conv2d_120 (Conv2D) (None, 37, 37, 64) 36928 batch_normalization_119[0][0] __________________________________________________________________________________________________ batch_normalization_116 (BatchN (None, 37, 37, 64) 256 conv2d_116[0][0] __________________________________________________________________________________________________ batch_normalization_120 (BatchN (None, 37, 37, 64) 256 conv2d_120[0][0] __________________________________________________________________________________________________ max_pooling2d_17 (MaxPooling2D) (None, 18, 18, 64) 0 batch_normalization_116[0][0] __________________________________________________________________________________________________ max_pooling2d_18 (MaxPooling2D) (None, 18, 18, 64) 0 batch_normalization_120[0][0] __________________________________________________________________________________________________ concatenate_2 (Concatenate) (None, 18, 18, 128) 0 max_pooling2d_17[0][0] max_pooling2d_18[0][0] __________________________________________________________________________________________________ global_average_pooling2d_2 (Glo (None, 128) 0 concatenate_2[0][0] __________________________________________________________________________________________________ dropout_2 (Dropout) (None, 128) 0 global_average_pooling2d_2[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 6) 774 dropout_2[0][0] ================================================================================================== Total params: 882,886 Trainable params: 878,150 Non-trainable params: 4,736  3.1.4. MODEL 4: # ------------------------MODEL 4------------------------------- visible = Input(shape=(150, 150, 3)) y = Conv2D_block(visible, number_of_layers= 1, filters= 32, kernel_size= 7) y = Conv2D_block(y, number_of_layers= 2, filters= 32, kernel_size= 3) y = MaxPooling2D(pool_size= 2)(y) y = Conv2D_block(y, number_of_layers= 3, filters= 32, kernel_size= 3) y = Conv2D_block(y, number_of_layers= 2, filters= 32, kernel_size= 3) y = MaxPooling2D(pool_size= 2)(y) y = Conv2D_block(y, number_of_layers= 1, filters= 32, kernel_size= 1) y = Conv2D_block(y, number_of_layers= 1, filters= 32, kernel_size= 3) y = Conv2D_block(y, number_of_layers= 1, filters= 64, kernel_size= 1) y = Conv2D_block(y, number_of_layers= 3, filters= 64, kernel_size= 3) y = MaxPooling2D(pool_size= 2)(y) y = Conv2D_block(y, number_of_layers= 3, filters= 64, kernel_size= 3) y = Conv2D_block(y, number_of_layers= 5, filters= 64, kernel_size= 3) y = Conv2D_block(y, number_of_layers= 3, filters= 128, kernel_size= 3) y = MaxPooling2D(pool_size= 2)(y) y = Conv2D_block(y, number_of_layers= 3, filters= 128, kernel_size= 3) y = Conv2D_block(y, number_of_layers= 3, filters= 256, kernel_size= 3) y = GlobalAveragePooling2D()(y) y = Dropout(0.5)(y) output = Dense(6, activation='softmax')(y) model_4 = Model(inputs=visible, outputs=output)  3.1.4.1. Model 4 summary Model: \u0026quot;model_4\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_4 (InputLayer) [(None, 150, 150, 3)] 0 _________________________________________________________________ conv2d_121 (Conv2D) (None, 150, 150, 32) 4736 _________________________________________________________________ batch_normalization_121 (Bat (None, 150, 150, 32) 128 _________________________________________________________________ conv2d_122 (Conv2D) (None, 150, 150, 32) 9248 _________________________________________________________________ batch_normalization_122 (Bat (None, 150, 150, 32) 128 _________________________________________________________________ conv2d_123 (Conv2D) (None, 150, 150, 32) 9248 _________________________________________________________________ batch_normalization_123 (Bat (None, 150, 150, 32) 128 _________________________________________________________________ max_pooling2d_19 (MaxPooling (None, 75, 75, 32) 0 _________________________________________________________________ conv2d_124 (Conv2D) (None, 75, 75, 32) 9248 _________________________________________________________________ batch_normalization_124 (Bat (None, 75, 75, 32) 128 _________________________________________________________________ conv2d_125 (Conv2D) (None, 75, 75, 32) 9248 _________________________________________________________________ batch_normalization_125 (Bat (None, 75, 75, 32) 128 _________________________________________________________________ conv2d_126 (Conv2D) (None, 75, 75, 32) 9248 _________________________________________________________________ batch_normalization_126 (Bat (None, 75, 75, 32) 128 _________________________________________________________________ conv2d_127 (Conv2D) (None, 75, 75, 32) 9248 _________________________________________________________________ batch_normalization_127 (Bat (None, 75, 75, 32) 128 _________________________________________________________________ conv2d_128 (Conv2D) (None, 75, 75, 32) 9248 _________________________________________________________________ batch_normalization_128 (Bat (None, 75, 75, 32) 128 _________________________________________________________________ max_pooling2d_20 (MaxPooling (None, 37, 37, 32) 0 _________________________________________________________________ conv2d_129 (Conv2D) (None, 37, 37, 32) 1056 _________________________________________________________________ batch_normalization_129 (Bat (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_130 (Conv2D) (None, 37, 37, 32) 9248 _________________________________________________________________ batch_normalization_130 (Bat (None, 37, 37, 32) 128 _________________________________________________________________ conv2d_131 (Conv2D) (None, 37, 37, 64) 2112 _________________________________________________________________ batch_normalization_131 (Bat (None, 37, 37, 64) 256 _________________________________________________________________ conv2d_132 (Conv2D) (None, 37, 37, 64) 36928 _________________________________________________________________ batch_normalization_132 (Bat (None, 37, 37, 64) 256 _________________________________________________________________ conv2d_133 (Conv2D) (None, 37, 37, 64) 36928 _________________________________________________________________ batch_normalization_133 (Bat (None, 37, 37, 64) 256 _________________________________________________________________ conv2d_134 (Conv2D) (None, 37, 37, 64) 36928 _________________________________________________________________ batch_normalization_134 (Bat (None, 37, 37, 64) 256 _________________________________________________________________ max_pooling2d_21 (MaxPooling (None, 18, 18, 64) 0 _________________________________________________________________ conv2d_135 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_135 (Bat (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_136 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_136 (Bat (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_137 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_137 (Bat (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_138 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_138 (Bat (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_139 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_139 (Bat (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_140 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_140 (Bat (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_141 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_141 (Bat (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_142 (Conv2D) (None, 18, 18, 64) 36928 _________________________________________________________________ batch_normalization_142 (Bat (None, 18, 18, 64) 256 _________________________________________________________________ conv2d_143 (Conv2D) (None, 18, 18, 128) 73856 _________________________________________________________________ batch_normalization_143 (Bat (None, 18, 18, 128) 512 _________________________________________________________________ conv2d_144 (Conv2D) (None, 18, 18, 128) 147584 _________________________________________________________________ batch_normalization_144 (Bat (None, 18, 18, 128) 512 _________________________________________________________________ conv2d_145 (Conv2D) (None, 18, 18, 128) 147584 _________________________________________________________________ batch_normalization_145 (Bat (None, 18, 18, 128) 512 _________________________________________________________________ max_pooling2d_22 (MaxPooling (None, 9, 9, 128) 0 _________________________________________________________________ conv2d_146 (Conv2D) (None, 9, 9, 128) 147584 _________________________________________________________________ batch_normalization_146 (Bat (None, 9, 9, 128) 512 _________________________________________________________________ conv2d_147 (Conv2D) (None, 9, 9, 128) 147584 _________________________________________________________________ batch_normalization_147 (Bat (None, 9, 9, 128) 512 _________________________________________________________________ conv2d_148 (Conv2D) (None, 9, 9, 128) 147584 _________________________________________________________________ batch_normalization_148 (Bat (None, 9, 9, 128) 512 _________________________________________________________________ conv2d_149 (Conv2D) (None, 9, 9, 256) 295168 _________________________________________________________________ batch_normalization_149 (Bat (None, 9, 9, 256) 1024 _________________________________________________________________ conv2d_150 (Conv2D) (None, 9, 9, 256) 590080 _________________________________________________________________ batch_normalization_150 (Bat (None, 9, 9, 256) 1024 _________________________________________________________________ conv2d_151 (Conv2D) (None, 9, 9, 256) 590080 _________________________________________________________________ batch_normalization_151 (Bat (None, 9, 9, 256) 1024 _________________________________________________________________ global_average_pooling2d_3 ( (None, 256) 0 _________________________________________________________________ dropout_3 (Dropout) (None, 256) 0 _________________________________________________________________ dense_3 (Dense) (None, 6) 1542 ================================================================= Total params: 2,787,238 Trainable params: 2,781,990 Non-trainable params: 5,248  3.1.5. MODEL 5: # ------------------------MODEL 5------------------------------- visible = Input(shape=(150, 150, 3)) y1 = Conv2D_block(visible, number_of_layers= 1, filters= 32, kernel_size= 5) y1 = Conv2D_block(y1, number_of_layers= 1, filters= 32, kernel_size= 3) y1 = Conv2D_block(y1, number_of_layers= 3, filters= 128, kernel_size= 1) y1 = MaxPooling2D(pool_size= 2)(y1) y2 = Conv2D_block(visible, number_of_layers= 1, filters= 32, kernel_size= 5) y2 = Conv2D_block(y2, number_of_layers= 1, filters= 32, kernel_size= 1) y2 = Conv2D_block(y2, number_of_layers= 2, filters= 32, kernel_size= 3) y2 = Conv2D_block(y2, number_of_layers= 3, filters= 128, kernel_size= 1) y2 = MaxPooling2D(pool_size= 2)(y2) y3 = Conv2D_block(visible, number_of_layers= 1, filters= 32, kernel_size= 1) y3 = Conv2D_block(y3, number_of_layers= 1, filters= 32, kernel_size= 3) y3 = Conv2D_block(y3, number_of_layers= 2, filters= 64, kernel_size= 1) y3 = MaxPooling2D(pool_size= 2)(y3) y4 = concatenate([y1, y2, y3]) y5 = Conv2D_block(y4, number_of_layers= 1, filters= 32, kernel_size= 3) y5 = Conv2D_block(y5, number_of_layers= 3, filters= 32, kernel_size= 3) y5 = MaxPooling2D(pool_size= 2)(y5) y5 = Conv2D_block(y5, number_of_layers= 3, filters= 64, kernel_size= 3) y5 = MaxPooling2D(pool_size= 2)(y5) y6 = Conv2D_block(y4, number_of_layers= 1, filters= 32, kernel_size= 3) y6 = Conv2D_block(y6, number_of_layers= 2, filters= 32, kernel_size= 3) y6 = Conv2D_block(y6, number_of_layers= 2, filters= 64, kernel_size= 3) y6 = MaxPooling2D(pool_size= 2)(y6) y6 = Conv2D_block(y6, number_of_layers= 2, filters= 64, kernel_size= 3) y6 = MaxPooling2D(pool_size= 2)(y6) y7 = concatenate([y5, y6]) y7 = MaxPooling2D(pool_size= 2)(y7) y8 = Conv2D_block(y7, number_of_layers= 1, filters= 32, kernel_size= 3) y8 = Conv2D_block(y8, number_of_layers= 1, filters= 64, kernel_size= 3) y8 = MaxPooling2D(pool_size= 2)(y8) y9 = Conv2D_block(y7, number_of_layers= 2, filters= 64, kernel_size= 3) y9 = MaxPooling2D(pool_size= 2)(y9) y = concatenate([y8, y9]) y = GlobalAveragePooling2D()(y) y = Dropout(0.5)(y) output = Dense(6, activation='softmax')(y) model_5 = Model(inputs=visible, outputs=output)  3.1.5.1 Model 5 summary Model: \u0026quot;model_5\u0026quot; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_5 (InputLayer) [(None, 150, 150, 3) 0 __________________________________________________________________________________________________ conv2d_157 (Conv2D) (None, 150, 150, 32) 2432 input_5[0][0] __________________________________________________________________________________________________ batch_normalization_157 (BatchN (None, 150, 150, 32) 128 conv2d_157[0][0] __________________________________________________________________________________________________ conv2d_158 (Conv2D) (None, 150, 150, 32) 1056 batch_normalization_157[0][0] __________________________________________________________________________________________________ batch_normalization_158 (BatchN (None, 150, 150, 32) 128 conv2d_158[0][0] __________________________________________________________________________________________________ conv2d_152 (Conv2D) (None, 150, 150, 32) 2432 input_5[0][0] __________________________________________________________________________________________________ conv2d_159 (Conv2D) (None, 150, 150, 32) 9248 batch_normalization_158[0][0] __________________________________________________________________________________________________ batch_normalization_152 (BatchN (None, 150, 150, 32) 128 conv2d_152[0][0] __________________________________________________________________________________________________ batch_normalization_159 (BatchN (None, 150, 150, 32) 128 conv2d_159[0][0] __________________________________________________________________________________________________ conv2d_153 (Conv2D) (None, 150, 150, 32) 9248 batch_normalization_152[0][0] __________________________________________________________________________________________________ conv2d_160 (Conv2D) (None, 150, 150, 32) 9248 batch_normalization_159[0][0] __________________________________________________________________________________________________ conv2d_164 (Conv2D) (None, 150, 150, 32) 128 input_5[0][0] __________________________________________________________________________________________________ batch_normalization_153 (BatchN (None, 150, 150, 32) 128 conv2d_153[0][0] __________________________________________________________________________________________________ batch_normalization_160 (BatchN (None, 150, 150, 32) 128 conv2d_160[0][0] __________________________________________________________________________________________________ batch_normalization_164 (BatchN (None, 150, 150, 32) 128 conv2d_164[0][0] __________________________________________________________________________________________________ conv2d_154 (Conv2D) (None, 150, 150, 128 4224 batch_normalization_153[0][0] __________________________________________________________________________________________________ conv2d_161 (Conv2D) (None, 150, 150, 128 4224 batch_normalization_160[0][0] __________________________________________________________________________________________________ conv2d_165 (Conv2D) (None, 150, 150, 32) 9248 batch_normalization_164[0][0] __________________________________________________________________________________________________ batch_normalization_154 (BatchN (None, 150, 150, 128 512 conv2d_154[0][0] __________________________________________________________________________________________________ batch_normalization_161 (BatchN (None, 150, 150, 128 512 conv2d_161[0][0] __________________________________________________________________________________________________ batch_normalization_165 (BatchN (None, 150, 150, 32) 128 conv2d_165[0][0] __________________________________________________________________________________________________ conv2d_155 (Conv2D) (None, 150, 150, 128 16512 batch_normalization_154[0][0] __________________________________________________________________________________________________ conv2d_162 (Conv2D) (None, 150, 150, 128 16512 batch_normalization_161[0][0] __________________________________________________________________________________________________ conv2d_166 (Conv2D) (None, 150, 150, 64) 2112 batch_normalization_165[0][0] __________________________________________________________________________________________________ batch_normalization_155 (BatchN (None, 150, 150, 128 512 conv2d_155[0][0] __________________________________________________________________________________________________ batch_normalization_162 (BatchN (None, 150, 150, 128 512 conv2d_162[0][0] __________________________________________________________________________________________________ batch_normalization_166 (BatchN (None, 150, 150, 64) 256 conv2d_166[0][0] __________________________________________________________________________________________________ conv2d_156 (Conv2D) (None, 150, 150, 128 16512 batch_normalization_155[0][0] __________________________________________________________________________________________________ conv2d_163 (Conv2D) (None, 150, 150, 128 16512 batch_normalization_162[0][0] __________________________________________________________________________________________________ conv2d_167 (Conv2D) (None, 150, 150, 64) 4160 batch_normalization_166[0][0] __________________________________________________________________________________________________ batch_normalization_156 (BatchN (None, 150, 150, 128 512 conv2d_156[0][0] __________________________________________________________________________________________________ batch_normalization_163 (BatchN (None, 150, 150, 128 512 conv2d_163[0][0] __________________________________________________________________________________________________ batch_normalization_167 (BatchN (None, 150, 150, 64) 256 conv2d_167[0][0] __________________________________________________________________________________________________ max_pooling2d_23 (MaxPooling2D) (None, 75, 75, 128) 0 batch_normalization_156[0][0] __________________________________________________________________________________________________ max_pooling2d_24 (MaxPooling2D) (None, 75, 75, 128) 0 batch_normalization_163[0][0] __________________________________________________________________________________________________ max_pooling2d_25 (MaxPooling2D) (None, 75, 75, 64) 0 batch_normalization_167[0][0] __________________________________________________________________________________________________ concatenate_3 (Concatenate) (None, 75, 75, 320) 0 max_pooling2d_23[0][0] max_pooling2d_24[0][0] max_pooling2d_25[0][0] __________________________________________________________________________________________________ conv2d_168 (Conv2D) (None, 75, 75, 32) 92192 concatenate_3[0][0] __________________________________________________________________________________________________ conv2d_175 (Conv2D) (None, 75, 75, 32) 92192 concatenate_3[0][0] __________________________________________________________________________________________________ batch_normalization_168 (BatchN (None, 75, 75, 32) 128 conv2d_168[0][0] __________________________________________________________________________________________________ batch_normalization_175 (BatchN (None, 75, 75, 32) 128 conv2d_175[0][0] __________________________________________________________________________________________________ conv2d_169 (Conv2D) (None, 75, 75, 32) 9248 batch_normalization_168[0][0] __________________________________________________________________________________________________ conv2d_176 (Conv2D) (None, 75, 75, 32) 9248 batch_normalization_175[0][0] __________________________________________________________________________________________________ batch_normalization_169 (BatchN (None, 75, 75, 32) 128 conv2d_169[0][0] __________________________________________________________________________________________________ batch_normalization_176 (BatchN (None, 75, 75, 32) 128 conv2d_176[0][0] __________________________________________________________________________________________________ conv2d_170 (Conv2D) (None, 75, 75, 32) 9248 batch_normalization_169[0][0] __________________________________________________________________________________________________ conv2d_177 (Conv2D) (None, 75, 75, 32) 9248 batch_normalization_176[0][0] __________________________________________________________________________________________________ batch_normalization_170 (BatchN (None, 75, 75, 32) 128 conv2d_170[0][0] __________________________________________________________________________________________________ batch_normalization_177 (BatchN (None, 75, 75, 32) 128 conv2d_177[0][0] __________________________________________________________________________________________________ conv2d_171 (Conv2D) (None, 75, 75, 32) 9248 batch_normalization_170[0][0] __________________________________________________________________________________________________ conv2d_178 (Conv2D) (None, 75, 75, 64) 18496 batch_normalization_177[0][0] __________________________________________________________________________________________________ batch_normalization_171 (BatchN (None, 75, 75, 32) 128 conv2d_171[0][0] __________________________________________________________________________________________________ batch_normalization_178 (BatchN (None, 75, 75, 64) 256 conv2d_178[0][0] __________________________________________________________________________________________________ max_pooling2d_26 (MaxPooling2D) (None, 37, 37, 32) 0 batch_normalization_171[0][0] __________________________________________________________________________________________________ conv2d_179 (Conv2D) (None, 75, 75, 64) 36928 batch_normalization_178[0][0] __________________________________________________________________________________________________ conv2d_172 (Conv2D) (None, 37, 37, 64) 18496 max_pooling2d_26[0][0] __________________________________________________________________________________________________ batch_normalization_179 (BatchN (None, 75, 75, 64) 256 conv2d_179[0][0] __________________________________________________________________________________________________ batch_normalization_172 (BatchN (None, 37, 37, 64) 256 conv2d_172[0][0] __________________________________________________________________________________________________ max_pooling2d_28 (MaxPooling2D) (None, 37, 37, 64) 0 batch_normalization_179[0][0] __________________________________________________________________________________________________ conv2d_173 (Conv2D) (None, 37, 37, 64) 36928 batch_normalization_172[0][0] __________________________________________________________________________________________________ conv2d_180 (Conv2D) (None, 37, 37, 64) 36928 max_pooling2d_28[0][0] __________________________________________________________________________________________________ batch_normalization_173 (BatchN (None, 37, 37, 64) 256 conv2d_173[0][0] __________________________________________________________________________________________________ batch_normalization_180 (BatchN (None, 37, 37, 64) 256 conv2d_180[0][0] __________________________________________________________________________________________________ conv2d_174 (Conv2D) (None, 37, 37, 64) 36928 batch_normalization_173[0][0] __________________________________________________________________________________________________ conv2d_181 (Conv2D) (None, 37, 37, 64) 36928 batch_normalization_180[0][0] __________________________________________________________________________________________________ batch_normalization_174 (BatchN (None, 37, 37, 64) 256 conv2d_174[0][0] __________________________________________________________________________________________________ batch_normalization_181 (BatchN (None, 37, 37, 64) 256 conv2d_181[0][0] __________________________________________________________________________________________________ max_pooling2d_27 (MaxPooling2D) (None, 18, 18, 64) 0 batch_normalization_174[0][0] __________________________________________________________________________________________________ max_pooling2d_29 (MaxPooling2D) (None, 18, 18, 64) 0 batch_normalization_181[0][0] __________________________________________________________________________________________________ concatenate_4 (Concatenate) (None, 18, 18, 128) 0 max_pooling2d_27[0][0] max_pooling2d_29[0][0] __________________________________________________________________________________________________ max_pooling2d_30 (MaxPooling2D) (None, 9, 9, 128) 0 concatenate_4[0][0] __________________________________________________________________________________________________ conv2d_182 (Conv2D) (None, 9, 9, 32) 36896 max_pooling2d_30[0][0] __________________________________________________________________________________________________ conv2d_184 (Conv2D) (None, 9, 9, 64) 73792 max_pooling2d_30[0][0] __________________________________________________________________________________________________ batch_normalization_182 (BatchN (None, 9, 9, 32) 128 conv2d_182[0][0] __________________________________________________________________________________________________ batch_normalization_184 (BatchN (None, 9, 9, 64) 256 conv2d_184[0][0] __________________________________________________________________________________________________ conv2d_183 (Conv2D) (None, 9, 9, 64) 18496 batch_normalization_182[0][0] __________________________________________________________________________________________________ conv2d_185 (Conv2D) (None, 9, 9, 64) 36928 batch_normalization_184[0][0] __________________________________________________________________________________________________ batch_normalization_183 (BatchN (None, 9, 9, 64) 256 conv2d_183[0][0] __________________________________________________________________________________________________ batch_normalization_185 (BatchN (None, 9, 9, 64) 256 conv2d_185[0][0] __________________________________________________________________________________________________ max_pooling2d_31 (MaxPooling2D) (None, 4, 4, 64) 0 batch_normalization_183[0][0] __________________________________________________________________________________________________ max_pooling2d_32 (MaxPooling2D) (None, 4, 4, 64) 0 batch_normalization_185[0][0] __________________________________________________________________________________________________ concatenate_5 (Concatenate) (None, 4, 4, 128) 0 max_pooling2d_31[0][0] max_pooling2d_32[0][0] __________________________________________________________________________________________________ global_average_pooling2d_4 (Glo (None, 128) 0 concatenate_5[0][0] __________________________________________________________________________________________________ dropout_4 (Dropout) (None, 128) 0 global_average_pooling2d_4[0][0] __________________________________________________________________________________________________ dense_4 (Dense) (None, 6) 774 dropout_4[0][0] ================================================================================================== Total params: 751,142 Trainable params: 747,046 Non-trainable params: 4,096  3.2. Creating objects modelObj_1 = Network(model_1, epochs=100, optimizer=Adam(lr=1e-4)) modelObj_2 = Network(model_2, epochs=100, optimizer=Adam(lr=1e-4)) modelObj_3 = Network(model_3, epochs=100, optimizer=Adam(lr=1e-4)) modelObj_4 = Network(model_4, epochs=100, optimizer=Adam(lr=1e-4)) modelObj_5 = Network(model_5, epochs=100, optimizer=Adam(lr=1e-4))  3.3. Networks training Model 1:\nmodelObj_1.train(verbose=0) Czas trenowania sieci: 1224.83913064003 sek Loss: 1.0283 Accuracy: 0.6278  Model 2:\nmodelObj_2.train(verbose=0) Czas trenowania sieci: 1252.3253486156464 sek Loss: 0.7970 Accuracy: 0.7116  Model 3:\nmodelObj_3.train(verbose=0) Czas trenowania sieci: 2207.768621444702 sek Loss: 0.3910 Accuracy: 0.8722  Model 4:\nmodelObj_4.train(verbose=0) Czas trenowania sieci: 1256.7936658859253 sek Loss: 0.6919 Accuracy: 0.7599  Model 5:\nmodelObj_5.train(verbose=0) Czas trenowania sieci: 2113.3248126506805 sek Loss: 0.6122 Accuracy: 0.7827  3.4. Models saving # modelObj_1.save_model('model_1_L0.43_A0.85') # modelObj_2.save_model('model_2_L0.43_A0.85') # modelObj_3.save_model('model_3_L0.33_A0.88') # modelObj_4.save_model('model_4_L0.51_A0.82') # modelObj_5.save_model('model_5_L0.46_A0.86')  3.5. The characteristics of the trained models modelObj_1.plot_hist(title='MODEL 1') modelObj_2.plot_hist(title='MODEL 2') modelObj_3.plot_hist(title='MODEL 3') modelObj_4.plot_hist(title='MODEL 4') modelObj_5.plot_hist(title='MODEL 5')  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/imageclassproject/project_imageclass_neural_networks/","summary":"3. Neural networks: class Network: def __init__(self, model_name, epochs, optimizer): self.model_name = model_name self.epochs = epochs self.optimizer = optimizer self.tz = 'CEST' def model_summary(self): self.model_name.summary() def save_model(self, file_name): self.models_dir = '/content/drive/My Drive/ColabNotebooks/intel_image/models/' + dt.now().strftime('%Y-%m-%d/') if not os.path.exists(self.models_dir): os.makedirs(self.models_dir) self.model_name.save(self.models_dir + file_name + '.h5') def train(self, verbose=0, ): self.model_name.compile(optimizer= self.optimizer, loss='categorical_crossentropy', metrics=['accuracy']) log_dir = 'logs/' + dt.now().strftime('%Y-%m-%d_%H:%M:%S'+ self.tz) tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1 ) # -------------------------------------------------------- time_start = time.time() self.history = self.model_name.fit( train_generator, steps_per_epoch= steps_per_epoch, epochs= self.","tags":null,"title":"Convolutional network testing for image classification - neural networks"},{"categories":null,"contents":"1. Description of the values ​​from the columns KOI - Kepler Objects of Interest\n   column name description     kepid Kepler Identification   kepoi_name KOI Name. A number used to identify and track a Kepler Object of Interest (KOI). A KOI is a target identified by the Kepler Project that displays at least one transit-like sequence within Kepler time-series photometry that appears to be of astrophysical origin and initially consistent with a planetary transit hypothesis.   kepler_name Kepler Name. Kepler number name in the form \u0026ldquo;Kepler-N,\u0026rdquo; plus a lower-case letter, identifying the planet. In general, these numbers are easier to remember than the corresponding KOI or KIC/KepID designations and are intended to clearly indicate a class of objects that have been confirmed or validated as planets—a step up from the planet candidate designation.   koi_pdisposition The pipeline flag that designates the most probable physical explanation of the KOI. Typical values are FALSE POSITIVE, NOT DISPOSITIONED, and CANDIDATE. The value of this flag may change over time as the evaluation of KOIs proceeds to deeper levels of analysis using Kepler time-series pixel and light curve data, or follow-up observations. A not dispositioned value corresponds to objects for which the disposition tests have not yet been completed.   koi_score A value between 0 and 1 that indicates the confidence in the KOI disposition. For CANDIDATEs, a higher value indicates more confidence in its disposition, while for FALSE POSITIVEs, a higher value indicates less confidence in that disposition. The value is calculated from a Monte Carlo technique such that the score\u0026rsquo;s value is equivalent to the frction of iterations where the Robovetter yields a disposition of CANDIDATE.   koi_fpflag_nt Not Transit-Like Flag A KOI whose light curve is not consistent with that of a transiting planet. This includes, but is not limited to, instrumental artifacts, non-eclipsing variable stars, and spurious (very low SNR) detections.   koi_fpflag_ss Stellar Eclipse Flag A KOI that is observed to have a significant secondary event, transit shape, or out-of-eclipse variability, which indicates that the transit-like event is most likely caused by an eclipsing binary. However, self-luminous, hot Jupiters with a visible secondary eclipse will also have this flag set, but with a disposition of PC.   koi_fpflag_co Centroid Offset Flag The source of the signal is from a nearby star, as inferred by measuring the centroid location of the image both in and out of transit, or by the strength of the transit signal in the target\u0026rsquo;s outer (halo) pixels as compared to the transit signal from the pixels in the optimal (or core) aperture.   koi_fpflag_ec Ephemeris Match Indicates Contamination Flag The KOI shares the same period and epoch as another object and is judged to be the result of flux contamination in the aperture or electronic crosstalk.   koi_period Orbital Period (days) The interval between consecutive planetary transits.   koi_time0bk Transit Epoch (BJD - 2 454 833,0)) The time corresponding to the center of the first detected transit in Barycentric Julian Day (BJD) minus a constant offset of 2,454,833.0 days. The offset corresponds to 12:00 on Jan 1, 2009 UTC.   koi_impact The sky-projected distance between the center of the stellar disc and the center of the planet disc at conjunction, normalized by the stellar radius.   koi_duration Transit Duration (hours) The duration of the observed transits. Duration is measured from first contact between the planet and star until last contact. Contact times are typically computed from a best-fit model produced by a Mandel-Agol (2002) model fit to a multi-quarter Kepler light curve, assuming a linear orbital ephemeris.   koi_depth Transit Depth (parts per million) The fraction of stellar flux lost at the minimum of the planetary transit. Transit depths are typically computed from a best-fit model produced by a Mandel-Agol (2002) model fit to a multi-quarter Kepler light curve, assuming a linear orbital ephemeris.   koi_prad Planetary Radius (Earth radii) The radius of the planet. Planetary radius is the product of the planet star radius ratio and the stellar radius.   koi_teq Equilibrium Temperature (Kelvin) Approximation for the temperature of the planet.   koi_insol Insolation Flux [Earth flux] Insolation flux is another way to give the equilibrium temperature. It depends on the stellar parameters (specifically the stellar radius and temperature), and on the semi-major axis of the planet. It\u0026rsquo;s given in units relative to those measured for the Earth from the Sun.   koi_model_snr Transit Signal-to-Noise Transit depth normalized by the mean uncertainty in the flux during the transits.   koi_tce_plnt_num TCE Planet Number federated to the KOI.   koi_tce_delivname TCE delivery name corresponding to the TCE data federated to the KOI.   koi_steff Stellar Effective Temperature (Kelvin) The photospheric temperature of the star.   koi_slogg Stellar Surface Gravity (log10(cm s-2) The base-10 logarithm of the acceleration due to gravity at the surface of the star.   koi_srad Stellar Radius (solar radii) The photospheric radius of the star.   ra **RA (deg).**KIC Right Ascension   dec **Dec (deg). **KIC Declination   koi_kepmag Kepler-band (mag)    link to identification columns\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/kepler/project_kepler_col_names/","summary":"1. Description of the values ​​from the columns KOI - Kepler Objects of Interest\n   column name description     kepid Kepler Identification   kepoi_name KOI Name. A number used to identify and track a Kepler Object of Interest (KOI). A KOI is a target identified by the Kepler Project that displays at least one transit-like sequence within Kepler time-series photometry that appears to be of astrophysical origin and initially consistent with a planetary transit hypothesis.","tags":null,"title":"Kepler Exoplanet Search Results - data analysis"},{"categories":null,"contents":"5. Data analysis Columns names: KOI - Kepler Objects of Interest\n   column name description     koi_period Orbital Period (days) The interval between consecutive planetary transits.   koi_time0bk Transit Epoch (BJD - 2 454 833,0)) The time corresponding to the center of the first detected transit in Barycentric Julian Day (BJD) minus a constant offset of 2,454,833.0 days. The offset corresponds to 12:00 on Jan 1, 2009 UTC.   koi_impact The sky-projected distance between the center of the stellar disc and the center of the planet disc at conjunction, normalized by the stellar radius.   koi_duration Transit Duration (hours) The duration of the observed transits. Duration is measured from first contact between the planet and star until last contact. Contact times are typically computed from a best-fit model produced by a Mandel-Agol (2002) model fit to a multi-quarter Kepler light curve, assuming a linear orbital ephemeris.   koi_depth Transit Depth (parts per million) The fraction of stellar flux lost at the minimum of the planetary transit. Transit depths are typically computed from a best-fit model produced by a Mandel-Agol (2002) model fit to a multi-quarter Kepler light curve, assuming a linear orbital ephemeris.   koi_prad Planetary Radius (Earth radii) The radius of the planet. Planetary radius is the product of the planet star radius ratio and the stellar radius.   koi_teq Equilibrium Temperature (Kelvin) Approximation for the temperature of the planet.   koi_insol Insolation Flux [Earth flux] Insolation flux is another way to give the equilibrium temperature. It depends on the stellar parameters (specifically the stellar radius and temperature), and on the semi-major axis of the planet. It\u0026rsquo;s given in units relative to those measured for the Earth from the Sun.   koi_model_snr Transit Signal-to-Noise Transit depth normalized by the mean uncertainty in the flux during the transits.   koi_tce_plnt_num TCE Planet Number federated to the KOI.   koi_tce_delivname TCE delivery name corresponding to the TCE data federated to the KOI.   koi_steff Stellar Effective Temperature (Kelvin) The photospheric temperature of the star.   koi_slogg Stellar Surface Gravity (log10(cm s-2) The base-10 logarithm of the acceleration due to gravity at the surface of the star.   koi_srad Stellar Radius (solar radii) The photospheric radius of the star.   ra **RA (deg).**KIC Right Ascension   dec **Dec (deg). **KIC Declination   koi_kepmag Kepler-band (mag)    kepler['koi_disposition'].value_counts() FALSE POSITIVE 4723 CONFIRMED 2292 CANDIDATE 2185 Name: koi_disposition, dtype: int64 kepler_CONFIRMED = kepler.where(kepler['koi_disposition'] == 'CONFIRMED').dropna() kepler_CANDIDATE = kepler.where(kepler['koi_disposition'] == 'CANDIDATE').dropna() kepler_FALSE_POSITIVE = kepler.where(kepler['koi_disposition'] == 'FALSE POSITIVE').dropna() kepler_CONFIRMED.head()  5.1 Determination of statistical values 5.2. Grouping of data kepler_groupby_disposition = kepler.groupby(by='koi_disposition') kepler_groupby_disposition.groups {'CANDIDATE': Int64Index([ 37, 58, 62, 63, 72, 84, 92, 94, 103, 112, ... 9533, 9539, 9542, 9544, 9546, 9550, 9551, 9557, 9558, 9561], dtype='int64', length=2185), 'CONFIRMED': Int64Index([ 0, 1, 4, 5, 6, 7, 9, 10, 11, 12, ... 9178, 9219, 9221, 9264, 9305, 9309, 9353, 9355, 9479, 9540], dtype='int64', length=2292), 'FALSE POSITIVE': Int64Index([ 2, 3, 8, 14, 15, 16, 17, 20, 24, 28, ... 9549, 9552, 9553, 9554, 9555, 9556, 9559, 9560, 9562, 9563], dtype='int64', length=4723)} [col for col in kepler.columns[5:]] ['koi_period', 'koi_time0bk', 'koi_impact', 'koi_duration', 'koi_depth', 'koi_prad', 'koi_teq', 'koi_insol', 'koi_model_snr', 'koi_steff', 'koi_slogg', 'koi_srad', 'ra', 'dec', 'koi_kepmag'] kepler_groupby_disposition_mean_std = kepler_groupby_disposition.agg( { 'koi_period' : ['mean', 'std'], 'koi_time0bk' : ['mean', 'std'], 'koi_impact' : ['mean', 'std'], 'koi_duration' : ['mean', 'std'], 'koi_depth' : ['mean', 'std'], 'koi_prad' : ['mean', 'std'], 'koi_teq' : ['mean', 'std'], 'koi_insol' : ['mean', 'std'], 'koi_model_snr' : ['mean', 'std'], 'koi_steff' : ['mean', 'std'], 'koi_slogg' : ['mean', 'std'], 'koi_srad' : ['mean', 'std'], 'ra' : ['mean', 'std'], 'dec' : ['mean', 'std'], 'koi_kepmag' : ['mean', 'std'] } ).apply(lambda x: round(x, 3)) kepler_groupby_disposition_mean_std.T      koi_disposition CANDIDATE CONFIRMED FALSE POSITIVE     koi_period mean 131.283 27.053 70.897    std 2783.360 54.028 138.975   koi_time0bk mean 170.316 157.246 167.210    std 75.209 42.481 72.731   koi_impact mean 0.537 0.427 0.976    std 1.990 0.332 4.454   koi_duration mean 4.814 4.307 6.698    std      Name: koi_disposition, dtype: int64        kepler_CONFIRMED = kepler.where(kepler['koi_disposition'] == 'CONFIRMED').dropna() kepler_CANDIDATE = kepler.where(kepler['koi_disposition'] == 'CANDIDATE').dropna() kepler_FALSE_POSITIVE = kepler.where(kepler['koi_disposition'] == 'FALSE POSITIVE').dropna() kepler_CONFIRMED.head()  5.1 Determination of statistical values 5.2. Grouping of data kepler_groupby_disposition = kepler.groupby(by='koi_disposition') kepler_groupby_disposition.groups {'CANDIDATE': Int64Index([ 37, 58, 62, 63, 72, 84, 92, 94, 103, 112, ... 9533, 9539, 9542, 9544, 9546, 9550, 9551, 9557, 9558, 9561], dtype='int64', length=2185), 'CONFIRMED': Int64Index([ 0, 1, 4, 5, 6, 7, 9, 10, 11, 12, ... 9178, 9219, 9221, 9264, 9305, 9309, 9353, 9355, 9479, 9540], dtype='int64', length=2292), 'FALSE POSITIVE': Int64Index([ 2, 3, 8, 14, 15, 16, 17, 20, 24, 28, ... 9549, 9552, 9553, 9554, 9555, 9556, 9559, 9560, 9562, 9563], dtype='int64', length=4723)} [col for col in kepler.columns[5:]] ['koi_period', 'koi_time0bk', 'koi_impact', 'koi_duration', 'koi_depth', 'koi_prad', 'koi_teq', 'koi_insol', 'koi_model_snr', 'koi_steff', 'koi_slogg', 'koi_srad', 'ra', 'dec', 'koi_kepmag'] kepler_groupby_disposition_mean_std = kepler_groupby_disposition.agg( { 'koi_period' : ['mean', 'std'], 'koi_time0bk' : ['mean', 'std'], 'koi_impact' : ['mean', 'std'], 'koi_duration' : ['mean', 'std'], 'koi_depth' : ['mean', 'std'], 'koi_prad' : ['mean', 'std'], 'koi_teq' : ['mean', 'std'], 'koi_insol' : ['mean', 'std'], 'koi_model_snr' : ['mean', 'std'], 'koi_steff' : ['mean', 'std'], 'koi_slogg' : ['mean', 'std'], 'koi_srad' : ['mean', 'std'], 'ra' : ['mean', 'std'], 'dec' : ['mean', 'std'], 'koi_kepmag' : ['mean', 'std'] } ).apply(lambda x: round(x, 3)) kepler_groupby_disposition_mean_std.T      koi_disposition CANDIDATE CONFIRMED FALSE POSITIVE     koi_period mean 131.283 27.053 70.897    std 2783.360 54.028 138.975   koi_time0bk mean 170.316 157.246 167.210    std 75.209 42.481 72.731   koi_impact mean 0.537 0.427 0.976    std 1.990 0.332 4.454   koi_duration mean 4.814 4.307 6.698    std 4.421 2.720 8.209   koi_depth mean 1864.239 1141.446 44924.240    std 12502.100 2686.208 110382.027   koi_prad mean 15.931 2.872 191.672    std 316.937 3.361 4288.541   koi_teq mean 882.420 839.126 1298.734    std 665.620 386.741 1028.230   koi_insol mean 5359.428 350.666 12383.162    std 155923.364 1223.676 195698.944   koi_model_snr mean 45.596 87.923 442.498    std 231.866 284.086 1049.244   koi_steff mean 5639.767 5477.974 5848.887    std 693.971 677.133 862.706   koi_slogg mean 4.331 4.411 4.251    std 0.391 0.235 0.508   koi_srad mean 1.566 1.067 2.125    std 5.875 0.643 7.523   ra mean 291.790 290.943 292.733    std 4.835 4.757 4.612   dec mean 43.928 44.366 43.484    std 3.606 3.569 3.581   koi_kepmag mean 14.348 14.339 14.212    std 1.303 1.224 1.477    5.3. Value distribution analysis cols = kepler.columns[5:] fig = plt.figure(figsize=(25,10)) fig.subplots_adjust(hspace=0.5, wspace=0.3) col_idx = 0 for i in range(1, 16): fig.add_subplot(4, 4, i) sns.distplot(kepler[cols[col_idx]]) col_idx += 1 plt.show()  5.4. Correlation analysis corr = kepler.corr() fig = plt.figure(figsize=(18,10)) g = sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot=True, fmt='.2f', linewidth=.3, cmap=sns.diverging_palette(20, 220, n=256)) g.set_xticklabels( g.get_xticklabels(), rotation=30, horizontalalignment='right', fontweight='light' ) plt.show()  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/kepler/project_kepler_data_analysis/","summary":"5. Data analysis Columns names: KOI - Kepler Objects of Interest\n   column name description     koi_period Orbital Period (days) The interval between consecutive planetary transits.   koi_time0bk Transit Epoch (BJD - 2 454 833,0)) The time corresponding to the center of the first detected transit in Barycentric Julian Day (BJD) minus a constant offset of 2,454,833.0 days. The offset corresponds to 12:00 on Jan 1, 2009 UTC.","tags":null,"title":"Kepler Exoplanet Search Results - data analysis"},{"categories":null,"contents":"3. Data preparation: 3.1. Data presentation kepler_raw = pd.read_csv('/content/drive/My Drive/ColabNotebooks/KeplerExoplanetSearchResults/datasets/cumulative.csv', low_memory=False) # Kopia danych kepler = kepler_raw.copy() kepler.head()  kepler.info(memory_usage='Deep') \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 9564 entries, 0 to 9563 Data columns (total 50 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 rowid 9564 non-null int64 1 kepid 9564 non-null int64 2 kepoi_name 9564 non-null object 3 kepler_name 2294 non-null object 4 koi_disposition 9564 non-null object 5 koi_pdisposition 9564 non-null object 6 koi_score 8054 non-null float64 7 koi_fpflag_nt 9564 non-null int64 8 koi_fpflag_ss 9564 non-null int64 9 koi_fpflag_co 9564 non-null int64 10 koi_fpflag_ec 9564 non-null int64 11 koi_period 9564 non-null float64 12 koi_period_err1 9110 non-null float64 13 koi_period_err2 9110 non-null float64 14 koi_time0bk 9564 non-null float64 15 koi_time0bk_err1 9110 non-null float64 16 koi_time0bk_err2 9110 non-null float64 17 koi_impact 9201 non-null float64 18 koi_impact_err1 9110 non-null float64 19 koi_impact_err2 9110 non-null float64 20 koi_duration 9564 non-null float64 21 koi_duration_err1 9110 non-null float64 22 koi_duration_err2 9110 non-null float64 23 koi_depth 9201 non-null float64 24 koi_depth_err1 9110 non-null float64 25 koi_depth_err2 9110 non-null float64 26 koi_prad 9201 non-null float64 27 koi_prad_err1 9201 non-null float64 28 koi_prad_err2 9201 non-null float64 29 koi_teq 9201 non-null float64 30 koi_teq_err1 0 non-null float64 31 koi_teq_err2 0 non-null float64 32 koi_insol 9243 non-null float64 33 koi_insol_err1 9243 non-null float64 34 koi_insol_err2 9243 non-null float64 35 koi_model_snr 9201 non-null float64 36 koi_tce_plnt_num 9218 non-null float64 37 koi_tce_delivname 9218 non-null object 38 koi_steff 9201 non-null float64 39 koi_steff_err1 9096 non-null float64 40 koi_steff_err2 9081 non-null float64 41 koi_slogg 9201 non-null float64 42 koi_slogg_err1 9096 non-null float64 43 koi_slogg_err2 9096 non-null float64 44 koi_srad 9201 non-null float64 45 koi_srad_err1 9096 non-null float64 46 koi_srad_err2 9096 non-null float64 47 ra 9564 non-null float64 48 dec 9564 non-null float64 49 koi_kepmag 9563 non-null float64 dtypes: float64(39), int64(6), object(5) memory usage: 3.6+ MB  3.2. Ridding of unnecessary columns for i, column in enumerate(kepler.columns): if column.endswith('_err1') or column.endswith('_err2'): kepler.drop([column], 1, inplace=True) for i, name in enumerate(['kepid', 'rowid', 'kepoi_name', 'kepler_name', 'koi_pdisposition','koi_tce_plnt_num', 'koi_tce_delivname', 'koi_score']): kepler.drop([name], 1, inplace=True) kepler.info(memory_usage='Deep') \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 9564 entries, 0 to 9563 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 koi_disposition 9564 non-null object 1 koi_fpflag_nt 9564 non-null int64 2 koi_fpflag_ss 9564 non-null int64 3 koi_fpflag_co 9564 non-null int64 4 koi_fpflag_ec 9564 non-null int64 5 koi_period 9564 non-null float64 6 koi_time0bk 9564 non-null float64 7 koi_impact 9201 non-null float64 8 koi_duration 9564 non-null float64 9 koi_depth 9201 non-null float64 10 koi_prad 9201 non-null float64 11 koi_teq 9201 non-null float64 12 koi_insol 9243 non-null float64 13 koi_model_snr 9201 non-null float64 14 koi_steff 9201 non-null float64 15 koi_slogg 9201 non-null float64 16 koi_srad 9201 non-null float64 17 ra 9564 non-null float64 18 dec 9564 non-null float64 19 koi_kepmag 9563 non-null float64 dtypes: float64(15), int64(4), object(1) memory usage: 1.5+ MB  3.3. Changing the type from object to type category kepler['koi_disposition'] = kepler['koi_disposition'].astype('category') kepler.info(memory_usage='Deep') \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 9564 entries, 0 to 9563 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 koi_disposition 9564 non-null category 1 koi_fpflag_nt 9564 non-null int64 2 koi_fpflag_ss 9564 non-null int64 3 koi_fpflag_co 9564 non-null int64 4 koi_fpflag_ec 9564 non-null int64 5 koi_period 9564 non-null float64 6 koi_time0bk 9564 non-null float64 7 koi_impact 9201 non-null float64 8 koi_duration 9564 non-null float64 9 koi_depth 9201 non-null float64 10 koi_prad 9201 non-null float64 11 koi_teq 9201 non-null float64 12 koi_insol 9243 non-null float64 13 koi_model_snr 9201 non-null float64 14 koi_steff 9201 non-null float64 15 koi_slogg 9201 non-null float64 16 koi_srad 9201 non-null float64 17 ra 9564 non-null float64 18 dec 9564 non-null float64 19 koi_kepmag 9563 non-null float64 dtypes: category(1), float64(15), int64(4) memory usage: 1.4 MB  3.4. Missing values kepler.isnull().sum()  Result:\nkoi_disposition 0 koi_fpflag_nt 0 koi_fpflag_ss 0 koi_fpflag_co 0 koi_fpflag_ec 0 koi_period 0 koi_time0bk 0 koi_impact 363 koi_duration 0 koi_depth 363 koi_prad 363 koi_teq 363 koi_insol 321 koi_model_snr 363 koi_steff 363 koi_slogg 363 koi_srad 363 ra 0 dec 0 koi_kepmag 1 dtype: int64  Showing missing values. Where are missing values in the data frame?\nkepler[pd.isnull(kepler['koi_impact'])].head()  Dropping missing values:\nkepler = kepler.dropna() kepler.info(memory_usage='Deep') \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 9200 entries, 0 to 9563 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 koi_disposition 9200 non-null category 1 koi_fpflag_nt 9200 non-null int64 2 koi_fpflag_ss 9200 non-null int64 3 koi_fpflag_co 9200 non-null int64 4 koi_fpflag_ec 9200 non-null int64 5 koi_period 9200 non-null float64 6 koi_time0bk 9200 non-null float64 7 koi_impact 9200 non-null float64 8 koi_duration 9200 non-null float64 9 koi_depth 9200 non-null float64 10 koi_prad 9200 non-null float64 11 koi_teq 9200 non-null float64 12 koi_insol 9200 non-null float64 13 koi_model_snr 9200 non-null float64 14 koi_steff 9200 non-null float64 15 koi_slogg 9200 non-null float64 16 koi_srad 9200 non-null float64 17 ra 9200 non-null float64 18 dec 9200 non-null float64 19 koi_kepmag 9200 non-null float64 dtypes: category(1), float64(15), int64(4) memory usage: 1.4 MB  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/kepler/project_kepler_data_prepare/","summary":"3. Data preparation: 3.1. Data presentation kepler_raw = pd.read_csv('/content/drive/My Drive/ColabNotebooks/KeplerExoplanetSearchResults/datasets/cumulative.csv', low_memory=False) # Kopia danych kepler = kepler_raw.copy() kepler.head()  kepler.info(memory_usage='Deep') \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 9564 entries, 0 to 9563 Data columns (total 50 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 rowid 9564 non-null int64 1 kepid 9564 non-null int64 2 kepoi_name 9564 non-null object 3 kepler_name 2294 non-null object 4 koi_disposition 9564 non-null object 5 koi_pdisposition 9564 non-null object 6 koi_score 8054 non-null float64 7 koi_fpflag_nt 9564 non-null int64 8 koi_fpflag_ss 9564 non-null int64 9 koi_fpflag_co 9564 non-null int64 10 koi_fpflag_ec 9564 non-null int64 11 koi_period 9564 non-null float64 12 koi_period_err1 9110 non-null float64 13 koi_period_err2 9110 non-null float64 14 koi_time0bk 9564 non-null float64 15 koi_time0bk_err1 9110 non-null float64 16 koi_time0bk_err2 9110 non-null float64 17 koi_impact 9201 non-null float64 18 koi_impact_err1 9110 non-null float64 19 koi_impact_err2 9110 non-null float64 20 koi_duration 9564 non-null float64 21 koi_duration_err1 9110 non-null float64 22 koi_duration_err2 9110 non-null float64 23 koi_depth 9201 non-null float64 24 koi_depth_err1 9110 non-null float64 25 koi_depth_err2 9110 non-null float64 26 koi_prad 9201 non-null float64 27 koi_prad_err1 9201 non-null float64 28 koi_prad_err2 9201 non-null float64 29 koi_teq 9201 non-null float64 30 koi_teq_err1 0 non-null float64 31 koi_teq_err2 0 non-null float64 32 koi_insol 9243 non-null float64 33 koi_insol_err1 9243 non-null float64 34 koi_insol_err2 9243 non-null float64 35 koi_model_snr 9201 non-null float64 36 koi_tce_plnt_num 9218 non-null float64 37 koi_tce_delivname 9218 non-null object 38 koi_steff 9201 non-null float64 39 koi_steff_err1 9096 non-null float64 40 koi_steff_err2 9081 non-null float64 41 koi_slogg 9201 non-null float64 42 koi_slogg_err1 9096 non-null float64 43 koi_slogg_err2 9096 non-null float64 44 koi_srad 9201 non-null float64 45 koi_srad_err1 9096 non-null float64 46 koi_srad_err2 9096 non-null float64 47 ra 9564 non-null float64 48 dec 9564 non-null float64 49 koi_kepmag 9563 non-null float64 dtypes: float64(39), int64(6), object(5) memory usage: 3.","tags":null,"title":"Kepler Exoplanet Search Results - data presentation"},{"categories":null,"contents":"2. Import modules import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.figure_factory as ff import plotly.express as px from plotly.subplots import make_subplots import seaborn as sns import plotly.express as px from plotly.subplots import make_subplots import seaborn as sns sns.set(style=\u0026quot;ticks\u0026quot;, color_codes=True) from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.svm import SVC from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import LabelEncoder from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report from mlxtend.plotting import plot_confusion_matrix  3. Data preparation: 3.1. Data presentation kepler_raw = pd.read_csv('/content/drive/My Drive/ColabNotebooks/KeplerExoplanetSearchResults/datasets/cumulative.csv', low_memory=False) # Kopia danych kepler = kepler_raw.copy() kepler.head()  kepler.info(memory_usage='Deep') \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 9564 entries, 0 to 9563 Data columns (total 50 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 rowid 9564 non-null int64 1 kepid 9564 non-null int64 2 kepoi_name 9564 non-null object 3 kepler_name 2294 non-null object 4 koi_disposition 9564 non-null object 5 koi_pdisposition 9564 non-null object 6 koi_score 8054 non-null float64 7 koi_fpflag_nt 9564 non-null int64 8 koi_fpflag_ss 9564 non-null int64 9 koi_fpflag_co 9564 non-null int64 10 koi_fpflag_ec 9564 non-null int64 11 koi_period 9564 non-null float64 12 koi_period_err1 9110 non-null float64 13 koi_period_err2 9110 non-null float64 14 koi_time0bk 9564 non-null float64 15 koi_time0bk_err1 9110 non-null float64 16 koi_time0bk_err2 9110 non-null float64 17 koi_impact 9201 non-null float64 18 koi_impact_err1 9110 non-null float64 19 koi_impact_err2 9110 non-null float64 20 koi_duration 9564 non-null float64 21 koi_duration_err1 9110 non-null float64 22 koi_duration_err2 9110 non-null float64 23 koi_depth 9201 non-null float64 24 koi_depth_err1 9110 non-null float64 25 koi_depth_err2 9110 non-null float64 26 koi_prad 9201 non-null float64 27 koi_prad_err1 9201 non-null float64 28 koi_prad_err2 9201 non-null float64 29 koi_teq 9201 non-null float64 30 koi_teq_err1 0 non-null float64 31 koi_teq_err2 0 non-null float64 32 koi_insol 9243 non-null float64 33 koi_insol_err1 9243 non-null float64 34 koi_insol_err2 9243 non-null float64 35 koi_model_snr 9201 non-null float64 36 koi_tce_plnt_num 9218 non-null float64 37 koi_tce_delivname 9218 non-null object 38 koi_steff 9201 non-null float64 39 koi_steff_err1 9096 non-null float64 40 koi_steff_err2 9081 non-null float64 41 koi_slogg 9201 non-null float64 42 koi_slogg_err1 9096 non-null float64 43 koi_slogg_err2 9096 non-null float64 44 koi_srad 9201 non-null float64 45 koi_srad_err1 9096 non-null float64 46 koi_srad_err2 9096 non-null float64 47 ra 9564 non-null float64 48 dec 9564 non-null float64 49 koi_kepmag 9563 non-null float64 dtypes: float64(39), int64(6), object(5) memory usage: 3.6+ MB  3.2. Ridding of unnecessary columns for i, column in enumerate(kepler.columns): if column.endswith('_err1') or column.endswith('_err2'): kepler.drop([column], 1, inplace=True) for i, name in enumerate(['kepid', 'rowid', 'kepoi_name', 'kepler_name', 'koi_pdisposition','koi_tce_plnt_num', 'koi_tce_delivname', 'koi_score']): kepler.drop([name], 1, inplace=True) kepler.info(memory_usage='Deep') \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 9564 entries, 0 to 9563 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 koi_disposition 9564 non-null object 1 koi_fpflag_nt 9564 non-null int64 2 koi_fpflag_ss 9564 non-null int64 3 koi_fpflag_co 9564 non-null int64 4 koi_fpflag_ec 9564 non-null int64 5 koi_period 9564 non-null float64 6 koi_time0bk 9564 non-null float64 7 koi_impact 9201 non-null float64 8 koi_duration 9564 non-null float64 9 koi_depth 9201 non-null float64 10 koi_prad 9201 non-null float64 11 koi_teq 9201 non-null float64 12 koi_insol 9243 non-null float64 13 koi_model_snr 9201 non-null float64 14 koi_steff 9201 non-null float64 15 koi_slogg 9201 non-null float64 16 koi_srad 9201 non-null float64 17 ra 9564 non-null float64 18 dec 9564 non-null float64 19 koi_kepmag 9563 non-null float64 dtypes: float64(15), int64(4), object(1) memory usage: 1.5+ MB  3.3. Changing the type from object to type category kepler['koi_disposition'] = kepler['koi_disposition'].astype('category') kepler.info(memory_usage='Deep') \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 9564 entries, 0 to 9563 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 koi_disposition 9564 non-null category 1 koi_fpflag_nt 9564 non-null int64 2 koi_fpflag_ss 9564 non-null int64 3 koi_fpflag_co 9564 non-null int64 4 koi_fpflag_ec 9564 non-null int64 5 koi_period 9564 non-null float64 6 koi_time0bk 9564 non-null float64 7 koi_impact 9201 non-null float64 8 koi_duration 9564 non-null float64 9 koi_depth 9201 non-null float64 10 koi_prad 9201 non-null float64 11 koi_teq 9201 non-null float64 12 koi_insol 9243 non-null float64 13 koi_model_snr 9201 non-null float64 14 koi_steff 9201 non-null float64 15 koi_slogg 9201 non-null float64 16 koi_srad 9201 non-null float64 17 ra 9564 non-null float64 18 dec 9564 non-null float64 19 koi_kepmag 9563 non-null float64 dtypes: category(1), float64(15), int64(4) memory usage: 1.4 MB  3.4. Missing values kepler.isnull().sum()  Result:\nkoi_disposition 0 koi_fpflag_nt 0 koi_fpflag_ss 0 koi_fpflag_co 0 koi_fpflag_ec 0 koi_period 0 koi_time0bk 0 koi_impact 363 koi_duration 0 koi_depth 363 koi_prad 363 koi_teq 363 koi_insol 321 koi_model_snr 363 koi_steff 363 koi_slogg 363 koi_srad 363 ra 0 dec 0 koi_kepmag 1 dtype: int64  Showing missing values. Where are missing values in the data frame?\nkepler[pd.isnull(kepler['koi_impact'])].head()  Dropping missing values:\nkepler = kepler.dropna() kepler.info(memory_usage='Deep') \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 9200 entries, 0 to 9563 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 koi_disposition 9200 non-null category 1 koi_fpflag_nt 9200 non-null int64 2 koi_fpflag_ss 9200 non-null int64 3 koi_fpflag_co 9200 non-null int64 4 koi_fpflag_ec 9200 non-null int64 5 koi_period 9200 non-null float64 6 koi_time0bk 9200 non-null float64 7 koi_impact 9200 non-null float64 8 koi_duration 9200 non-null float64 9 koi_depth 9200 non-null float64 10 koi_prad 9200 non-null float64 11 koi_teq 9200 non-null float64 12 koi_insol 9200 non-null float64 13 koi_model_snr 9200 non-null float64 14 koi_steff 9200 non-null float64 15 koi_slogg 9200 non-null float64 16 koi_srad 9200 non-null float64 17 ra 9200 non-null float64 18 dec 9200 non-null float64 19 koi_kepmag 9200 non-null float64 dtypes: category(1), float64(15), int64(4) memory usage: 1.4 MB  5. Data analysis Columns names: Oznaczenia: KOI - Kepler Objects of Interest\n   Oznaczenie Opis     koi_period Orbital Period (days) The interval between consecutive planetary transits.   koi_time0bk Transit Epoch (BJD - 2 454 833,0)) The time corresponding to the center of the first detected transit in Barycentric Julian Day (BJD) minus a constant offset of 2,454,833.0 days. The offset corresponds to 12:00 on Jan 1, 2009 UTC.   koi_impact The sky-projected distance between the center of the stellar disc and the center of the planet disc at conjunction, normalized by the stellar radius.   koi_duration Transit Duration (hours) The duration of the observed transits. Duration is measured from first contact between the planet and star until last contact. Contact times are typically computed from a best-fit model produced by a Mandel-Agol (2002) model fit to a multi-quarter Kepler light curve, assuming a linear orbital ephemeris.   koi_depth Transit Depth (parts per million) The fraction of stellar flux lost at the minimum of the planetary transit. Transit depths are typically computed from a best-fit model produced by a Mandel-Agol (2002) model fit to a multi-quarter Kepler light curve, assuming a linear orbital ephemeris.   koi_prad Planetary Radius (Earth radii) The radius of the planet. Planetary radius is the product of the planet star radius ratio and the stellar radius.   koi_teq Equilibrium Temperature (Kelvin) Approximation for the temperature of the planet.   koi_insol **Insolation Flux [Earth flux]**Insolation flux is another way to give the equilibrium temperature. It depends on the stellar parameters (specifically the stellar radius and temperature), and on the semi-major axis of the planet. It’s given in units relative to those measured for the Earth from the Sun.   koi_model_snr Transit Signal-to-Noise Transit depth normalized by the mean uncertainty in the flux during the transits.   koi_tce_delivname TCE delivery name corresponding to the TCE data federated to the KOI.   koi_steff **Stellar Effective Temperature (Kelvin)**The photospheric temperature of the star.   koi_slogg Stellar Surface Gravity (log10(cm s-2) The base-10 logarithm of the acceleration due to gravity at the surface of the star.   koi_srad Stellar Radius (solar radii) The photospheric radius of the star   ra RA (deg). Rektascensja, KIC Right Ascension / α (łac. recta ascensio – \u0026ldquo;wznoszenie proste\u0026rdquo;) – jedna ze współrzędnych astronomicznych, określających położenie ciała niebieskiego na sferze niebieskiej w układzie współrzędnych astronomicznych zwanym układem równikowym równonocnym.   dec Dec (deg). Deklinacja, KIC Declination / (łac. declinatio – „odchylenie”, oznaczana symbolem δ) – jedna ze współrzędnych określających położenie ciała w obydwu układach równikowych: równonocnym i godzinnym. Definiujemy ją jako kąt pomiędzy kierunkiem poprowadzonym od obserwatora do obiektu a płaszczyzną równika niebieskiego.   koi_kepmag Kepler-band (mag)    kepler['koi_disposition'].value_counts() FALSE POSITIVE 4723 CONFIRMED 2292 CANDIDATE 2185 Name: koi_disposition, dtype: int64 kepler_CONFIRMED = kepler.where(kepler['koi_disposition'] == 'CONFIRMED').dropna() kepler_CANDIDATE = kepler.where(kepler['koi_disposition'] == 'CANDIDATE').dropna() kepler_FALSE_POSITIVE = kepler.where(kepler['koi_disposition'] == 'FALSE POSITIVE').dropna() kepler_CONFIRMED.head()  5.1 Determination of statistical values 5.2. Grouping of data kepler_groupby_disposition = kepler.groupby(by='koi_disposition') kepler_groupby_disposition.groups {'CANDIDATE': Int64Index([ 37, 58, 62, 63, 72, 84, 92, 94, 103, 112, ... 9533, 9539, 9542, 9544, 9546, 9550, 9551, 9557, 9558, 9561], dtype='int64', length=2185), 'CONFIRMED': Int64Index([ 0, 1, 4, 5, 6, 7, 9, 10, 11, 12, ... 9178, 9219, 9221, 9264, 9305, 9309, 9353, 9355, 9479, 9540], dtype='int64', length=2292), 'FALSE POSITIVE': Int64Index([ 2, 3, 8, 14, 15, 16, 17, 20, 24, 28, ... 9549, 9552, 9553, 9554, 9555, 9556, 9559, 9560, 9562, 9563], dtype='int64', length=4723)} [col for col in kepler.columns[5:]] ['koi_period', 'koi_time0bk', 'koi_impact', 'koi_duration', 'koi_depth', 'koi_prad', 'koi_teq', 'koi_insol', 'koi_model_snr', 'koi_steff', 'koi_slogg', 'koi_srad', 'ra', 'dec', 'koi_kepmag'] kepler_groupby_disposition_mean_std = kepler_groupby_disposition.agg( { 'koi_period' : ['mean', 'std'], 'koi_time0bk' : ['mean', 'std'], 'koi_impact' : ['mean', 'std'], 'koi_duration' : ['mean', 'std'], 'koi_depth' : ['mean', 'std'], 'koi_prad' : ['mean', 'std'], 'koi_teq' : ['mean', 'std'], 'koi_insol' : ['mean', 'std'], 'koi_model_snr' : ['mean', 'std'], 'koi_steff' : ['mean', 'std'], 'koi_slogg' : ['mean', 'std'], 'koi_srad' : ['mean', 'std'], 'ra' : ['mean', 'std'], 'dec' : ['mean', 'std'], 'koi_kepmag' : ['mean', 'std'] } ).apply(lambda x: round(x, 3)) kepler_groupby_disposition_mean_std.T      koi_disposition CANDIDATE CONFIRMED FALSE POSITIVE     koi_period mean 131.283 27.053 70.897    std 2783.360 54.028 138.975   koi_time0bk mean 170.316 157.246 167.210    std 75.209 42.481 72.731   koi_impact mean 0.537 0.427 0.976    std 1.990 0.332 4.454   koi_duration mean 4.814 4.307 6.698    std 4.421 2.720 8.209   koi_depth mean 1864.239 1141.446 44924.240    std 12502.100 2686.208 110382.027   koi_prad mean 15.931 2.872 191.672    std 316.937 3.361 4288.541   koi_teq mean 882.420 839.126 1298.734    std 665.620 386.741 1028.230   koi_insol mean 5359.428 350.666 12383.162    std 155923.364 1223.676 195698.944   koi_model_snr mean 45.596 87.923 442.498    std 231.866 284.086 1049.244   koi_steff mean 5639.767 5477.974 5848.887    std 693.971 677.133 862.706   koi_slogg mean 4.331 4.411 4.251    std 0.391 0.235 0.508   koi_srad mean 1.566 1.067 2.125    std 5.875 0.643 7.523   ra mean 291.790 290.943 292.733    std 4.835 4.757 4.612   dec mean 43.928 44.366 43.484    std 3.606 3.569 3.581   koi_kepmag mean 14.348 14.339 14.212    std 1.303 1.224 1.477    5.3. Value distribution analysis cols = kepler.columns[5:] fig = plt.figure(figsize=(25,10)) fig.subplots_adjust(hspace=0.5, wspace=0.3) col_idx = 0 for i in range(1, 16): fig.add_subplot(4, 4, i) sns.distplot(kepler[cols[col_idx]]) col_idx += 1 plt.show()  5.4. Correlation analysis 6. Machine learning: 6.1. Preparing data for machine learning: 6.1.1. The number of occurrences for each label 6.1.2. Coding the label values 6.1.3. Division of data into training and test sets 6.1.4. Scaling features 6.2. Model testing: 6.2.1. Logistic regression model 6.2.2. K-nearest neighbors model 6.2.3. Decision tree model 6.2.4. Random forest model 6.2.5. Support vector machine model 6.2.6. Naive Bayesian classifier 6.3. Comparison of the obtained results 6.4. Performing cross-validation 6.5. Preview on error matrices for individual models 6.6. Testing the neural network 6.7. Application of dimensionality reduction - PCA algorithm 6.8. KMeans algorithm ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/kepler/project_kepler_modules/","summary":"2. Import modules import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.figure_factory as ff import plotly.express as px from plotly.subplots import make_subplots import seaborn as sns import plotly.express as px from plotly.subplots import make_subplots import seaborn as sns sns.set(style=\u0026quot;ticks\u0026quot;, color_codes=True) from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.svm import SVC from sklearn.naive_bayes import GaussianNB from sklearn.","tags":null,"title":"Kepler Exoplanet Search Results - imported modules"},{"categories":null,"contents":"6. Machine learning: 6.1. Preparing data for machine learning: kepler.head()   | | koi_disposition | koi_fpflag_nt | koi_fpflag_ss | koi_fpflag_co | koi_fpflag_ec | koi_period | koi_time0bk | koi_impact | koi_duration | koi_depth | koi_prad | koi_teq | koi_insol | koi_model_snr | koi_steff | koi_slogg | koi_srad | ra | dec | koi_kepmag | |---|-----------------|---------------|---------------|---------------|---------------|------------|-------------|------------|--------------|-----------|----------|---------|-----------|---------------|-----------|-----------|----------|-----------|-----------|------------| | 0 | CONFIRMED | 0 | 0 | 0 | 0 | 9.488036 | 170.538750 | 0.146 | 2.95750 | 615.8 | 2.26 | 793.0 | 93.59 | 35.8 | 5455.0 | 4.467 | 0.927 | 291.93423 | 48.141651 | 15.347 | | 1 | CONFIRMED | 0 | 0 | 0 | 0 | 54.418383 | 162.513840 | 0.586 | 4.50700 | 874.8 | 2.83 | 443.0 | 9.11 | 25.8 | 5455.0 | 4.467 | 0.927 | 291.93423 | 48.141651 | 15.347 | | 2 | FALSE POSITIVE | 0 | 1 | 0 | 0 | 19.899140 | 175.850252 | 0.969 | 1.78220 | 10829.0 | 14.60 | 638.0 | 39.30 | 76.3 | 5853.0 | 4.544 | 0.868 | 297.00482 | 48.134129 | 15.436 | | 3 | FALSE POSITIVE | 0 | 1 | 0 | 0 | 1.736952 | 170.307565 | 1.276 | 2.40641 | 8079.2 | 33.46 | 1395.0 | 891.96 | 505.6 | 5805.0 | 4.564 | 0.791 | 285.53461 | 48.285210 | 15.597 | | 4 | CONFIRMED | 0 | 0 | 0 | 0 | 2.525592 | 171.595550 | 0.701 | 1.65450 | 603.3 | 2.75 | 1406.0 | 926.16 | 40.9 | 6031.0 | 4.438 | 1.046 | 288.75488 | 48.226200 | 15.509 |  6.1.1. The number of occurrences for each label sns.countplot(x='koi_disposition', data=kepler) plt.show()  6.1.2. Coding the label values encoder = LabelEncoder() y = kepler['koi_disposition'] kepler['koi_disposition'] = encoder.fit_transform(y) kepler.head()   | | koi_disposition | koi_fpflag_nt | koi_fpflag_ss | koi_fpflag_co | koi_fpflag_ec | koi_period | koi_time0bk | koi_impact | koi_duration | koi_depth | koi_prad | koi_teq | koi_insol | koi_model_snr | koi_steff | koi_slogg | koi_srad | ra | dec | koi_kepmag | |---|-----------------|---------------|---------------|---------------|---------------|------------|-------------|------------|--------------|-----------|----------|---------|-----------|---------------|-----------|-----------|----------|-----------|-----------|------------| | 0 | 1 | 0 | 0 | 0 | 0 | 9.488036 | 170.538750 | 0.146 | 2.95750 | 615.8 | 2.26 | 793.0 | 93.59 | 35.8 | 5455.0 | 4.467 | 0.927 | 291.93423 | 48.141651 | 15.347 | | 1 | 1 | 0 | 0 | 0 | 0 | 54.418383 | 162.513840 | 0.586 | 4.50700 | 874.8 | 2.83 | 443.0 | 9.11 | 25.8 | 5455.0 | 4.467 | 0.927 | 291.93423 | 48.141651 | 15.347 | | 2 | 2 | 0 | 1 | 0 | 0 | 19.899140 | 175.850252 | 0.969 | 1.78220 | 10829.0 | 14.60 | 638.0 | 39.30 | 76.3 | 5853.0 | 4.544 | 0.868 | 297.00482 | 48.134129 | 15.436 | | 3 | 2 | 0 | 1 | 0 | 0 | 1.736952 | 170.307565 | 1.276 | 2.40641 | 8079.2 | 33.46 | 1395.0 | 891.96 | 505.6 | 5805.0 | 4.564 | 0.791 | 285.53461 | 48.285210 | 15.597 | | 4 | 1 | 0 | 0 | 0 | 0 | 2.525592 | 171.595550 | 0.701 | 1.65450 | 603.3 | 2.75 | 1406.0 | 926.16 | 40.9 | 6031.0 | 4.438 | 1.046 | 288.75488 | 48.226200 | 15.509 |  6.1.3. Data splitting into training and test sets X = kepler.drop(\u0026quot;koi_disposition\u0026quot;, axis=1) y = kepler['koi_disposition'] print(f'X: {X.shape} ') print(f'y: {y.shape} ') # result: X: (9200, 19) y: (9200,)   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,) print(f'X_train shape: {X_train.shape}') print(f'X_test shape: {X_test.shape}') print(f'y_train shape: {y_train.shape}') print(f'y_test shape: {y_test.shape}') # result: X_train shape: (7360, 19) X_test shape: (1840, 19) y_train shape: (7360,) y_test shape: (1840,)  6.1.4. Scaling features scaler = StandardScaler() scaler.fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test)  6.2. Pipeline testing: # lista kolumn numerycznych cols_numerical = X_train.select_dtypes(include=['int64', 'float64']).columns # lista kolmn kategorycznych cols_categorical = [ 'koi_fpflag_nt', 'koi_fpflag_ss', 'koi_fpflag_co', 'koi_fpflag_ec']   # pipeline testing # ################ # transformer dla kolumn numerycznych transformer_numerical = Pipeline( steps = [ ('num_transformer', None ) ] ) # preprocesor danych preprocessor = ColumnTransformer( transformers = [ ('numerical', transformer_numerical, cols_numerical ), ] ) # klasyfikatory classifiers = [ LogisticRegression(max_iter=500), KNeighborsClassifier(3), # 3 bo mamy dwie klasy ?? DecisionTreeClassifier(max_depth=4, random_state=42 ), RandomForestClassifier(n_estimators=15, random_state=42 ), SVC(C=1.0, kernel='rbf'), GaussianNB() ] # transformatory dla kolumn liczbowych scalers = [StandardScaler(), MinMaxScaler(), Normalizer()] # # transformatory dla kolumn kategorycznych # cat_transformers = [OrdinalEncoder(), OneHotEncoder()]   models_df = pd.DataFrame() # utworzenie pipeline: pipe = Pipeline( steps = [ ('preprocessor', preprocessor), ('classifier', None) ] )models_df = pd.DataFrame() # utworzenie pipeline: pipe = Pipeline( steps = [ ('preprocessor', preprocessor), ('classifier', None) ] )   # dla każdego typu modelu zmieniamy kolejne transformatory kolumn for model in classifiers: for num_tr in scalers: # odpowiednio zmieniamy jego paramety - dobieramy transformatory pipe_params = { 'preprocessor__numerical__num_transformer': num_tr, # 'preprocessor__categorical__cat_trans': cat_tr, 'classifier': model } pipe.set_params(**pipe_params) start_time = time.time() pipe.fit(X_train, y_train) # trenowanie modelu end_time = time.time() score = pipe.score(X_test, y_test) # zapisanie wyniku param_dict = { 'model': model.__class__.__name__, 'num_trans': num_tr.__class__.__name__, # 'cat_trans': cat_tr.__class__.__name__, 'Accuracy': score, 'time_elapsed': end_time - start_time } # zapisanie wyników do data frame models_df = models_df.append(pd.DataFrame(param_dict, index=[0])) models_df.reset_index(drop=True, inplace=True)  6.3. Comparison of the obtained results 6.4. Performing cross-validation 6.5. Preview on error matrices for individual models 6.6. Testing the neural network 6.7. Application of dimensionality reduction - PCA algorithm 6.8. KMeans algorithm ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/kepler/project_kepler_machine_learning/","summary":"6. Machine learning: 6.1. Preparing data for machine learning: kepler.head()   | | koi_disposition | koi_fpflag_nt | koi_fpflag_ss | koi_fpflag_co | koi_fpflag_ec | koi_period | koi_time0bk | koi_impact | koi_duration | koi_depth | koi_prad | koi_teq | koi_insol | koi_model_snr | koi_steff | koi_slogg | koi_srad | ra | dec | koi_kepmag | |---|-----------------|---------------|---------------|---------------|---------------|------------|-------------|------------|--------------|-----------|----------|---------|-----------|---------------|-----------|-----------|----------|-----------|-----------|------------| | 0 | CONFIRMED | 0 | 0 | 0 | 0 | 9.488036 | 170.","tags":null,"title":"Kepler Exoplanet Search Results - machine learning"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"2. Data reviewing: I checked if data are completed and if there is no NaN values.\n2.1. Correlation maps: On the correlation maps below, we see which factor has the highest influence on happiness score and which of them has lowest. Regardless the years, the highest influence on happiness score of the country has economy level of this country. On the opposite side, the generosity factor has little effect on happiness score.\n2.2. Countries with the highest happiness score In 2015, Switzerland was the country with the highest happiness score their citizens, but in 2016 this country took the second position in the ranking. Through the next years, Switzerland fell on the fourth, the fifth and the sixth place. In 2016, Denmark took the first place in this ranking. Unfortunately, in 2017 came on the second place, and Norway won the competion. In 2018 and 2019, Finland led in the happiness score ranking.\n3. Preparing report for one trait including all years: I wanted to transform data into one file which would be contain all factors for only one year. To do this, I prepared two functions:\ndef create_report_for_one_feature(report, col1, col2): ''' create_report_for_one_feature - Create report which consist only two columns report - set data frame col1 - name of first column from chosen data frame col2 - name of second column from chosen data frame ''' one_feature_report = report[[col1, col2]] return one_feature_report def create_ALLYears_report(report1, report2, report3, report4, report5, merge_on): ''' create_ALLYears_report - merging few data frames together report1, report2, report3, report4, report5 - names of data frames merge_on - set a common column for all data frames ''' report_AllYears = pd.merge(report1, report2, on= merge_on, suffixes=['_2015', '_2016']) report_AllYears = report_AllYears.merge(report3, on= merge_on ) report_AllYears = report_AllYears.merge(report4, on= merge_on, suffixes=['_2017','_2018'] ) report_AllYears = report_AllYears.merge(report5, on= merge_on ) report_AllYears.set_index('Country or region', inplace=True) return report_AllYears  First, I run function \u0026ldquo;create_report_for_one_feature()\u0026rdquo; for all 5 reports to extract one factor from all years.\nhappy_report2015 = create_report_for_one_feature(report2015, 'Country or region', 'HappinessScore') happy_report2016 = create_report_for_one_feature(report2016, 'Country or region', 'HappinessScore') happy_report2017 = create_report_for_one_feature(report2017, 'Country or region', 'HappinessScore') happy_report2018 = create_report_for_one_feature(report2018, 'Country or region', 'HappinessScore') happy_report2019 = create_report_for_one_feature(report2019, 'Country or region', 'HappinessScore') economy_report2015 = create_report_for_one_feature(report2015, 'Country or region', 'Economy') economy_report2016 = create_report_for_one_feature(report2016, 'Country or region', 'Economy') economy_report2017 = create_report_for_one_feature(report2017, 'Country or region', 'Economy') economy_report2018 = create_report_for_one_feature(report2018, 'Country or region', 'Economy') economy_report2019 = create_report_for_one_feature(report2019, 'Country or region', 'Economy') healthy_report2015 = create_report_for_one_feature(report2015, 'Country or region', 'Healthy_life_expectancy') healthy_report2016 = create_report_for_one_feature(report2016, 'Country or region', 'Healthy_life_expectancy') healthy_report2017 = create_report_for_one_feature(report2017, 'Country or region', 'Healthy_life_expectancy') healthy_report2018 = create_report_for_one_feature(report2018, 'Country or region', 'Healthy_life_expectancy') healthy_report2019 = create_report_for_one_feature(report2019, 'Country or region', 'Healthy_life_expectancy') social_support_report2015 = create_report_for_one_feature(report2015, 'Country or region', 'Social_support') social_support_report2016 = create_report_for_one_feature(report2016, 'Country or region', 'Social_support') social_support_report2017 = create_report_for_one_feature(report2017, 'Country or region', 'Social_support') social_support_report2018 = create_report_for_one_feature(report2018, 'Country or region', 'Social_support') social_support_report2019 = create_report_for_one_feature(report2019, 'Country or region', 'Social_support')  Then, using function \u0026ldquo;create_ALL Years report\u0026rdquo;, I merged all years into into one file.\nhappy_AllYears = create_ALLYears_report(happy_report2015, happy_report2016, happy_report2017, happy_report2018, happy_report2019, merge_on='Country or region') happy_AllYears.head()  economy_AllYears = create_ALLYears_report(economy_report2015, economy_report2016, economy_report2017, economy_report2018, economy_report2019, merge_on='Country or region') economy_AllYears.head()  healthy_AllYears = create_ALLYears_report(healthy_report2015, healthy_report2016, healthy_report2017, healthy_report2018, healthy_report2019, merge_on='Country or region') healthy_AllYears.head()  social_support_AllYears = create_ALLYears_report(social_support_report2015, social_support_report2016, social_support_report2017, social_support_report2018, social_support_report2019, merge_on='Country or region') social_support_AllYears.head()  3.1. Setting all years in one column I wanted to set all years in one column of data frame. To do this, I use pandas method called stack(). Next, I have to use additional method - to_frame() which helps convert result of stack() method to data frame.\nhappy_AllYears_stacked = happy_AllYears.stack().to_frame() economy_AllYears_stacked = economy_AllYears.stack().to_frame() healthy_AllYears_stacked = healthy_AllYears.stack().to_frame() social_support_AllYears_stacked = social_support_AllYears.stack().to_frame()  It\u0026rsquo;s a good idea to reset the index:\nhappy_AllYears_stacked.reset_index(inplace=True) economy_AllYears_stacked.reset_index(inplace=True) healthy_AllYears_stacked.reset_index(inplace=True) social_support_AllYears_stacked.reset_index(inplace=True)  Result of used above methods:\nhappy_AllYears_stacked.head()  It\u0026rsquo;s good to rename columns names in oder to better recognize them. I used builded in pandas method rename()\nnewHappyColNames = { 'level_1' : 'Year', 0 : 'HappinessScore'} newEconomyColNames = { 'level_1' : 'Year', 0 : 'Economy'} newHealthyColNames = { 'level_1' : 'Year', 0 : 'Health (LifeExpectancy)'} newSocialColNames = { 'level_1' : 'Year', 0 : 'socialSupport'} happy_AllYears_stacked.rename(columns=newHappyColNames, inplace=True) economy_AllYears_stacked.rename(columns=newEconomyColNames, inplace=True) healthy_AllYears_stacked.rename(columns=newHealthyColNames, inplace=True) social_support_AllYears_stacked.rename(columns=newSocialColNames, inplace=True) happy_AllYears_stacked.head() economy_AllYears_stacked.head() healthy_AllYears_stacked.head() social_support_AllYears_stacked.head()  3.2. Changes of happiness levels in 2015-2019 on line plot Here below, we can see how the citizens happiness score has changed from 2015 to 2019. The largest increase from 2015 to 2019 had Finland. An interesting case is Austria. From 2015 to 2017, score of this country was decreasing, but in 2017 to 2019, it was start to rising. From 2015 Norway and Denmark are the countries with the highest happiness score, which hasn\u0026rsquo;t changed rapidly to 2019.\n3.3. Presentation of changes in the level of traits that have the greatest impact on the level of happiness in 2015-2019 This time, I use heat map to show how factors has changed during next years for chosen countries, from 2015 to 2019. From this heat maps we can got to know that:\n in 2015 country with the highest:  happiness score was Switzerland, economy score was Qatar, life expectancy (health) score was Singapore, social support score was Iceland,   in 2016 country with the highest:  happiness score was Denmark, economy score was Qatar (score has increased from 1.69 to 1.82 points), life expectancy (health) score was Singapore (score has decreased from 1.03 to 0.95 points), social support score was Iceland (score has decreased from 1.40 to 1.18 points),   in 2017 country with the highest:  happiness score was Norway, economy score was Qatar (score has increased from 1.82 to 1.87 points), life expectancy (health) score was Singapore (score hasn\u0026rsquo;t changed, still equal 0.95 points), social support score was Iceland (score has increased from 1.18 to 1.61 points),   in 2018 country with the highest:  happiness score was Finland, economy score was United Arab Emirates (score has increased from 1.63 to 2.10 points), life expectancy (health) score was Singapore (score has increased from 0.95 to 1.01 points), social support score was Iceland (score has increased from 1.61 to 1.64 points),   in 2019 country with the highest:  happiness score was Finland (score has increased from 7.63 to 7.77 points), economy score was Qatar (score has increased from 1.65 to 1.68 points), life expectancy (health) score was Singapore (score has increased from 1.01 to 1.14 points), social support score was Iceland (score has decreased from 1.64 to 1.62 points).    ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/worldhappiness/project_worldhappiness_data_analysis/","summary":"2. Data reviewing: I checked if data are completed and if there is no NaN values.\n2.1. Correlation maps: On the correlation maps below, we see which factor has the highest influence on happiness score and which of them has lowest. Regardless the years, the highest influence on happiness score of the country has economy level of this country. On the opposite side, the generosity factor has little effect on happiness score.","tags":null,"title":"World Happiness project - data analysis"},{"categories":null,"contents":"1. Data presentation: Data downloaded from kaggle was splitted into years, which means in one file we have all factors from one year and we\u0026rsquo;ve 5 data files.:\n  Data from 2015:   Data from 2016:   Data from 2017:\n  Data from 2018:   Data from 2019:   At the beginning, I considered each report separately, and then sum up my analysis into one report - summary report.\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/worldhappiness/project_worldhappiness_data_presentation/","summary":"1. Data presentation: Data downloaded from kaggle was splitted into years, which means in one file we have all factors from one year and we\u0026rsquo;ve 5 data files.:\n  Data from 2015:   Data from 2016:   Data from 2017:\n  Data from 2018:   Data from 2019:   At the beginning, I considered each report separately, and then sum up my analysis into one report - summary report.","tags":null,"title":"World Happiness project - data presentation"},{"categories":null,"contents":"4. Preparing data to machine learning: I wanted to link all factors into one dataframe, where all years were located in one column. I used merge() method, where I chose merging on columns \u0026lsquo;Country or region\u0026rsquo; and \u0026lsquo;Year\u0026rsquo;.\nAllFeatures = pd.merge(happy_AllYears_stacked, economy_AllYears_stacked, on= ['Country or region', 'Year'] ) AllFeatures = AllFeatures.merge(healthy_AllYears_stacked, on= ['Country or region', 'Year'] ) AllFeatures = AllFeatures.merge(social_support_AllYears_stacked, on= ['Country or region', 'Year'] ) AllFeatures.head()  Next, I wished to find out what the descibe() method returns for column \u0026lsquo;happinessScore\u0026rsquo; in this \u0026lsquo;AllYears\u0026rsquo; data frame. I received count of values, mean and standard deviation of them. Moreover there is min and max values and quantiles. Technically speaking, the median is 0.5 quantile (50% quantile), which split data into equal sized groups. Another quantiles are 0.25 quantile and 0.75 quantile. They are are called quartiles. Quartiles and median split data into four equal sized groups.\nAllFeatures.groupby(by='Year')['HappinessScore'].describe()  4.1. Visualization of the factors that had the greatest impact on the level of happiness in 2015-2019 4.2. Ridding of outliers values cols = ['HappinessScore', 'Economy', 'Health (LifeExpectancy)', 'socialSupport'] colors = ['y', 'b', 'r', 'g'] fig, ax = plt.subplots(1, 4, figsize=(20,2), constrained_layout=True) for i, col in enumerate(cols): sns.boxplot( x=col, data=AllFeatures, ax=ax[i], color=colors[i]) plt.show()  # wyznaczamy dolną część pudełka (lewa część) Q1 = AllFeatures[cols].quantile(0.25) # wyznaczamy górną część pudełka (prawa część) Q3 = AllFeatures[cols].quantile(0.75) # IQR - rozstęp kwartylowy IQR = Q3 - Q1 outlier_condit = ((AllFeatures[cols] \u0026lt; (Q1 - 1.5*IQR)) | (AllFeatures[cols] \u0026gt; (Q3 + 1.5*IQR)) ) AllFeatures_iqr = AllFeatures[cols][~outlier_condit.any(axis=1)] fig, ax = plt.subplots(1, 4, figsize=(20,2), constrained_layout=True) for i, col in enumerate(cols): sns.boxplot( x=col, data=AllFeatures_iqr, ax=ax[i], color=colors[i]) plt.show()  fig, ax = plt.subplots(1,4, figsize=(20,4)) for i, col in enumerate(cols): sns.distplot(AllFeatures[col], ax=ax[i]) for i, col in enumerate(cols): sns.distplot(AllFeatures_iqr[col], ax=ax[i]) plt.show()  Without IQR: With IQR: AllFeatures.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 705 entries, 0 to 704 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Country or region 705 non-null object 1 Year 705 non-null object 2 HappinessScore 705 non-null float64 3 Economy 705 non-null float64 4 Health (LifeExpectancy) 705 non-null float64 5 socialSupport 705 non-null float64 dtypes: float64(4), object(2) memory usage: 58.6+ KB  Tekst\nfor col in ['Country or region', 'Year']: AllFeatures[col] = AllFeatures[col].astype('category') AllFeatures.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 705 entries, 0 to 704 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Country or region 705 non-null category 1 Year 705 non-null category 2 HappinessScore 705 non-null float64 3 Economy 705 non-null float64 4 Health (LifeExpectancy) 705 non-null float64 5 socialSupport 705 non-null float64 dtypes: category(2), float64(4) memory usage: 55.9 KB  Tekst\nAllFeatures.describe().T  Po pozbyciu się wartości odstających\nAllFeatures_iqr.describe().T  AllFeatures.describe(include=['category']).T  Tekst\nAllFeatures.isnull().sum() Country or region 0 Year 0 Country or region 0 Year 0 HappinessScore 0 Economy 0 Health (LifeExpectancy) 0 socialSupport 0 dtype: int64 AllFeatures_iqr.isnull().sum() HappinessScore 0 Economy 0 Health (LifeExpectancy) 0 socialSupport 0 dtype: int64  4.3. Visualization of the influence of the analyzed factors on the level of happiness score 4.4. Splitting data into training and test sets features = ['Economy', 'Health (LifeExpectancy)', 'socialSupport'] label = 'HappinessScore' X = AllFeatures[features].values y = AllFeatures[label].values.reshape(-1,1) # Bez wartości odstających: X_iqr = AllFeatures_iqr[features].values y_iqr = AllFeatures_iqr[label].values.reshape(-1,1) print(f'X: {X.shape}, X_iqr: {X_iqr.shape}') print(f'y: {y.shape}, y_iqr: {y_iqr.shape}') from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) X_iqr_train, X_iqr_test, y_iqr_train, y_iqr_test = train_test_split(X_iqr, y_iqr, test_size=0.2) print(f'X_train shape: {X_train.shape}, X_iqr_train shape: {X_iqr_train.shape}') print(f'X_test shape: {X_test.shape}, X_iqr_test shape: {X_iqr_test.shape}') print(f'y_train shape: {y_train.shape}, y_iqr_train shape: {y_iqr_train.shape}') print(f'y_test shape: {y_test.shape}, y_iqr_test shape: {y_iqr_test.shape}')  4.5. Scaling the values scaler = StandardScaler() scaler.fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) scaler_iqr = StandardScaler() scaler_iqr.fit(X_iqr_train) X_iqr_train = scaler.transform(X_iqr_train) X_iqr_test = scaler.transform(X_iqr_test)  4.6. Linear regression model testing metric_cols =[('SCORES', 'SCORE[%]'), ('SCORES', 'SCORE(IQR)[%]'), ('R2','Train[%]'), ('R2','Test[%]'), ('R2(IQR)','Train[%]'), ('R2(IQR)','Test[%]'), ('MSE','Train'), ('MSE','Test'), ('MSE(IQR)','Train'), ('MSE(IQR)','Test'), ('RMSE','Train'), ('RMSE','Test'), ('RMSE(IQR)','Train'), ('RMSE(IQR)','Test'), ] metrics_df = pd.DataFrame(columns=metric_cols ) metrics_df.columns = pd.MultiIndex.from_tuples(metrics_df.columns) index = 0 for i in range(150): # podzielimy dane na zbiory uczące i testowe: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) X_iqr_train, X_iqr_test, y_iqr_train, y_iqr_test = train_test_split(X_iqr, y_iqr, test_size=0.2) lr = LinearRegression() lr.fit(X_train, y_train) lin_reg_iqr = LinearRegression() lin_reg_iqr.fit(X_iqr_train, y_iqr_train) y_pred_test = lr.predict(X_test) y_pred_iqr_sc_test = lin_reg_iqr.predict(X_iqr_test) y_pred_train = lr.predict(X_train) y_pred_iqr_sc_train = lin_reg_iqr.predict(X_iqr_train) # --------------------------------------------------------------------------- score = lr.score(X_test, y_test) score_iqr = lr.score(X_iqr_test, y_iqr_test) r2_train = r2_score(y_train, y_pred_train) r2_train_iqr = r2_score(y_iqr_train, y_pred_iqr_sc_train) r2_test = r2_score(y_test, y_pred_test) r2_test_iqr = r2_score(y_iqr_test, y_pred_iqr_sc_test) mse_train = mean_squared_error(y_train, y_pred_train) mse_train_iqr = mean_squared_error(y_iqr_train, y_pred_iqr_sc_train) mse_test = mean_squared_error(y_test, y_pred_test) mse_test_iqr = mean_squared_error(y_iqr_test, y_pred_iqr_sc_test) rmse_train = mean_squared_error(y_train, y_pred_train, squared=False) rmse_train_iqr = mean_squared_error(y_iqr_train, y_pred_iqr_sc_train, squared=False) rmse_test = mean_squared_error(y_test, y_pred_test, squared=False) rmse_test_iqr = mean_squared_error(y_iqr_test, y_pred_iqr_sc_test, squared=False) # ----------------------------------------------------------------------------- SCORE, SCORE_IQR = round(score*100, 2), round(score_iqr*100, 2) R2_train, R2_test = round(r2_train*100, 2), round(r2_test*100,2) R2_IQR_train, R2_IQR_test = round(r2_train_iqr*100, 2), round(r2_test_iqr*100, 2) mse_train, mse_test = round(mse_train, 4), round(mse_test, 4) mse_IQR_train, mse_IQR_test = round(mse_train_iqr, 4), round(mse_test_iqr, 4) rmse_train, rmse_test = round(rmse_train, 4), round(rmse_test, 4) rmse_IQR_train, rmse_IQR_test = round(rmse_train_iqr, 4), round(rmse_test_iqr, 4) # ############################################################################# metrics_df.loc[index] = [ SCORE, SCORE_IQR, R2_train, R2_test, R2_IQR_train, R2_IQR_test, mse_train, mse_test, mse_IQR_train, mse_IQR_test, rmse_train, rmse_test, rmse_IQR_train, rmse_IQR_test ] index +=1 metrics_df.tail()  def highlight_best(s): index = {index: '' for index in s.index} if s[('R2','Test[%]')] \u0026gt; s[('R2','Train[%]')]: index[('R2','Test[%]')] = 'background-color: green' if s[('R2(IQR)','Test[%]')] \u0026gt; s[('R2(IQR)','Train[%]')]: index[('R2(IQR)','Test[%]')] = 'background-color: green' if s[('MSE','Test')] \u0026lt; s[('MSE','Train')]: index[('MSE','Test')] = 'background-color: green' if s[('MSE(IQR)','Test')]\u0026lt; s[('MSE(IQR)','Train')]: index[('MSE(IQR)','Test')] = 'background-color: green' if s[('RMSE','Test')] \u0026lt; s[('RMSE','Train')]: index[('RMSE','Test')] = 'background-color: green' if s[('RMSE(IQR)','Test')] \u0026lt; s[('RMSE(IQR)','Train')]: index[('RMSE(IQR)','Test')] = 'background-color: green' return index metrics_df.tail().style.apply( highlight_best, axis=1)  # Jak poszło przewidywanie danych fig, ax = plt.subplots(1, 2, figsize=(10,5), constrained_layout=True) y_list = [ { 'b' : [ y_test , y_pred_test ] }, { 'g' : [ y_iqr_test , y_pred_iqr_sc_test ] } ] labels_dict = [ {'Z wartościami odstającymi' : [ 'y_test' , 'y_pred' ] }, {'bez wartości odstających' : [ 'y_test' , 'y_pred' ] } ] index = 0 for col in range(2): yTestAndPred = [val for val in y_list[index].values() ][0] color = [key for key in y_list[index].keys() ] xyLabels = [val for val in labels_dict[index].values() ][0] title = [key for key in labels_dict[index].keys() ] ax[col].scatter(yTestAndPred[0], yTestAndPred[1], s=80, facecolors='none', edgecolors= color) ax[col].set_xlabel(xyLabels[0]) ax[col].set_ylabel(xyLabels[1]) ax[col].set_title(title[0]) index+=1 plt.show()  # Wykresy wartości resztowych: fig, ax = plt.subplots(2, 2, figsize=(10,9), constrained_layout=True) error_train = y_pred_train - y_train # ta wartość mówi nam, jak bardzo pomyliliśmy się w trakcie przewidywania wartości error_test = y_pred_test - y_test error_iqr_train = y_pred_iqr_sc_train - y_iqr_train error_iqr_test = y_pred_iqr_sc_test - y_iqr_test y_list = [ { 'b' : [ y_train , error_train ] }, { 'g' : [ y_test , error_test ] }, { 'b' : [ y_iqr_train , error_iqr_train ] }, { 'g' : [ y_iqr_test , error_iqr_test ] } ] labels_dict = [ {'Z wartościami odstającymi' : [ 'y_train' , 'error_train' ] }, {'Z wartościami odstającymi' : [ 'y_test' , 'error_test' ] }, {'bez wartości odstających' : [ 'y_train' , 'error_train' ] }, {'bez wartości odstających' : [ 'y_test' , 'error_test' ] } ] index = 0 for row in range(2): for col in range(2): yAndError= [val for val in y_list[index].values() ][0] color = [key for key in y_list[index].keys() ] xyLabels = [val for val in labels_dict[index].values() ][0] title = [key for key in labels_dict[index].keys() ] ax[row,col].scatter(yAndError[0], yAndError[1], s=80, facecolors='none', edgecolors= color) ax[row,col].hlines(y=1, xmin=3, xmax=8, colors='r', linestyles='--', lw=1.5) ax[row,col].hlines(y=-1, xmin=3, xmax=8, colors='r', linestyles='--', lw=1.5) ax[row,col].set_xlabel(xyLabels[0]) ax[row,col].set_ylabel(xyLabels[1]) ax[row,col].set_title(title[0]) index+=1 plt.show()  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://armind93.github.io/posts/worldhappiness/project_worldhappiness_machine_learning/","summary":"4. Preparing data to machine learning: I wanted to link all factors into one dataframe, where all years were located in one column. I used merge() method, where I chose merging on columns \u0026lsquo;Country or region\u0026rsquo; and \u0026lsquo;Year\u0026rsquo;.\nAllFeatures = pd.merge(happy_AllYears_stacked, economy_AllYears_stacked, on= ['Country or region', 'Year'] ) AllFeatures = AllFeatures.merge(healthy_AllYears_stacked, on= ['Country or region', 'Year'] ) AllFeatures = AllFeatures.merge(social_support_AllYears_stacked, on= ['Country or region', 'Year'] ) AllFeatures.head()  Next, I wished to find out what the descibe() method returns for column \u0026lsquo;happinessScore\u0026rsquo; in this \u0026lsquo;AllYears\u0026rsquo; data frame.","tags":null,"title":"World Happiness project - machine learning"}]